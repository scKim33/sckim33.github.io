---
title: "[CoT] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
last_modified_at: 2025-02-08
categories:
  - paper_review
tags:
  - CoT
  - LLM
  - Google
  - NeurIPS
excerpt: "CoT paper review"
use_math: true
classes: wide
---

> NeurIPS 2022 Accept. [[Paper](https://arxiv.org/abs/2201.11903)]  
> Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou  
> 28 Jan 2022


## Summary

Chain-of-thought prompting으로 답변의 성능을 높일 수 있음을 보인다. 다양한 reasoning task 모두에서 성능 향상을 보인다. 

## Introduction

<div style="display: flex; justify-content: center; align-items: center;">
  <div style="text-align: center; margin-right: 0px;">
    <img src='{{"assets/images/CoT/cot10.png" | relative_url}}' width="80%">
    <figcaption></figcaption>
  </div>
  <div style="text-align: center;">
    <img src='{{"assets/images/CoT/cot1.png" | relative_url}}' width="80%">
    <figcaption></figcaption>
  </div>
</div>

Reasoning 능력을 얻기 위해 이전 연구에서는 아예 자연어 reasoning 과정을 처음부터 학습시키거나, fine-tuning에 자연어 reasoning을 학습시키거나 neuro-symbolic 방식(마치 pseudocode 같은 형식으로 학습시키는 것)을 사용한다. 다른 방법으로는 prompting을 이용한 few-shot learning이다. 첫번째 방법은 많은 학습 데이터를 요구하기에 비용이 많이 든다. 두번째 방법도 reasoning을 요구하는 task에서는 성능이 영 좋지 못하다.
CoT에서는 <input, chain of thought, output>의 형식으로 prompting하여 기존 prompting이 갖는 <input, output> 형식의 한계를 극복하고자 한다.


## Arithmetic reasoning

<center>
<img src='{{"assets/images/CoT/cot2.png" | relative_url}}' width="50%">
<figcaption style="text-align: center;">Arithmetic reasoning problem 예시</figcaption>
</center>

### Results

<center>
<img src='{{"assets/images/CoT/cot3.png" | relative_url}}' width="40%">
<figcaption style="text-align: center;">Arithmetic reasoning results</figcaption>
</center>

3가지 특징을 확인할 수 있다. 첫째로, 모델의 사이즈가 커짐에 따라 CoT가 더 효과적임을 확인할 수 있다. 작은 모델은 종종 illogical한 CoT를 출력하는 경우가 있었다. 두번째로, CoT는 더 복잡한 문제에서 효과적이었다. 난이도가 있는 GSM8K에서 더 많은 성능 향상을 보였고, 상대적으로 쉬운 MAWPS에서는 성능 향상은 있었지만 GSM8K보다는 적었다. 세번째로 GPT-3 175B, PaLM 540B를 CoT prompting한 경우 일부분에서 SOTA를 뛰어넘는 성적을 보여준다.

### Ablation study

<center>
<img src='{{"assets/images/CoT/cot4.png" | relative_url}}' width="30%">
<figcaption style="text-align: center;">ablation study results</figcaption>
</center>

Ablation study로 세 가지 의심에 대해 검증한다. 첫번째로 CoT가 문제를 풀기 위한 수학 공식을 만들어내도록 하기 때문이라는 의견이 있다. 하지만 equation만을 중간 prompt로 제공해주었을 때는 큰 성능 변화를 주지 못한다. 이는 질문의 의미를 자연어 없이 equation으로 번역하는 것이 어려웠다는 것을 의미한다. 그리고 단순히 더 많은 토큰이 사용되어 더 나은 성능을 이끌어내었다는 의견이 있다. 이를 확인하기 위해 CoT의 과정에서 사용되는 토큰의 양만큼 점(.)을 출력하여 표현하도록 하였다. 이 때에도 성능 향상을 보이지는 않았다. CoT prompting이 관련 지식을 습득히는 접근성을 늘리기 때문이라는 의견이 있을 수 있다. 따라서 CoT를 답변 뒤에 주는 방식으로 prompting을 사용해보았고 이 때도 성능 향상을 이끌어내지 못했다.

<center>
<img src='{{"assets/images/CoT/cot5.png" | relative_url}}' width="30%">
<figcaption style="text-align: center;">prompt style change results</figcaption>
</center>

CoT는 다른 사람이 쓴 prompt에 대해서나 의도적으로 style을 변경해서 적거나 데이터셋 내의 서로 다른 예제 문제들에 대해서도 robust하게 성능이 유지되었다.

## Commonsense reasoning

<center>
<img src='{{"assets/images/CoT/cot6.png" | relative_url}}' width="80%">
<figcaption style="text-align: center;">Commonsense reasoning problem 예시</figcaption>
</center>

일반 상식에 대한 문제들이다.

<center>
<img src='{{"assets/images/CoT/cot7.png" | relative_url}}' width="80%">
<figcaption style="text-align: center;">Commonsense reasoning problem reuslts</figcaption>
</center>


## Symbolic reasoning

<center>
<img src='{{"assets/images/CoT/cot8.png" | relative_url}}' width="50%">
<figcaption style="text-align: center;">Symbolic reasoning problem 예시</figcaption>
</center>

두 가지 toy tasks를 사용한다.
- Last letter condcatenation : 단어를 주고 단어의 끝을 이어붙인 것을 답변하도록 함.
- Coin flip : 글을 읽고 동전의 앞/뒷면을 예측.

<center>
<img src='{{"assets/images/CoT/cot9.png" | relative_url}}' width="30%">
<figcaption style="text-align: center;">Symbolic reasoning problem results</figcaption>
</center>

In-domain(문제 해결에 기존 문제와 같은 수의 step을 거침)과 OOD(문제 해결에 기존 문제보다 더 많은 step을 거침) 환경에서 테스트한다. OOD 데이터에 대해서도 예측을 잘 하여 generalization 성능을 확인할 수 있다.

## Discussion

일반적인 prompting에서 모델의 크기에 따라 나타나는 정확도의 scaling curve가 flat한 모습을 보인다면, CoT prompting에서는 더 가파르고 지수적으로 나타났다.

하지만 이것이 신경망이 진짜 reasoning 하고 있는가?에 대해서는 우리가 알 수 없다는 한계가 있다. 실제로 추론을 하고 있는 것인지 단순히 패턴을 따라가는 것인지 알 수 없다는 것이다. 그리고 우리가 few-shot learning을 통해 CoT를 제공하는 경우 그 cost가 무시할 수 있을 정도지만, 만약 LLM의 fine-tuning에 CoT를 사용하고 싶은 경우 학습 데이터 모두에 그러한 과정을 거쳐야 하기 때문에 비용이 매우 커질 수 있다. 더욱이 (심지어 정답을 맞췄다 하더라도) 모델의 reasoning 과정이 올바르지 않을 수 있다. 그리고 parameter size가 큰 모델에 대해서 CoT가 더욱 효과적으로 나타나 작은 모델에 대해서 CoT 효용을 높이기 위한 추가적인 연구가 필요해 보인다.

## Why does increasing model scale improve chain-of-thought prompting?

모델이 일으키는 오류를 크게 3가지로 분류하면 다음과 같다.
- Semantic understanding : 문제 자체를 잘못 이해한 경우. 즉, 풀이가 틀림.
- One step missing : 조건 하나를 빼먹은 경우. 즉, 계산 과정 하나가 빠진 경우.
- Ohters : hallucination, repetitive outputs, symbol mapping errors가 포함.

<center>
<img src='{{"assets/images/CoT/cot11.png" | relative_url}}' width="50%">
<figcaption style="text-align: center;">3가지 오류 및 scale에 따른 error correction</figcaption>
</center>

큰 모델에서는 이러한 오류들을 종류 관계없이 상당 부분 해결한다. Model scale이 semantic understanding이나 logical reasoning 성능의 함수로써 적용되고 있음을 의미한다.

<center>
<img src='{{"assets/images/CoT/cot12.png" | relative_url}}' width="80%">
<figcaption style="text-align: center;">model scale에 따른 error correction examples</figcaption>
</center>