---
title: "[PPO] Proximal Policy Optimization Algorithms"
last_modified_at: 2025-03-04
categories:
  - paper_review
tags:
  - PPO
  - Reinforcement Learning
  - On policy
  - Model free
  - OpenAI
excerpt: "PPO paper review"
use_math: true
classes: wide
---

> arXiv. [[Paper](https://arxiv.org/abs/1812.05905)]  
> John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov  
> 20 Jul 2017

## Summary

TRPO의 장점을 계승하면서도 implement하기 쉬운 알고리즘인 PPO를 제시한다.

## Introduction

이전에 제시된 알고리즘들에는 DQN, PG, TRPO가 있다. DQN은 함수 근사로 인한 불안정한 학습을 한다. PG는 데이터효율이 낮고 하이퍼파라미터에 민감하다. TPRO는 알고리즘이 복잡하고 드롭아웃이나 파라미터를 공유하는 구조와는 호환되지 않는 특성이 있다. PPO는 TRPO의 안정성을 유지하면서도 단순한 알고리즘으로 implementation에 효과적이다.

## Background: Policy Optimization

Vanilla policy gradient은 다음과 같다.

$$
L^{PG}(\theta) = \hat{\mathbb{E}}_t \left[ \log \pi_{\theta} (a_t \mid s_t) \hat{A}_t \right].
$$

TRPO에서는 KL divergence에 대한 constraint를 두어 다음의 constrained optimization problem을 해결한다.

$$
\begin{aligned}
    &\max_{\theta} \quad \hat{\mathbb{E}}_t \left[ \frac{\pi_{\theta} (a_t \mid s_t)}{\pi_{\theta_{\text{old}}} (a_t \mid s_t)} \hat{A}_t \right] \\
    &\text{subject to} \quad \hat{\mathbb{E}}_t \left[ \text{KL} [\pi_{\theta_{\text{old}}} (\cdot \mid s_t), \pi_{\theta} (\cdot \mid s_t)] \right] \leq \delta.
\end{aligned}
$$

## Clipped Surrogate Objective

TRPO에서 다루는 conservative policy iteration loss를 정의한다.

$$
L^{CPI}(\theta) = \hat{\mathbb{E}}_t \left[ \frac{\pi_{\theta} (a_t \mid s_t)}{\pi_{\theta_{\text{old}}} (a_t \mid s_t)} \hat{A}_t \right] = \hat{\mathbb{E}}_t \left[ r_t(\theta) \hat{A}_t \right].
$$

그런데 importance sampling coefficient에 의해 이 값은 굉장히 불안해질 수 있다. TRPO에서는 이를 해결하기 위해 KL divergence에 제약을 주어 conjugate gradient method로 해결한다. 하지만 이는 state에 대한 행렬 계산과 같이 많은 연산량을 동반한다. PPO에서는 이를 clip하여 해결하고자 한다.

$$
L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip} (r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right].
$$

<center>
<img src='{{"assets/images/PPO/ppo1.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">clipped objective</figcaption>
</center>

Old policy와 new policy간의 ratio 차이가 큰 경우 \\(r_t(\theta)\\)는 0에 근접하거나 발산할 수 있다. 이를 방지함으로써 안정적인 학습이 가능하도록 한다. (그런데 $A<0,r\to\infty$이면 발산하는데 그 부분은 설명이 없는 것 같다)

<center>
<img src='{{"assets/images/PPO/ppo2.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;"></figcaption>
</center>

Linear interpolation factor에 따른 손실 함수의 그래프이다. CPI objective의 경우(주황) 분포 차이가 커지면 극단적인 \\(r_t\\)가 값을 증가시키는 것으로 보인다. 초록 objective의 경우 asymptotic한 성향을 보이는데 분포 차이가 커짐에 따라 상수 구간의 \\(r_t\\) 값에 포함되는 경우가 많기 때문이다. CLIP objective(빨강)의 경우 실제 KL divergence 차이(0.02)를 기준으로 극댓값을 가지며 감소하고 있다. CPI objective가 분포 차이가 커짐에 따라 estimate의 variance가 커지고 min 연산이 더욱 많은 near-zero estimate를 취할 수 있기 때문인 것으로 보인다.


## Adaptive KL Penalty Coefficient

또 다른 방법으로는 KL divergence를 직접적으로 loss에 포함시키는 방법이 있다. KL divergence가 상한 임계값을 넘어가면 penalty coefficient를 증가시키고, 하한 임계값보다 작아지면 감소시키는 식으로 한다.

$$
L^{KLPEN}(\theta) = \hat{\mathbb{E}}_t \left[ \frac{\pi_{\theta} (a_t \mid s_t)}{\pi_{\theta_{\text{old}}} (a_t \mid s_t)} \hat{A}_t - \beta \text{KL} [\pi_{\theta_{\text{old}}} (\cdot \mid s_t), \pi_{\theta} (\cdot \mid s_t)] \right]\\
$$

$$
\begin{aligned}
    &\text{- If } d < d_{\text{targ}} / 1.5, \quad \beta \gets \beta / 2 \\
    &\text{- If } d > d_{\text{targ}} \times 1.5, \quad \beta \gets \beta \times 2
\end{aligned}
$$

결론적으로는 Clip loss보다 낮은 성능을 보이지만, baseline으로서의 역할을 하기 때문에 소개하고 있다.

## Algorithm

최종적인 objective로는 다음과 같이 Clip objective, Value function objective, Entropy bonus가 있다. 특이한 점은 value function network도 존재한다는 것인데, advantage estimate를 계산할 때 사용된다.

$$
L_t^{CLIP+VF+S}(\theta) = \hat{\mathbb{E}}_t \left[ L_t^{CLIP}(\theta) - c_1 L_t^{VF}(\theta) + c_2 S[\pi_{\theta}(s_t)] \right],
$$

Value function objective는 squared error loss \\((V_{\theta}(s_t) - V_t^{\text{targ}})^2.\\)이다.

Advantage estimator로는 discounted return style이나 GAE style을 사용할 수 있다.

$$
\hat{A}_t = -V(s_t) + r_t + \gamma r_{t+1} + \dots + \gamma^{T - t + 1} r_{T - 1} + \gamma^{T - t} V(s_T)
$$

$$
\hat{A}_t = \delta_t + (\gamma \lambda) \delta_{t+1} + \dots + (\gamma \lambda)^{T - t + 1} \delta_{T-1},\\
\text{where} \quad \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
$$

<center>
<img src='{{"assets/images/PPO/ppo3.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">PPO Algorithm</figcaption>
</center>

## Experiments

<center>
<img src='{{"assets/images/PPO/ppo4.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Results</figcaption>
</center>

랜덤 policy가 주는 값을 0, 가장 좋은 점수를 낸 case를 1로 두었을 때, no clipping된 경우 랜덤 policy보다 못한 성능을 보인다.

<center>
<img src='{{"assets/images/PPO/ppo5.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Results</figcaption>
</center>

<center>
<img src='{{"assets/images/PPO/ppo6.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Results</figcaption>
</center>

<center>
<img src='{{"assets/images/PPO/ppo7.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Results</figcaption>
</center>

Atari domain에서의 성능은 total reward에 대해서는 ACER보다 우수하고, 마지막 100 에피소드에 대해서는 ACER가 조금 더 우수했다.