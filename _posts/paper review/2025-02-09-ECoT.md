---
title: "[ECoT] Robotic Control via Embodied Chain-of-Thought Reasoning"
last_modified_at: 2025-02-09
categories:
  - paper_review
tags:
  - CoT
  - VLA
  - Robotics
  - Imitation learning
  - CoRL
excerpt: "ECoT paper review"
use_math: true
classes: wide
---

> CoRL 2024. [[Paper](https://arxiv.org/abs/2407.08693)] [[Github](https://embodied-cot.github.io/)]  
> Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, Sergey Levine  
> 11 Jul 2024


## Summary

복잡한 문제를 단계적으로 다루는 Chain-of-thought prompting을 단순히 VLA에 적용하는 것은 VLA가 주로 단순한 예제를 다룬다는 점에서 덜 효과적이다. 또한 온전히 semantic reasoning만을 다루는 것이 robot policies에는 충분하지 않다. 추론한 내용을 다듬어서 센서들을 통한 관측과 로봇 상태변수와 상호작용해야하기 때문이다. 본 연구에서는 ECoT(Embodied chain-of-thought)를 제시한다. ECoT는 VLA가 action predict하기 전에 plan, sub-tasks, motions, visually grounded features의 reasoning 과정을 거치게 한다.

## Introduction

VLA는 VLM을 robot action을 하도록 fine-tune한 모델이다. 이전의 VLA들은 task가 주어졌을 때 바로 observation에서 action으로의 mapping이 이루어진다. ECoT를 통해 글씨를 통해 reasoning하도록 학습시켜 성능을 끌어올릴 수 있다. 다만 이를 위해서는 기존 CoT와 다른 차별점을 가져야 한다. 첫째로 지금 나오고 있는 open source VLA들은 model scale이 작은 편이라 CoT만으로 의미있는 reasoning을 하기에는 부족할 수 있다. (CoT에서는 PaLM 540B모델을 사용함) 둘째로 언어 모델과 다르게 robot tasks는 task와 reasoning이 observation과의 상호작용에서 이루어진다. 따라서 '생각'으로서의 reasoning 뿐만 아니라 이를 '관찰'과 결합하여야 좋은 성능을 기대할 수 있다.

## Vision-language-action models

<center>
<img src='{{"assets/images/ECoT/ecot1.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">OpenVLA structure</figcaption>
</center>

Backbone model로 OpenVLA를 사용한다. OpenVLA는 Llama2 7B 기반 VLA로 image observation을 tokenize하는 ViT 계열의 DinoV2와 SigLIP가 있다. Task는 Llama tokenizer를 거쳐 토큰화되고, Llama는 robot의 action token을 출력하게 된다.

## Embodied chain-of-thought reasoning for visumotor policies

<center>
<img src='{{"assets/images/ECoT/ecot2.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">vanilla VLA vs VLA with ECoT</figcaption>
</center>

ECoT의 학습 파이프라인을 구성하기 위해 VLA를 학습하기 위한 데이터셋이 필요하다. 기존 observation-action 형태의 데이터셋을 pre-trained model을 이용하여 observation-reasoning-action의 형태로 가공한다. Reasoning은 자연어 string으로 표현되어 tokenizer를 통해 토큰화될 수 있다. VLA는 reasoning token에 대해 autoregressively 다음 reasoning token을 예측하고, 바로 이어서 action token도 예측하게 된다.

<center>
<img src='{{"assets/images/ECoT/ecot4.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Generating synthetic ECoT data</figcaption>
</center>

데이터 생성 과정은 구체적으로 5단계로 이루어진다. 인간이 직접 생성하는 것은 비용이 많이 들기 때문에 (수백만 개의 transitions) pre-trained vision/language foundation model이 자동적으로 데이터를 생성하도록 한다. 2.5M size의 Bridge v2 dataset을 가공하였고 7일이 소요되었다. 데이터 생성 과정은 5단계로 구성된다.
1. 기존 observation 데이터를 보고 Prismatic-7B VLM이 scene을 설명한다.
2. Task instruction과 1.에서 생성된 description을 concat하여 GroundingDINO(open-vocabulary object detector)에게 input으로 보낸다. GroundingDINO는 모든 연관있는 단어를 observation에서 찾아내어 bounding-boxes로 표시한다. 그 중 box-confidence가 0.3, text-confidence가 0.2 이상인 것들을 고른다.
3. 729개의 template화 되어있는 간단한 움직임의 조합(action primitives)으로 low-level action을 나타낸다. OWLv2와 SAM을 이용하여 2D end effector 위치를 추정하고 로봇의 3D 상태정보를 바탕으로 RANSAC을 통해 projection matrix를 추정한다. 이를 통해 학습 과정에서 로봇의 end effector 위치의 2D projection을 사용하여 카메라 위치를 일일이 고려할 필요가 없게 된다.
4. 앞서 만든 모든 내용을 Gemini 1.0에 전달하여 high-level의 sub-task planning을 하도록 한다. 각 sub-task 내에는 task instruction, movement primitives, current sub-task가 포함된다.


<center>
<img src='{{"assets/images/ECoT/ecot3.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">ECoT process</figcaption>
</center>


ECoT 과정은 6단계로 이루어진다.
1. TASK : task에 대해 rephrase 하는 과정.
2. PLAN : high-level로 task를 수행하는 과정을 기술한다.
3. SUBTASK : 각 plan 단계에서 수행할 일을 reasoning을 통해 도출한다.
4. MOVE : lower-level language로 로봇이 어떤 움직임을 주어야 할지 명령한다.
5. GRIPPER : end effector의 위치를 표시한다.
6. OBJECTS : 인식되는 물체들의 위치를 각각 표시한다.

ECoT의 경우 한 step에 350개의 토큰을 예측해야 하므로 속도가 느리다. (OpenVLA의 경우 7개) 이를 해결하기 위해 high-level plan을 몇 step 동안 고정할 수 있다. 또한 transformer 계열 모델 특성 상 이전에 이미 예측했던 token에 대해서는 저장했다가 encoding으로 처리하는게 새로 generating하는 것 보다 cost가 적게 든다는 것을 이용할 수 있다. 따라서 두 가지 전략을 사용한다. 첫번째는 동기적 실행; high-level reasoning에 대해서는 N step마다 실행한다. 두번째는 비동기적 실행; 한 모델이 high-level reasoning을 계속 실행하고, 다른 모델이 (high-level reasoning을 받아 encoding만 하면서) low-level reasoning을 실행한다.

## Experiments

<center>
<img src='{{"assets/images/ECoT/ecot5.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">ECoT results</figcaption>
</center>

55B 모델을 사용하는 RT-2-X와 비슷하거나 일부는 능가하는 모습을 보여준다. Naive CoT는 high-level reasoning만 수행하며 observation이나 로봇 state와 같은 정보들을 활용하지 않는 방식이다.

<center>
<img src='{{"assets/images/ECoT/ecot6.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Successful/failed example</figcaption>
</center>

ECoT 방식의 좋은 점 중 하나는 task가 실패했을 때 왜 실패했는지에 대한 이유를 어느정도 파악할 수 있기 때문이다. 가령, 실패한 task 예시(우측)을 보면 빨간 망치를 screwdriver로 착각하여 실패한 것을 알 수 있다. 이를 바탕으로 모델을 개선할 수 있게 된다.

<center>
<img src='{{"assets/images/ECoT/ecot7.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Human interrupt effect</figcaption>
</center>

또한 ECoT는 인간의 개입에 의해 가장 많은 성능 향상을 확인할 수 있다. 인간의 개입에 의해 ChatGPT로 reasoning을 하도록 하고, 이를 5 step 동안 입력으로 넣어준다.

<center>
<img src='{{"assets/images/ECoT/ecot8.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">sync/async results</figcaption>
</center>

동기(5-step)/비동기 방식에 의한 성능 변화도 확인한다. 비동기 방식의 경우 parallel 구조로 인해 계산 비용은 두배로 들지만 속도 향상이 가장 높았고, 동기 방식은 trade-off가 적절히 이루어진 형태로 main result로 사용하였다.

## Additional analysis

Bounding box를 planning 바로 뒤 단계에 위치시킨다. 그리고 gripper position에 대해서는 4-step만큼 예측하게 하여 policy가 어떻게 작동하는지 시각적으로 정보를 제공할 수 있다. Bbox의 순서 변경은 비록 성능은 다소 떨어뜨렸지만 여전히 baseline보다 우수했고 추론 속도를 30~50% 향상시켰다. 

<center>
<img src='{{"assets/images/ECoT/ecot9.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">additional analysis results</figcaption>
</center>

ECoT 데이터만을 학습하게 되면 상대적으로 VLM의 원래 능력(자연어로 대답하는 능력)을 잃을 수 있다. 따라서 ECoT 데이터와 VLM 데이터를 3:1 비율로 학습하여 진행한 결과 task performance에서는 큰 성능 차이를 보이지는 않았다. 다만 “bring coke can to Taylor Swift”와 같이 celebrities를 인식하는 과제에서는 더 좋은 성능을 보였다.(성공률 4/4 vs 0/4)

<center>
<img src='{{"assets/images/ECoT/ecot10.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Implementation on the other robot embodiments</figcaption>
</center>

ECoT 모델을 다른 로봇에 대해서도 잘 적용되는지 알아보기 위해 transfer learning을 테스트한다. BridgeData V2로 학습한 OpenVLA-7B 모델의 checkpoint를 가져와 ECoT 데이터셋으로 학습시킨다. 데이터 볼륨으로 따지면 13% 가량이 ECoT 데이터셋이다. 학습 결과 transfer model의 20k training step 성능과 base model의 80k training step 성능이 비슷하게 나왔다. 그리고 실제로 WidowX robot으로 학습한 모델을 다른 로봇에서 사용하여도 reasoning을 수행할 수 있었다. 다만 시뮬레이션 환경 SIMPLER에서는 sim-to-real gap에 의해 올바른 reasoning을 하지 못하는 모습을 보인다.


## Discussion and limitations

한계점을 제시한다. 우선 reasoning 과정 자체는 fixed된 단계를 거치도록 설정되었다는 점이다. 그리고 ECoT 학습 데이터를 OXE 데이터셋을 가공하여 더 확보한다면 transfer learning의 성능 개선을 기대할 수 있다. 마지막으로 ECoT policy의 추론 속도는 여전히 제한되어 있다.