---
title: "[CQL] Conservative Q-Learning for Offline Reinforcement Learning"
last_modified_at: 2025-02-03
categories:
  - paper_review
tags:
  - Reinforcement Learning
  - 
excerpt: "CQL paper review"
use_math: true
classes: wide
---

> ICLR 2022 Poster. [[Paper](https://arxiv.org/abs/2006.04779)] [[Github](https://github.com/young-geng/CQL/)]  
> Ilya Kostrikov, Ashvin Nair, Sergey Levine  
> 29 Jan 2022

## Summary

Offline RL에서는 학습 데이터셋과 learned policy 간에 발생하는 distributional shift로 인해 학습이 원활하게 이루어지지 않는 문제가 있다. CQL은 Q-function의 lower bound를 학습하여 theoretical guarantees를 보장하고하 한다.

## Introduction

Offline RL은 미리 수집해 둔 학습 데이터로 policy를 학습하여 활용할 수 있는 데이터셋의 pool이 크다. 다만 distributional shift와 같은 문제점 또한 존재한다. Offline setting에서 value based method를 바로 사용하게 되는 경우 OOD actions나 overfitting에 의한 부작용이 크게 발생할 수 있다.

이를 해결하기 위해 CQL을 제시한다. CQL에서는 expected value of Q-function을 이용하여 true policy value의 lower bound를 설정하게 된다. Expected value를 이용하는 경우 point-wise lower bounded Q-function에서 발생하는 extra under-estimation을 막는다. Implementation 또한 20줄 남짓의 코드로 regularization term만 추가해 주면 되기 때문에 간단하다.

## Preliminaries

생략

## The conservative Q-learning (CQL) framework

### Conservative off-policy evaluation

설명에 앞서 몇 가지 용어들에 대해 설명하면, target policy(이후에 학습의 대상이 되는) $\pi$, 데이터셋 $\mathcal{D}$를 생성하는 behavior policy $\pi_\beta(a\|s)$가 있다. (정확히 알려지지 않은 distribution이다.) 그리고 데이터셋에서 관찰된 empirical distribution $\mu(s,a)$가 있다. $\mu$는 데이터셋의 state-marginal에 제한되도록 $\mu(s,a)=d^{\pi_\beta}(s)\mu(a\|s)$의 꼴로 둔다. (이상적으로는 $\mu(a\|s)=\pi_\beta(a\|s)$이겠지만)

CQL에서는 기존 Q-learning setting에서 추가적으로 $\mu$에서 sampling된 데이터의 Q-value expectation를 최소화하는 term을 추가하게 된다. 이 term의 역할은 $\pi$에 대한 policy evaluation를 lower bound하는 것이다. 이를 수식으로 나타내면

$$
\hat{Q}^{k+1} \leftarrow \arg\min_{Q} \, \alpha \, \mathbb{E}_{s \sim \mathcal{D}, a \sim \mu(a|s)} [Q(s, a)] 
+ \frac{1}{2} \, \mathbb{E}_{s, a \sim \mathcal{D}} \left[ \left( Q(s, a) - \hat{\mathcal{B}}^\pi \hat{Q}^k(s, a) \right)^2 \right]
$$

> $\textbf{Theorem 3.1.}$ For any $\mu(a\|s)$ with $\operatorname{supp} \mu \subset \operatorname{supp} \hat{\pi}_\beta$, with probability $\geq 1 - \delta$, $\hat{Q}^\pi$ (the Q-function obtained by iterating Equation 1) satisfies:  
> <div align="center">
> $$
  \forall s \in \mathcal{D}, \, a, \quad \hat{Q}^\pi(s, a) \leq Q^\pi(s, a) - \alpha 
  \left[ 
  \left( I - \gamma P^\pi \right)^{-1} \frac{\mu}{\hat{\pi}_\beta} 
  \right](s, a) + 
  \left[
  \left( I - \gamma P^\pi \right)^{-1} 
  \frac{C_{r, T, \delta} R_{\text{max}}}{(1 - \gamma) \sqrt{\lvert \mathcal{D} \rvert}}
  \right](s, a).
  $$
> </div>

<details>
<summary>Proof of Theorem 3.1.</summary>
<div markdown="1">

우선 $\operatorname{supp}$는 지지집합(support)로, 그 함수가 0이 아닌 점들의 집합의 closure이다. 여기서는 좀 더 쉽게 non-zero probability action을 의미한다. 따라서 가정에 의해 $\mu(a\|s)$가 취할 수 있는 action은 $\hat{\pi}_\beta$가 취할 수 있는 action에 한정된다.

$$
\hat{Q}^{k+1} \leftarrow \arg\min_{Q} \, \alpha \, \mathbb{E}_{s \sim \mathcal{D}, a \sim \mu(a|s)} [Q(s, a)] 
+ \frac{1}{2} \, \mathbb{E}_{s, a \sim \mathcal{D}} \left[ \left( Q(s, a) - \hat{\mathcal{B}}^\pi \hat{Q}^k(s, a) \right)^2 \right]
$$

위 식에서, 최소로 하는 $Q$의 값을 explicit하게 구할 수 있다. 식을 미분하여 0이 되도록 하는 $Q$를 다음 iteration의 $Q^{k+1}$ 값으로 두면,

$$
\nabla_Q\left(\alpha \, \mathbb{E}_{s \sim \mathcal{D}, a \sim \mu(a|s)} [Q(s, a)] 
+ \frac{1}{2} \, \mathbb{E}_{s \sim \mathcal{D}, a \sim \hat{\pi}_\beta(a|s)} \left[ \left( Q(s, a) - \hat{\mathcal{B}}^\pi \hat{Q}^k(s, a) \right)^2 \right]\right) = 0 \\
\alpha \mu(a|s) 
+ \frac{1}{2} \cdot 2 \left( Q(s, a) - \hat{\mathcal{B}}^\pi \hat{Q}^k(s, a) \right) \hat{\pi}_\beta(a|s) = 0 \\
\forall s, a \in \mathcal{D}, \, k, \quad \hat{Q}^{k+1}(s, a) = \mathcal{B}^\pi \hat{Q}^k(s, a) - \alpha \frac{\mu(a|s)}{\hat{\pi}_\beta(a|s)}.
$$

마이너스 되는 항의 요소들이 모두 양수이므로 각 iteration은 next Q-value를 underestimate한다. 즉, $\hat{Q}^{k+1} \leq \mathcal{B}^\pi \hat{Q}^k$

다음으로는 empirical Bellman operator $\hat{\mathcal{B}}^\pi$와 actual Bellman operator $\mathcal{B}^\pi$에 의한 차이를 bound할 수 있다. Empirical Bellman operator의 경우, 얻은 데이터 샘플을 통해 Q-value를 추정하는 방식이고, actual Bellman operator의 경우, transition matrix에 대한 정보와 $\pi$에 의해 Q-value를 추정하게 된다.

$$
(\mathcal{B}^\pi Q)(s, a) = r(s, a) + \gamma \mathbb{E}_{s' \sim P(s' \mid s, a), a' \sim \pi(a' \mid s')} \big[ Q(s', a') \big] \\
(\hat{\mathcal{B}}^\pi Q)(s, a) = r + \gamma Q(s', a'),\quad a'\sim\pi(\cdot|s)
$$

여기서 reward와 transition에 대해 concentration property가 성립한다고 가정하면 다음을 만족한다.

$$
|r - r(s, a)| \leq \frac{C_{r, \delta}}{\sqrt{| \mathcal{D}(s, a) |}}, \quad
\| \hat{T}(s' \mid s, a) - T(s' \mid s, a) \|_1 \leq \frac{C_{T, \delta}}{\sqrt{| \mathcal{D}(s, a) |}}.
$$

$Q^\pi(s, a) \leq \sum_{t=0}^\infty \gamma^t R_{\text{max}} = \frac{R_{\text{max}}}{1 - \gamma}$임을 이용하면

$$
\begin{aligned}
\left| \left( \hat{\mathcal{B}}^\pi \hat{Q}^k \right) - \left( \mathcal{B}^\pi \hat{Q}^k \right) \right| 
&= \left| \left( r - r(s, a) \right) + \gamma \sum_{s'} \left( \hat{T}(s' \mid s, a) - T(s' \mid s, a) \right) 
\mathbb{E}_{\pi(a' \mid s')} \left[ \hat{Q}^k(s', a') \right] \right| \\
&\leq |r - r(s, a)| + \gamma \sum_{s'} \left| \hat{T}(s' \mid s, a) - T(s' \mid s, a) \right| 
\mathbb{E}_{\pi(a' \mid s')} \left[ \hat{Q}^k(s', a') \right] \\
&\leq \frac{C_{r, \delta} + \gamma C_{T, \delta} R_{\text{max}} / (1 - \gamma)}{\sqrt{| \mathcal{D}(s, a) |}}.
\end{aligned}
$$

다시 $\hat{Q}^{k+1}(s, a) = \mathcal{B}^\pi \hat{Q}^k(s, a) - \alpha \frac{\mu(a\|s)}{\hat{\pi}_\beta(a\|s)}$와 결합하면

$$
\hat{Q}^{k+1}(s, a) = \hat{\mathcal{B}}^\pi \hat{Q}^k(s, a) - \alpha \frac{\mu(a \mid s)}{\hat{\pi}_\beta(a \mid s)} 
\leq \mathcal{B}^\pi \hat{Q}^k(s, a) - \alpha \frac{\mu(a \mid s)}{\hat{\pi}_\beta(a \mid s)} 
+ \frac{C_{r, T, \delta} R_{\text{max}}}{(1 - \gamma) \sqrt{| \mathcal{D}(s, a) |}}.
$$

Banach's fixed point theorem에 의해 $Q^\pi=\mathcal{B}^\pi Q^\pi = r + \gamma P^\pi Q^\pi$를 만족하는 fixed point $Q^\pi$가 존재한다. 이전 방정식과 합치면

$$
\begin{aligned}
\hat{Q}^\pi &\leq \mathcal{B}^\pi \hat{Q}^\pi - \alpha \frac{\mu(a \mid s)}{\hat{\pi}_\beta(a \mid s)} 
+ \frac{C_{r, T, \delta} R_{\text{max}}}{(1 - \gamma) \sqrt{| \mathcal{D} |}} \\
\hat{Q}^\pi &\leq \left( I - \gamma P^\pi \right)^{-1} 
\left[ R - \alpha \frac{\mu}{\hat{\pi}_\beta} 
+ \frac{C_{r, T, \delta} R_{\text{max}}}{1 - \gamma \sqrt{| \mathcal{D} |}} \right]
\end{aligned}
$$


$$
\hat{Q}^\pi(s, a) \leq Q^\pi(s, a) - \alpha 
\left[ \left( I - \gamma P^\pi \right)^{-1} \frac{\mu}{\hat{\pi}_\beta} \right](s, a) 
+ \left[ \left( I - \gamma P^\pi \right)^{-1} 
\frac{C_{r, T, \delta} R_{\text{max}}}{(1 - \gamma) \sqrt{| \mathcal{D} |}} \right](s, a).
$$

즉, 적절한 $\alpha$에 대해 $\hat{Q}^\pi(s, a) \leq Q^\pi(s, a)$가 성립함을 알 수 있다. 추가적으로 under-estimation을 성립하게 하는 $\alpha$의 범위에 대해서도 생각해 볼 수 있다. $\left( I - \gamma P^\pi \right)^{-1}$가 non-negative entry matrix이기 때문에 소거할 수 있다. 이제 min, max연산을 통해 가장 극단적인 경우를 생각해 보자.

$$
\alpha \cdot \min_{s, a} \left[ \frac{\mu(a \mid s)}{\hat{\pi}_\beta(a \mid s)} \right] 
\geq \max_{s, a} \frac{C_{r, T, \delta} R_{\text{max}}}{(1 - \gamma) \sqrt{| \mathcal{D}(s, a) |}} \\
\implies 
\alpha \geq \max_{s, a} \frac{C_{r, T, \delta} R_{\text{max}}}{(1 - \gamma) \sqrt{| \mathcal{D}(s, a) |}} 
\cdot \min_{s, a} \left[ \frac{\mu(a \mid s)}{\hat{\pi}_\beta(a \mid s)} \right]^{-1}.
$$

핵심적인 부분은 $\| \mathcal{D}(s, a) \|$이 아주 큰 값일 경우 $\alpha$는 충분히 작아도 lower bound를 만족시킨다. 더 나아가 $\hat{\mathcal{B}}^\pi = \mathcal{B}^\pi$인 경우 $C_{r, T, \delta}\approx 0$가 되어 $\alpha \geq 0$만 만족하게 되면 lower bound 성질을 만족시킬 수 있다.

</div>
</details>
\
Theorem 3.1. 에 의하면 적절한 $\alpha$에 대해 $\hat{Q}^\pi(s,a)\leq Q^\pi(s,a)$을 만족하게 된다. 즉, $\hat{Q}^\pi := \lim_{k \to \infty} \hat{Q}^k$는 $Q^\pi$의 lower bound를 형성한다.

여기서 lower bound를 tighten하기 위한 추가적인 작업이 들어간다. $\hat{\pi}_\beta$의 분포를 따르는 action에 대해서는 Q-value를 최대화하도록 Q를 업데이트시킨다.

$$
\hat{Q}^{k+1} \leftarrow \arg\min_{Q} \, \alpha \cdot \left( \mathbb{E}_{s \sim \mathcal{D}, a \sim \mu(a|s)} [Q(s, a)] 
- \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi_\beta(a|s)} [Q(s, a)] \right) 
+ \frac{1}{2} \, \mathbb{E}_{s, a, s' \sim \mathcal{D}} \left[ \left( Q(s, a) - \hat{\mathcal{B}}^\pi \hat{Q}^k(s, a) \right)^2 \right]
$$

> $\textbf{Theorem 3.2.}$ (Equation results in a tighter lower bound). The value of the policy under the Q-function from the above Equation, $\hat{V}^\pi(s) = \mathbb{E}\_{\pi(a\|s)} \left[ \hat{Q}^\pi(s, a) \right]$, lower-bounds the true value of the policy obtained via exact policy evaluation, 
$V^\pi(s) = \mathbb{E}\_{\pi(a\|s)} \left[ Q^\pi(s, a) \right]$, when $\mu = \pi$, according to:  
> <div align="center">
> $$
  \forall s \in \mathcal{D}, \quad \hat{V}^\pi(s)
  \leq V^\pi(s) - \alpha \left[ \left( I - \gamma P^\pi \right)^{-1}
  \mathbb{E}_\pi \left[ \frac{\pi}{\hat{\pi}_\beta} - 1 \right] \right](s) + \left[ \left( I - \gamma P^\pi \right)^{-1}
  \frac{C_{r, T, \delta} R_{\text{max}}}{(1 - \gamma) \sqrt{| \mathcal{D} |}} \right](s).
  $$
> </div>


<details>
<summary>Proof of Theorem 3.2.</summary>
<div markdown="1">

Iteration은 다음과 같이 진행된다.

$$
\hat{Q}^{k+1} \leftarrow \arg\min_{Q} \, \alpha \cdot \left( \mathbb{E}_{s \sim \mathcal{D}, a \sim \mu(a|s)} [Q(s, a)] 
- \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi_\beta(a|s)} [Q(s, a)] \right) 
+ \frac{1}{2} \, \mathbb{E}_{s, a, s' \sim \mathcal{D}} \left[ \left( Q(s, a) - \hat{\mathcal{B}}^\pi \hat{Q}^k(s, a) \right)^2 \right]
$$

Theorem 3.1.과 동일한 방식으로 최소가 되는 $Q^{k+1}$을 찾으면 다음과 같다.

$$
\forall s, a, k, \quad \hat{Q}^{k+1}(s, a) = \mathcal{B}^\pi \hat{Q}^k(s, a) 
- \alpha \left[ \frac{\mu(a \mid s)}{\pi_\beta(a \mid s)} - 1 \right].
$$

이번에는 이전과 다르게 alpha를 달아둔 항이 positive라는 보장이 없다. 만약 $\mu(a \mid s) < \pi_\beta(a \mid s)$인 $(s,a)$ pair가 존재한다면 어떤 $(s,a)$에 대해서는 $\hat{Q}^{k+1}(s, a) \geq Q^{k+1}(s, a)$를 만족하는 경우가 발생할 수 있다. (간단한 3 state, 2 action MDP에서도 이를 보일 수 있다고 한다.) 따라서 이 iteration에서 Q-function의 lower bound는 더 이상 보장받지 못하게 된다.

하지만 $V^{k+1}$에서는 여전히 under-estimation됨을 확인할 수 있다. 위 식에서 

$$
\hat{V}^{k+1}(s) := \mathbb{E}_{a \sim \pi(a \mid s)} \left[ \hat{Q}^{k+1}(s, a) \right] 
= \mathcal{B}^\pi \hat{V}^k(s) - \alpha \mathbb{E}_{a \sim \pi(a \mid s)} 
\left[ \frac{\mu(a \mid s)}{\pi_\beta(a \mid s)} - 1 \right].
$$

이고, $\alpha$항에 있는 $$D_{\text{CQL}}(s) := \sum_a \pi(a \mid s) \left[ \frac{\mu(a \mid s)}{\pi_\beta(a \mid s)} - 1 \right]$을 정의하고 $\mu(a \mid s) = \pi(a \mid s)$일 때 positive함을 보일 수 있다.

$$
\begin{aligned}
D_{\text{CQL}}(s) &:= \sum_a \pi(a \mid s) \left[ \frac{\mu(a \mid s)}{\pi_\beta(a \mid s)} - 1 \right]\\
&= \sum_a \left( \pi(a \mid s) - \pi_\beta(a \mid s) + \pi_\beta(a \mid s) \right) 
\left[ \frac{\mu(a \mid s)}{\pi_\beta(a \mid s)} - 1 \right]\\
&= \sum_a (\pi(a \mid s) - \pi_\beta(a \mid s)) 
\left[ \frac{\mu(a \mid s)}{\pi_\beta(a \mid s)} - 1 \right] 
+ \sum_a \pi_\beta(a \mid s) \left[ \frac{\mu(a \mid s)}{\pi_\beta(a \mid s)} - 1 \right]\\
&= \sum_a \frac{(\pi(a \mid s) - \pi_\beta(a \mid s))^2}{\pi_\beta(a \mid s)} +\sum_a \left[ \mu(a \mid s) - \pi_\beta(a \mid s) \right]
\geq 0, \quad
\text{since } \sum_a \pi(a \mid s) = \sum_a \pi_\beta(a \mid s) = 1.
\end{aligned}
$$

이 때, $\pi_\beta(a \mid s)=\pi(a \mid s)$이면 $D_{\text{CQL}}(s)=0$을 만족하게 된다. 지금까지의 내용을 정리하면 iteration이 value-estimation에 대해 under-estimation한다는 사실을 알 수 있다. 즉, $\hat{V}^{k+1}(s) \leq \mathcal{B}^\pi \hat{V}^k(s).$

이전과 마찬가지로 iteration에 대해 fixed point $V^\pi=\mathcal{B}^\pi $를 대입할 수 있다.

$$
\hat{V}^\pi(s) = V^\pi(s) - \alpha \left[
\underbrace{\left[ \left( I - \gamma P^\pi \right)^{-1} \right]}_{\text{non-negative entries}} 
\mathbb{E}_\pi 
\underbrace{\left[ \frac{\pi}{\pi_\beta} - 1 \right]}_{\geq 0} \right] (s).
$$

하지만 이 버전은 actual bellman operator $\mathcal{B}$로 업데이트 했을 때의 식이고, 실제로는 empirical bellman operator $\hat{\mathcal{B}}$를 사용하게 된다. 하지만 이전 theorem에서 보았듯 concentration property를 가정하면 그 결과는 비슷한 형태를 보인다.

$$
\hat{V}^\pi(s) \leq V^\pi(s) - \alpha 
\left[ \left( I - \gamma P^\pi \right)^{-1} 
\mathbb{E}_\pi \left[ \frac{\pi}{\hat{\pi}_\beta} - 1 \right] \right](s) 
+ \left[ \left( I - \gamma P^\pi \right)^{-1} 
\frac{C_{r, T, \delta} R_{\text{max}}}{(1 - \gamma) \sqrt{| \mathcal{D} |}} \right](s).
$$

$\alpha$의 범위에 관해서도 완전히 동일한 방식으로 보일 수 있다.

$$
\alpha \geq 
\max_{s, a \in \mathcal{D}} \frac{C_{r, T} R_{\text{max}}}{(1 - \gamma) \sqrt{| \mathcal{D}(s, a) |}} 
\cdot \min_{s \in \mathcal{D}} 
\left[ \sum_a \pi(a \mid s) \left( \frac{\pi(a \mid s)}{\hat{\pi}_\beta(a \mid s)} - 1 \right) \right]^{-1}.
$$


</div>
</details>
\
Theorem 3.2.에 의해 $\mu=\pi$를 가정하면 $\hat{V}^\pi(s)\leq V^\pi(s)$를 만족하게 된다. point-wise lower bound가 아닌 expectation에 대한 lower bound를 형성하여 extra under-estimation을 막는다. $\hat{V}^\pi(s)=\mathbb{E}_{\pi(a\|s)} \left[ \hat{Q}^\pi(s, a) \right]$에 대해 minimization을 취해주면.

Theorem 증명에 의해 더 많은 양의 데이터 $\|\mathcal{D}(s, a)\|$가 available하면 $\alpha$가 제공하는 lower bound가 감소하게 되고, 무한히 많은 양의 데이터를 가정하면 아주 작은 $\alpha$로도 lower bound를 보장할 수 있게 된다.

각 항의 역할을 정리하면
- 첫번째 항은 $\mu$에서 sample된 action의 Q-value를 최대화하도록 하며, true Q-function $Q^\pi$를 lower bound한다.
- 두번째 항은 $\hat{\pi}_\beta$에서 sample된 action의 Q-value를 최대화하는데 사용되며, target policy $\pi$의 expected Q-value(즉, $V^\pi$)의 lower bound한다.
- 세번째 항은 기존 value based Q-learning이다.

### Conservative Q-learning for offline RL

이전 section에서 $\mu=\pi$ 가정을 통해 $V^\pi$를 lower bound하는 iteration을 정의할 수 있었다. 기존 policy iteration처럼 각 step 마다 수렴할때까지 policy evaluation을 반복하고 이후에 policy improvement로 policy update하는 방식은 너무 비용이 많이 든다. 따라서 수식에 regularization term을 두고, 이 term으로 KL divergence를 사용한다. 가장 기본적인 형태를 $\text{CQL}(\mathcal{R})$로 제시한다.

$$
\begin{aligned}
\min_Q \max_\mu \,\, &\alpha 
\left( 
\mathbb{E}_{s \sim \mathcal{D}, a \sim \mu(a \mid s)} \left[ Q(s, a) \right] 
- \mathbb{E}_{s \sim \mathcal{D}, a \sim \hat{\pi}_\beta(a \mid s)} \left[ Q(s, a) \right] 
\right)\\
&+ \frac{1}{2} \mathbb{E}_{s, a, s' \sim \mathcal{D}} 
\left[ 
\left( Q(s, a) - \hat{\mathcal{B}}^\pi \hat{Q}^k(s, a) \right)^2 
\right]
+ R(\mu) \quad (\text{CQL}(R)).
\end{aligned}
$$

Regularization term은 여러 가지 선택지가 있는데, 첫째로 엔트로피 항을 사용할 수 잇다. 그리고 식을 최대화하는 $\mu$는 Lagrangian을 이용해서 구할 수 있다. (논문에서는 KKT condition 구하듯이 하는데, convexity 가정 없이 써도 되는지는 잘 모르겠다...)

<details>
<summary>$\mu$를 구하는 과정</summary>
<div markdown="1">
이 식은 최적화를 포함한 문제로, 확률 분포 \\(\mu(x)\\)에 대해 기대값과 엔트로피를 최대화하는 방식입니다. 아래에 유도 과정을 정리해 보겠습니다:

### 최적화 문제
\\[
\max_{\mu} \mathbb{E}_{x \sim \mu(x)}[f(x)] + \mathcal{H}(\mu), \quad \text{s.t.} \quad \sum_x \mu(x) = 1, \quad \mu(x) \geq 0 \, \forall x,
\\]
여기서:
- \\( \mathbb{E}_{x \sim \mu(x)}[f(x)] = \sum_x \mu(x) f(x) \\)는 기대값,
- \\(\mathcal{H}(\mu) = -\sum_x \mu(x) \log \mu(x)\\)는 \\(\mu(x)\\)의 엔트로피입니다.

#### 목적 함수
목적 함수는 다음과 같습니다:
\\[
\mathcal{L}(\mu) = \sum_x \mu(x) f(x) - \sum_x \mu(x) \log \mu(x).
\\]

#### 제약 조건
제약 조건은 두 가지입니다:
1. \\(\sum_x \mu(x) = 1\\) (확률 분포의 합은 1),
2. \\(\mu(x) \geq 0\\) (확률 분포는 음이 될 수 없음).

이를 라그랑주 승수법으로 풀기 위해 라그랑지안 \\( \mathcal{L} \\)을 정의합니다.

### 라그랑지안 정의
\\[
\mathcal{L}(\mu, \lambda) = \sum_x \mu(x) f(x) - \sum_x \mu(x) \log \mu(x) + \lambda \left(1 - \sum_x \mu(x)\right).
\\]

여기서 \\(\lambda\\)는 \\(\sum_x \mu(x) = 1\\)에 대한 라그랑주 승수입니다.

### 최적화 조건
라그랑지안 \\(\mathcal{L}\\)에 대해 \\(\mu(x)\\)를 편미분하여 최적 조건을 찾습니다:
\\[
\frac{\partial \mathcal{L}}{\partial \mu(x)} = f(x) - \log \mu(x) - 1 - \lambda = 0.
\\]

이 식을 정리하면:
\\[
\log \mu(x) = f(x) - \lambda - 1.
\\]

양변에 지수를 취합니다:
\\[
\mu(x) = \exp(f(x) - \lambda - 1).
\\]

여기서 \\(\lambda\\)는 정규화 상수 \\(Z\\)를 포함합니다.

### 정규화 조건
확률 분포 \\(\mu(x)\\)의 합이 1이어야 하므로:
\\[
\sum_x \mu(x) = \sum_x \exp(f(x) - \lambda - 1) = 1.
\\]

이를 통해 \\(\exp(-\lambda - 1)\\)는 다음과 같이 정의됩니다:
\\[
\exp(-\lambda - 1) = \frac{1}{Z}, \quad \text{where } Z = \sum_x \exp(f(x)).
\\]

따라서:
\\[
\mu(x) = \frac{\exp(f(x))}{Z}.
\\]

### 최적 분포
최적의 확률 분포는 다음과 같습니다:
\\[
\mu^*(x) = \frac{1}{Z} \exp(f(x)),
\\]
여기서 \\(Z = \sum_x \exp(f(x))\\)는 정규화 상수입니다.

이 과정을 통해 제시된 식을 유도할 수 있습니다.
</div>
</details>
\
$\mu^\*$의 계산 결과가 softmax 형태로 나타난다. 논문에서는 이 variant를 $\text{CQL}(\mathcal{H})$로 소개한다. $\mu^\*$를 대입하게 되면 다음과 같다.

$$
\min_Q \alpha \mathbb{E}_{s \sim \mathcal{D}} \left[ 
    \log \sum_a \exp(Q(s, a)) - \mathbb{E}_{a \sim \pi_\beta(a|s)} \left[ Q(s, a) \right] 
\right] 
+ \frac{1}{2} \mathbb{E}_{s, a, s' \sim \mathcal{D}} \left[ 
    \left( Q - \mathcal{B}^\pi \hat{Q}^k \right)^2 
\right].
$$

둘째로 regularization term을 KL divergence로 둔다면 같은 방법으로 $\mu^\*(\mathbf{x}) = \frac{1}{Z} \rho(\mathbf{x}) \exp(f(\mathbf{x}))$가 된다. 이 variant를 $\text{CQL}(\mathcal{\rho})$로 소개한다. (사실, 여기서 $\rho(\mathbf{x})$를 uniform 분포로 둔다면 $\text{CQL}(\mathcal{H})$의 $\mu^\*$와 동일해진다. 엔트로피 자체가 uniform 분포일 때 최대이니까 직관에 들어맞기도 하다.)

$$
\min_Q \alpha \mathbb{E}_{s \sim d^{\pi_\beta}(s)} \left[
    \mathbb{E}_{a \sim \rho(a|s)} \left[
      Q(s, a)
        \frac{ \exp(Q(s, a))}{Z}
    \right] 
    - \mathbb{E}_{a \sim \pi_\beta(a|s)} \left[ Q(s, a) \right]
\right]
+ \frac{1}{2} \mathbb{E}_{s, a, s' \sim \mathcal{D}} \left[
    \left( Q - \mathcal{B}^{\pi_k} \hat{Q}^k \right)^2
\right].
$$

$\text{CQL}(\rho)$에서는 $\rho(a\|s)$로 이전 policy $\hat{\pi}^{k-1}$을 선택할 수 있으며, 이 경우 an exponential weighted average of Q-value로 첫번째 항이 바뀌게 된다. $\text{CQL}(\rho)$로 학습하는 경우 higher-dimension에서 좀 더 robust한 학습을 하게 됨을 empirically 확인할 수 있다.

Theorem 3.3.에서는 $\pi^k$의 action-distribution 하에서 학습될 때 $\pi^{k+1}$이 충분히 $\pi^k$에 충분히 가깝게만 업데이트된다면 actual Q-function을 lower bound하는 Q-value estimates를 학습할 수 있음을 보인다.

> $\textbf{Theorem 3.3 (CQL learns lower-bounded Q-values).}$
Let $\pi\_{\hat{Q}\_k}(a|s) \propto \exp(\hat{Q}^k(s, a))$ and assume that 
$D\_{\text{TV}}(\hat{\pi}^{k+1}, \pi\_{\hat{Q}\_k}) \leq \varepsilon$ 
(i.e., $\hat{\pi}^{k+1}$ changes slowly w.r.t. $\pi\_{\hat{Q}\_k}$). 
Then, the policy value under $\pi\_{\hat{Q}\_k}$ lower-bounds the actual policy value, 
$\hat{V}^{k+1}(s) \leq V^{k+1}(s), \forall s$ if
> <div align="center">
> $$
  \mathbb{E}_{\pi_{\hat{Q}_k}(a|s)} \left[
      \frac{\pi_{\hat{Q}_k}(a|s)}{\pi_\beta(a|s)} - 1
  \right] 
  \geq 
  \max_{a \,:\, \pi_\beta(a|s) > 0} \left( 
      \frac{\pi_{\hat{Q}_k}(a|s)}{\pi_\beta(a|s)}
  \right) \cdot \varepsilon.
  $$
> </div>

<details>
<summary>Proof of theorem 3.3.</summary>
<div markdown="1">
Theorem 3.2.에서 나왔던 식을 recap하면,
$$
\hat{Q}^{k+1}(s, a) = \mathcal{B}^\pi \hat{Q}^k(s, a) - \alpha \left[ 
    \frac{\mu(a|s)}{\pi_\beta(a|s)} - 1 
\right]
$$

$$
\begin{align*}
\mathbb{E}_{\hat{\pi}^{k+1}(a|s)} \left[ \hat{Q}^{k+1}(s, a) \right] 
&= \mathbb{E}_{\hat{\pi}^{k+1}(a|s)} \left[ \mathcal{B}^\pi \hat{Q}^k(s, a) \right] 
- \mathbb{E}_{\hat{\pi}^{k+1}(a|s)} \left[ 
    \frac{\pi_{\hat{Q}^k}(a|s)}{\pi_\beta(a|s)} - 1 
\right] \\
&= \mathbb{E}_{\hat{\pi}^{k+1}(a|s)} \left[ \mathcal{B}^\pi \hat{Q}^k(s, a) \right] 
- \underbrace{\mathbb{E}_{\pi_{\hat{Q}^k}(a|s)} \left[ 
    \frac{\pi_{\hat{Q}^k}(a|s)}{\pi_\beta(a|s)} - 1 
\right]}_{\text{(underestimation, (a))}} \\
&\quad + \sum_a \underbrace{\left( \pi_{\hat{Q}^k}(a|s) - \hat{\pi}^{k+1}(a|s) \right)}_{\text{(b), } \leq D_{\text{TV}}(\pi_{\hat{Q}^k}, \hat{\pi}^{k+1})} 
\frac{\pi_{\hat{Q}^k}(a|s)}{\pi_\beta(a|s)}
\end{align*}
$$

여기서 total variance distance $D\_{\text{TV}}(P, Q) = \sup\_{A \subset \mathcal{X}} \|P(A) - Q(A)\|$이다. 이전 theorem들의 증명들에서 뒤쪽에 붙은 항이 0보다 작을 때 under-estimation을 유발하는 것을 그대로 활용하면 (a)가 (b)보다 큰 경우에 그것이 항상 성립하게 됨을 알 수 있다.

$$
- \mathbb{E}_{\pi_{\hat{Q}^k}(a|s)} \left[ 
    \frac{\pi_{\hat{Q}^k}(a|s)}{\pi_\beta(a|s)} - 1 
\right] + \sum_a \left( \pi_{\hat{Q}^k}(a|s) - \hat{\pi}^{k+1}(a|s) \right)
\frac{\pi_{\hat{Q}^k}(a|s)}{\pi_\beta(a|s)} \leq 0 \\
\mathbb{E}_{\pi_{\hat{Q}^k}(a|s)} \left[ 
    \frac{\pi_{\hat{Q}^k}(a|s)}{\pi_\beta(a|s)} - 1 
\right] \geq
D_{\text{TV}}(\pi_{\hat{Q}^k}, \hat{\pi}^{k+1})
\max_a \frac{\pi_{\hat{Q}^k}(a|s)}{\pi_\beta(a|s)}  \geq
\sum_a \left( \pi_{\hat{Q}^k}(a|s) - \hat{\pi}^{k+1}(a|s) \right) 
\frac{\pi_{\hat{Q}^k}(a|s)}{\pi_\beta(a|s)} 
$$

따라서 증명이 완료된다.

</div>
</details>
\
Policy가 천천히 업데이트된다면 $\varepsilon$의 값은 작아지고 이에 따라 lower bound를 얻을 수 있다.

CQL 알고리즘은 gap-expanding의 특성을 지닌다. Actual Q-value gap 보다 estimated Q-value의 gap이 더 크다는 것인데, 이것이 의미하는 바는 policy가 in-distribution action을 좀 더 선호하도록 업데이트 되도록 한다.

> $\textbf{Theorem 3.4 (CQL is gap-expanding).}$ At any iteration \\(k\\), CQL expands the difference in expected Q-values under the behavior policy \\(\pi\_\beta(a\|s)\\) and \\(\mu\_k\\), such that for large enough values of \\(\alpha\_k\\), we have that \\(\forall s\\), \\
> <div align="center">
> $$
\mathbb{E}_{\pi_\beta(a|s)} \left[ \hat{Q}^k(s, a) \right] - \mathbb{E}_{\mu_k(a|s)} \left[ \hat{Q}^k(s, a) \right] >
\mathbb{E}_{\pi_\beta(a|s)} \left[ Q^k(s, a) \right] - \mathbb{E}_{\mu_k(a|s)} \left[ Q^k(s, a) \right].
$$
> </div>

<details>
<summary>Proof of theorem 3.4.</summary>
<div markdown="1">

Q-value iterationa at $k$

$$
\hat{Q}^{k+1}(s, a) = \mathcal{B}^\pi \hat{Q}^k(s, a) 
- \alpha_k \frac{\mu_k(a|s) - \pi_\beta(a|s)}{\pi_\beta(a|s)}.
$$

sampling error가 없다고 가정($\mathcal{B}^\pi=\hat{\mathcal{B}}^\pi$)하면

$$
\mathbb{E}_{a \sim \mu_k(a|s)} \left[ \hat{Q}^{k+1}(s, a) \right] 
= \mathbb{E}_{a \sim \mu_k(a|s)} \left[ \mathcal{B}^\pi \hat{Q}^k(s, a) \right] 
- \alpha_k \underbrace{\mu_k^T \left( \frac{\mu_k(a|s) - \pi_\beta(a|s)}{\pi_\beta(a|s)} \right)}_{= \Delta_k \geq 0, \text{ by proof of Theorem 3.2}}.
$$

맨 뒤 term의 positiveness는 $D_{\text{CQL}}(s)\geq 0$에 의해 확인되었다. 특히 $\mu_k=\pi_\beta$인 경우 $D_{\text{CQL}}(s)=0$인 것도 확인할 수 있다.

$$
\mathbb{E}_{a \sim \pi_\beta(a|s)} \left[ \hat{Q}^{k+1}(s, a) \right] 
= \mathbb{E}_{a \sim \pi_\beta(a|s)} \left[ \mathcal{B}^\pi \hat{Q}^k(s, a) \right] 
- \alpha_k \underbrace{\pi_\beta^T \left( \frac{\mu_k(a|s) - \pi_\beta(a|s)}{\pi_\beta(a|s)} \right)}_{= 0\text{, since numerator is 0}}.
$$

위 식 두개를 결합하면,

$$
\mathbb{E}_{a \sim \pi_\beta(a|s)} \left[ \hat{Q}^{k+1}(s, a) \right] 
- \mathbb{E}_{a \sim \mu_k(a|s)} \left[ \hat{Q}^{k+1}(s, a) \right]
= \mathbb{E}_{a \sim \pi_\beta(a|s)} \left[ \mathcal{B}^\pi \hat{Q}^k(s, a) \right] 
- \mathbb{E}_{a \sim \mu_k(a|s)} \left[ \mathcal{B}^\pi \hat{Q}^k(s, a) \right] 
- \alpha_k \Delta_k.
$$

$\mathbb{E}\_{\pi_\beta(a\|s)} \left[ Q^{k+1}(s, a) \right] - \mathbb{E}\_{\mu\_k(a\|s)} \left[ Q^{k+1}(s, a) \right]$을 우변에 더하면

$$
\begin{aligned}
\mathbb{E}_{a \sim \pi_\beta(a|s)} \left[ \hat{Q}^{k+1}(s, a) \right] 
- \mathbb{E}_{a \sim \mu_k(a|s)} \left[ \hat{Q}^{k+1}(s, a) \right]
&= \mathbb{E}_{a \sim \pi_\beta(a|s)} \left[ \hat{Q}^{k+1}(s, a) \right] 
- \mathbb{E}_{a \sim \mu_k(a|s)} \left[ \hat{Q}^{k+1}(s, a) \right]\\
&+ \left( \mu_k(a|s) - \pi_\beta(a|s) \right)^T 
\underbrace{\left[ \mathcal{B}^{\pi^k} \left( \hat{Q}^k - Q^k \right)(s, \cdot) \right]}_{(a)}
- \alpha_k \Delta_k.
\end{aligned}
$$

(a)와 $\alpha_k$항이 0보다 크도록 하면 식을 만족시킬 수 있다. $\alpha_k$를 적절히 조정하면 식을 만족시킬 수 있게 된다.

$$
\alpha_k > \max \left( 
\frac{
    \left( \pi_\beta(a|s) - \mu_k(a|s) \right)^T 
    \left[ \mathcal{B}^{\pi^{k+1}} \left( \hat{Q}^k - Q^k \right)(s, \cdot) \right]
}{\Delta_k}, 0 \right).
$$

따라서 증명이 완료된다. (sampling error가 있는 경우에도 여전히 증명할 수 있다)

</div>
</details>
\
이번 section의 요약은 다음과 같다.
- $\alpha$값을 적절히 조절하여 Q-value의 lower bound를 학습할 수 있다.
- In-distribution과 out-of-distribution의 gap을 over-estimate한다.

### Safe policy improvement guarantees

CQL은 실제로 잘 정의된 penalized RL empirical objective임을 보일 수 있다. $J(\pi,M)$을 expected discounted return으로 둘 때 CQL은 empirical MDP를 optimize하게 된다.

> $\textbf{Theorem 3.5.}$ Let \\(\hat{Q}^\pi\\) be the fixed point of Equation 2, then \\(\pi^*(a\|s) := \arg\max_\pi \mathbb{E}\_{s \sim \rho(s)} \left[ V^\pi(s) \right]\\) is equivalently obtained by solving:
> <div align="center">
> $$
\pi^*(a|s) \leftarrow \arg\max_\pi \; J(\pi, \hat{M}) 
- \alpha \frac{1}{1-\gamma} \mathbb{E}_{s \sim d^{\pi}_{\hat{M}}(s)} \left[ D_{\text{CQL}}(\pi, \pi_\beta)(s) \right], 
$$
> </div>
> where
$ D\_{\text{CQL}}(\pi, \pi_\beta)(s) := \sum_a \pi(a\|s) \cdot \left( \frac{\pi(a\|s)}{\pi_\beta(a\|s)} - 1 \right). $

그리고 CQL은 $\zeta$-safe policy improvement임을 보일 수 있다.

> $\textbf{Theorem 3.6.}$ Let \\(\pi^\*(a\|s)\\) be the policy obtained by optimizing Equation 5. Then, the policy \\(\pi^\*(a\|s)\\) is a \\(\zeta\\)-safe policy improvement over \\(\pi\_\beta\\) in the actual MDP \\(M\\), i.e., \\(J(\pi^*, M) \geq J(\pi\_\beta, M) - \zeta\\) with high probability \\(1-\delta\\), where \\(\zeta\\) is given by,
> <div align="center">
> $$
\zeta = 2 \left( 
    \frac{C_{r, \delta}}{1-\gamma} 
    + \frac{\gamma R_{\max} C_{T, \delta}}{(1-\gamma)^2}
\right) 
\mathbb{E}_{s \sim d^{\pi^*}_M(s)} \left[
    \frac{\sqrt{|A|}}{\sqrt{|D(s)|}} 
    \sqrt{D_{\text{CQL}}(\pi^*, \pi_\beta)(s) + 1} 
    - \underbrace{\left( J(\pi^*, \hat{M}) - J(\pi_\beta, \hat{M}) \right)}_{\geq \alpha \frac{1}{1-\gamma} \mathbb{E}_{s \sim d^{\pi^*}_{\hat{M}}(s)} \left[ D_{\text{CQL}}(\pi^*, \pi_\beta)(s) \right]} 
\right].
$$
> </div>

첫째 항은 $\hat{M}$과 $M$의 mismatch로 인한 policy performance drop을 의미하고, 둘째 항은 CQL로 인한 policy performance increase를 의미한다.

## Practical algorithm and implementation details

<center>
<img src='{{"assets/images/CQL/cql1.png" | relative_url}}' width="50%">
<figcaption style="text-align: center;"></figcaption>
</center>

알고리즘 자체는 Q-function update term에서만 차이가 있다. $\alpha$의 경우, continuous domain에서는 Lagrangian dual gradient descent를 이용하여 자동으로 tuned 시켰고, discrete domain에서는 고정된 값을 사용하였다.


## Related work

### Off-policy evaluation

초기 연구는 MC return에 IS를 이용하였다. 최근 연구는 marginalized IS를 사용하는 연구들이 있다.

### Offline RL

기존의 offline RL은 behavior policy에 contraint를 두는 방식으로 OOD action을 방지하고자 했다. 이러한 방법은 behavior policy를 위한 estimated model이 (CQL은 불필요) 추가적으로 필요하다는 단점이 있다. 다른 방법들로는 Q-value prediction의 uncertainty를 측정하는 방법이 있으나 online RL에서 주로 사용되었고, offline RL에서는 그것의 정확도 요구치가 높고 offline 데이터셋의 특성(데이터 내에 없는 행동에 대한 추가적인 데이터 수집이 어려운 부분) 때문에 성능이 떨어지는 경향을 보인다. Robust MDP류 연구는 policy improvement에 대해 과도하게 conservative하다.

### Theretical results

Safe policy improvement와 관련 연구를 찾아보면 좋을 것 같다.


## Experimental evaluation

<center>
<img src='{{"assets/images/CQL/cql2.png" | relative_url}}' width="80%">
<figcaption style="text-align: center;">gym env</figcaption>
</center>

## Discussion



