---
title: "(DDPO) TRAINING DIFFUSION MODELS WITH REINFORCEMENT LEARNING"
last_modified_at: 2025-01-24
categories:
  - paper_review
tags:
  - Reinforcement Learning
  - RL
  - Diffusion model
  - Overoptimization
  - Generalization
  - DDPO
  - RWR
excerpt: "DDPO paper"
use_math: true
classes: wide
---

> CVPR 2022 (Oral). [[Paper](https://arxiv.org/abs/2305.13301)] [[Github](https://github.com/jannerm/ddpo)]  
> Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine  
> 12 Aug 2022

## Summary

많은 생성 모델의 활용 사례에서는 목적 함수로써의 likelihood 자체에 초점을 두기보다는 인간의 인식을 바탕으로 이미지가 얼마나 잘 나왔는지, 약효가 얼마나 뛰어난지 등에 초점을 둔다. 이 논문에서는 이러한 목표들에 좀 더 집중하여 diffusion model을 학습하는 denoising diffusion policy optimizaiton(DDPO)를 제시한다. DDPO는 text-to-image diffusion model에서 image compressibility, aesthetic quality와 같이 다루기 어려운 objective에 대해서도 잘 adapt한다. 또한, 추가적인 데이터 수집이나 인간의 개입 없이 vision-language model의 피드백으로부터 prompt-image alignment을 점차 개선해 나갈 수 있다.

## Introduction

Diffusion model의 핵심은 순차적인 denoising 과정을 통해 반복적으로 prior 분포를 target 분포로 변환하는 것이다. maximum likelihood estimation 문제이며, objective는 log-likelihood의 variational lower bound가 된다.

많은 연구에서 likelihood를 신경쓰기보다는 인간이 인식하는 이미지의 퀄리티나 약의 효과와 같이 다른 계층의 objective에 집중한다. 이 논문에서는 이러한 data distribution 측면의 접근과는 반대로 objective를 directly하게 다룬다. 기존 강화학습에서는 likelihood의 intractable한 특성 때문에 함부로 적용하기가 어렵다. 이를 해결하기 위해 denoising 과정을 multi-step decision-making task로 생각하여, 근사된 likelihood가 아닌 실제 likelihood를 사용한다. 이 과정을 policy gradient로 학습한 것이 denoising diffusion policy optimization(DDPO)이다.

DDPO를 거대 text-to-image diffusion model에 적용하여 fine-tuning 하였고, image compressibility, aesthetic quality와 같은 task를 수행하였다.

이 논문에서 제시하는 Contribution은 다음과 같다.
1. 다른 reward-weighted likelihood methods 보다 효과적이다.
2. unseen prompts에 대해서도 finetuning, generalization 성능이 좋다.

## Related work

### Defusion probabilistic models

Diffusion model에서 denoising objective가 diffusion model이 실제로 maximum likelihood와는 거리가 먼 방식으로 학습이 된다. (DDPM 논문에서 noise를 예측하는 그걸 말하는 것인듯??) 오히려 likelihood를 stictly하게 optimize한 경우, 이미지의 퀄리티가 더 떨어지는 결과가 있었다. 이는 시각적으로 느끼는 이미지의 성능을 likelihood를 optimize하는 것만으로는 완전히 보장해주지 못함을 의미한다.

### Controllable generation with diffusion models

Text-to-image diffusion model의 성능을 끌어올리기 위해서 여러 방법들이 연구되었다. (여러 방법들이 있으나 따로 논문을 보아야 알 수 있는 내용들이라 설명은 생략)

### Reinforcement learning from human feedback

model optimization을 위해 인간의 피드백을 사용한다. DDPO는 인간의 선호에 따라 reward-weighted likelihood maximization을 사용하는 RWR-style optimization보다 뛰어난 성능을 보인다.

### Diffusion models as sequential decision-making processes

이미지 생성을 sequential dicision-making problem으로 보고 강화학습을 이용해 데이터 생성을 연구한 사례가 있다. 최근의 연구로는 DPOK가 있다. 인간의 선호를 기반으로 single preference-based reward function가 정의된다. DDPO는 multiple reward functions를 사용하는 것에서 차이가 있다.

## Preliminaries

### Diffusion models

Conditional diffusion probabilistic models에서는 $p(x_t\mid c)$, 즉 context $c$가 주어졌을 때 샘플 데이터 $x_0$를 추정하게 된다. 구체적으로는 Markovian forward process의 역과정 $p(x_t\mid x_{t-1})$을 여러 번 거치면서 noise를 추가하게 되는 것이다. 아울러 이 과정을 모사하는 network $\mu_\theta(x_t,c,t)$도 학습하게 된다.

$$
\mathcal{L}_{\text{DDPM}}(\boldsymbol{\theta}) = 
\mathbb{E}_{(x_0, c) \sim p(x_0, c), \; t \sim \mathcal{U}\{0, T\}, \; x_t \sim q(x_t \mid x_0)} 
\left[
\left\|
\tilde{\boldsymbol{\mu}}(x_0, t) - \boldsymbol{\mu}_{\boldsymbol{\theta}}(x_t, c, t)
\right\|^2
\right]
$$

$x_T\sim\mathcal{N}(0,\mathbf{I})$를 샘플링 한 뒤 reverse process $p_\theta(x_{t-1}\mid x_t,c)$를 반복하여 최종적으로는 $x_0$를 얻게 된다.

### Markov decision processes and reinforcement learning

이 부분은 생략

## Reinforcement learning training of diffusion models

### Problem statement

강화학습을 위한 reward를 설정해야 한다. 따라서 reward signal $r(x_0,c)$를 정의하고 이를 최대화한다.

$$
\mathcal{J}_{\text{DDRL}}(\boldsymbol{\theta}) = 
\mathbb{E}_{c \sim p(c), \; x_0 \sim p_{\boldsymbol{\theta}}(x_0 \mid c)} 
\left[ r(x_0, c) \right]
$$

### Reward-weighted regression

DDRL을 최적화하기 위해 기존 DDPM의 loss에 $r$에 따라 결정되는 weighting을 주는 방법을 사용할 수 있다. 이러한 류의 알고리즘을 reward weighted regression (RWR)이라고 한다. 구체적인 weighting을 주는 방식은 여러 가지가 있는데 여기서는 아래 두 가지를 소개한다.
- exponentiated rewards
  
  $$
  w_{\text{RWR}}(x_0, c) = \frac{1}{Z} \exp\left(\beta r(x_0, c)\right)
  $$
- binary weights
  
  $$
  w_{\text{sparse}}(x_0, c) = \mathbb{1}\left[r(x_0, c) \geq C\right]
  $$

아울러 다음과 같이 context를 state, 생성된 이미지를 action으로 하는 MDP를 정의할 수 있다.

$$
s \triangleq c, \quad a \triangleq x_0, \quad 
\pi(a \mid s) \triangleq p_{\boldsymbol{\theta}}(x_0 \mid c), \quad 
\rho_0(s) \triangleq p(c), \quad 
R(s, a) \triangleq r(x_0, c)
$$

Log-likelihood objective에 weighting을 하는 것은 강화학습의 측면에서 KL-divergence 제약조건을 걸고 RL objective을 최대화하는 것과 같다. 하지만 RWR의 경우에는 lower bound에 대해 학습하기 때문에 수학적으로 엄밀하지 못하다.

### Denoising diffusion policy optimization

RWR에서는 최종적으로 생성된 $x_0$만을 이용한다. 하지만 denoising process는 $x_{t-1},x_t$의 과정이 여러 번 반복되는 것을 이용하여 multi-step MDP로 구성한다.

$$
s_t \triangleq (c, t, x_t), \quad 
a_t \triangleq x_{t-1}, \quad 
\pi(a_t \mid s_t) \triangleq p_{\boldsymbol{\theta}}(x_{t-1} \mid x_t, c), \quad 
P(s_{t+1} \mid s_t, a_t) \triangleq (\delta_c, \delta_{t-1}, \delta_{x_{t-1}}), \\
\rho_0(s_0) \triangleq \big(p(c), \delta_T, \mathcal{N}(0, \mathbf{I})\big), \quad 
R(s_t, a_t) \triangleq 
\begin{cases} 
r(x_0, c) & \text{if } t = 0, \\ 
0 & \text{otherwise}.
\end{cases}
$$

DDPO는 policy gradient descent를 이용하며, 두 가지 variant를 제시한다.
- DDPO with score function
  
  $$
  \nabla_{\boldsymbol{\theta}} \mathcal{J}_{\text{DDRL}} = 
  \mathbb{E} \left[
  \sum_{t=0}^T \nabla_{\boldsymbol{\theta}} \log p_{\boldsymbol{\theta}}(x_{t-1} \mid x_t, c) \, r(x_0, c)
  \right]
  $$
- DDPO with importance sampling
  
  $$
  \nabla_{\boldsymbol{\theta}} \mathcal{J}_{\text{DDRL}} = 
  \mathbb{E} \left[
  \sum_{t=0}^T 
  \frac{p_{\boldsymbol{\theta}}(x_{t-1} \mid x_t, c)}{p_{\boldsymbol{\theta}_{\text{old}}}(x_{t-1} \mid x_t, c)} 
  \nabla_{\boldsymbol{\theta}} \log p_{\boldsymbol{\theta}}(x_{t-1} \mid x_t, c) \, r(x_0, c)
  \right]
  $$

Importance sampling은 업데이트 후에도 계속 데이터를 사용할 수 있지만, policy 간의 ratio에 따라 부정확해질 수 있다. 따라서 trust region을 이용하는 TRPO, PPO 방식의 update rule을 따를 수 있다.

## Reward functions for text-to-image diffusion
### Compressibility and incompressibility

이미지 캡션에 이미지의 크기가 나타난 경우가 적기 때문에 prompting으로 이미지 크기를 결정하는 것은 어려운 과제이다. 그렇기 때문에 오히려 강화학습의 접근방법이 편리한 방법이 될 수 있다. 512 x 512의 고정된 크기의 이미지를 출력한 뒤, 이를 JPEG 포맷으로 압축하여 압축된 이미지의 크기를 바탕으로 이를 평가하였다.

### Aesthetic quality

LAION aesthetics predictor를 사용하였다. CLIP 모델을 기반으로 구축된 이 predictor는 1부터 10사이의 값을 통해 평가 점수를 내게 된다. 이 predictor가 인간의 피드백을 바탕으로 학습되었기 때문에 DDPO의 전체 학습 과정이 RLHF의 형태를 띈다.

### Automated prompt alignment with vision-language models

일반적으로 사용되는 방법은 prompt-image alignment이다. 그런데 이 방법은 인간이 이미지에 대해 라벨링하는 과정이 정말 많이 필요하다. 따라서 VLM(visual-language model)을 이용한 방식을 제시한다. RLAIF 연구에서 언어 모델이 인간의 개입 없이 스스로 피드백을 통해 개선되었다는 점에서 착안한 이 아이디어는 다음과 같다.

<center><img src='{{"assets/images/DDPO/ddpo4.png" | relative_url}}' 
width="70%"></center>

아래쪽 pipeline이 우리가 학습하고자 하는 대상이고 이를 위해 LLaVA 를 VLM 모델로 설정하였으며, 이 모델에 "what is happening in this image?"라고 질문하여 이미지를 설명하게 한다. 이제 diffusion model에 입력했던 prompt sentence와 VLM description을 BERTScore를 통해 평가한다. 사용된 diffusion model은 Stable Diffusion v1.4이다.

## Experimental evaluation

<center><img src='{{"assets/images/DDPO/ddpo1.png" | relative_url}}' width="90%"></center>

각 reward function을 통해 학습한 뒤의 정성적 결과

<center><img src='{{"assets/images/DDPO/ddpo3.png" | relative_url}}' width="90%"></center>

학습 진행에 따른 각 task에서의 결과물

### Generalization

<center><img src='{{"assets/images/DDPO/ddpo5.png" | relative_url}}' width="90%"></center>

강화학습으로 언어 모델을 finetuning하게 될 때 보여주는 일반화 특성을 확인할 수 있었다. 45개의 동물, 3개의 동작에 대해서만 학습을 하였음에도 학습 데이터에 포함되지 않은 대상, 동작에 대해서 일반화를 잘 하는 모습을 보였다.
  
### Overoptimization

<center><img src='{{"assets/images/DDPO/ddpo6.png" | relative_url}}' width="90%"></center>

강화학습을 하는 경우 결국에는 모델이 reward에 overoptimization되는 현상이 발생한다. 그림 왼쪽의 경우와 같이 노이즈만 남게 되는 경우가 발생한다. 그리고 오른쪽과 같이 VLM이 typographic attacks에 취약하다는 점을 과도하게 이용하여 "n animals"라는 입력을 주었을 때 특정 숫자를 loosely 닮은 형식으로 출력하고 동물의 숫자 또한 맞지 않는 모습을 보인다. 이러한 overoptimization에 KL-regularization term을 추가하는 대응법이 있으나, 이 방식도 early-stopping와 본질적인 차이가 없다는 연구 결과가 있다. 따라서 저자는 성능이 떨어지기 전의 checkpoint를 사용하였다.


## Discussion and limitations
