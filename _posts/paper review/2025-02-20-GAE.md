---
title: "[GAE] HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION"
last_modified_at: 2025-02-20
categories:
  - paper_review
tags:
  - RL
  - Advantage function
  - TD learning
excerpt: "GAE paper review"
use_math: true
classes: wide
---

> ICLR 2016. [[Paper](https://arxiv.org/abs/1506.02438)]
> John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel  
> 8 Jun 2015

## Summary
강화학습에서 겪는 두 가지 주된 어려움은 첫번째로 요구되는 많은 양의 샘플들이 있고 두번째로 변화되는 분포로부터 얻어지는 데이터를 이용하여 안정적, 단조적으로 정책을 개선해야한다는 것이 있다. 첫번째 어려움은 TD($\lambda$)같은 방식으로 exponentially weighted estimator를 사용하여
bias-variance tradeoff를 통해 샘플의 효율성을 높일 수 있다. 두번째 어려움은 trust region optimization을 policy와 value function에 동시에 적용하여 완화할 수 있다.

아울러 Exponentially weighted estimator를 reward shaping, response function의 관점에서도 설명한다.

## Introduction

강화학습은 기대 보상을 최대화하기 위해 policy gradient를 이용하거나 value function(Actor-critic)를 이용한다. Policy gradient는 bias 없이 policy를 업데이트하지만, variance가 크기 때문에 많은 샘플을 필요로 한다. Actor-critic 방법에서는 반대로 variance를 줄이지만 bias가 생긴다. 따라서 최적해로 수렴하지 못하는 문제가 생길 수 있다. GAE에서는 advantage function을 추정하며 파라미터를 통해 variance, bias tradeoff를 한다. 또한 가치 함수 학습을 위해 trust region optimization method를 제시하여 안정적으로 학습하도록 한다.

## Preliminaries
강화학습에서는 미래 보상을 최대화하기 위해 gradient를 사용한다. Policy gradient의 형태는 다음과 같다.

$$
\begin{aligned}
g :&= \nabla_{\theta} \mathbb{E} \left[ \sum_{t=0}^{\infty} r_t \right]\\
&= \mathbb{E} \left[ \sum_{t=0}^{\infty} \Psi_t \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \right]
\end{aligned}
$$

여기서 $\Psi_t$에 들어갈 수 있는 것들로는

$$
\begin{aligned}
&\sum_{t=0}^{\infty} r_t: \text{total reward of the trajectory.}\\
&\sum_{t'=t}^{\infty} r_{t'}: \text{reward following action $a_t$.}\\
&\sum_{t'=t}^{\infty} r_{t'} - b(s_t): \text{baselined version of previous formula.}\\
&Q^{\pi}(s_t, a_t): \text{state-action value function.}\\
&A^{\pi}(s_t, a_t): \text{advantage function.}\\
&r_t + V^{\pi}(s_{t+1}) - V^{\pi}(s_t): \text{TD residual.}\\
\end{aligned}
$$

$$
\text{where}\quad V^{\pi}(s_t) := \mathbb{E}_{s_{t+1:\infty}, a_{t:\infty}} \left[ \sum_{l=0}^{\infty} r_{t+l} \right]\quad\quad Q^{\pi}(s_t, a_t) := \mathbb{E}_{s_{t+1:\infty}, a_{t+1:\infty}} \left[ \sum_{l=0}^{\infty} r_{t+l} \right]\\
A^{\pi}(s_t, a_t) := Q^{\pi}(s_t, a_t) - V^{\pi}(s_t),
\quad \text{(Advantage function).}
$$

이 있다. 이제 discount factor $\gamma$를 고려한 estimation과 gradient를 다시 정의하면

$$
V^{\pi, \gamma}(s_t) := \mathbb{E}_{s_{t+1:\infty}, a_{t:\infty}} \left[ \sum_{l=0}^{\infty} \gamma^l r_{t+l} \right]\\
Q^{\pi, \gamma}(s_t, a_t) := \mathbb{E}_{s_{t+1:\infty}, a_{t+1:\infty}} \left[ \sum_{l=0}^{\infty} \gamma^l r_{t+l} \right]\\
A^{\pi, \gamma}(s_t, a_t) := Q^{\pi, \gamma}(s_t, a_t) - V^{\pi, \gamma}(s_t).\\
g^{\gamma} := \mathbb{E}_{s_{0:\infty}, a_{0:\infty}} \left[ \sum_{t=0}^{\infty} A^{\pi, \gamma}(s_t, a_t) \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \right]
$$

이 때, advantage function은 $\gamma$-just estimator가 된다.

> $\textbf{Definition 1.}$ $\textit{The estimator } \hat{A}_t \textit{ is } \gamma\textit{-just if}$
> <div align="center">
>
$$
\mathbb{E}_{s_{0:\infty}, a_{0:\infty}} \left[ \hat{A}_t(s_{0:\infty}, a_{0:\infty}) \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \right]
= \mathbb{E}_{s_{0:\infty}, a_{0:\infty}} \left[ A^{\pi, \gamma}(s_t, a_t) \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \right].
$$
> </div>

만약 $\hat{A}\_t$가 $\gamma$-just라면 다음을 만족한다.

$$
\mathbb{E}_{s_{0:\infty}, a_{0:\infty}} \left[ \sum_{t=0}^{\infty} \hat{A}_t(s_{0:\infty}, a_{0:\infty}) \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \right] = g^{\gamma}
$$

또한 $\hat{A}\_t$가 $\gamma$-just이기 위한 필요조건은 $\hat{A}\_t$가 $Q\_t$와 $b\_t$로 분해될 수 있다는 것이다. $Q\_t$는 unbiased estimator이고 $b_t$는 임의의 baseline으로써 state history, action history(before $t$)의 함수이다.

> $\textbf{Proposition 1.}$ Suppose that $\hat{A}\_t$ can be written in the form $\hat{A}\_t(s_{0:\infty}, a\_{0:\infty}) = Q\_t(s\_{t:\infty}, a\_{t:\infty}) - b\_t(s\_{0:t}, a\_{0:t-1})$ such that for all $(s\_t, a\_t)$, $\mathbb{E}\_{s\_{t+1:\infty}, a\_{t+1:\infty} \mid s\_t, a\_t} \left[ Q\_t(s\_{t:\infty}, a\_{t:\infty}) \right] = Q^{\pi, \gamma}(s\_t, a\_t)$. Then $\hat{A}$ is $\gamma$-just.

<details>
<summary>Proof of Proposition 1.</summary>
<div markdown="1">
$Q_t$와 $b_t$를 나누어 생각한다. 확률분포의 pdf는 항상 합이 1이므로 미분했을 때 0이 나오게 된다.

$$
\mathbb{E}_{s_{0:t}, a_{0:t}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \right] = 0
$$

이를 고려하면 두 경우에 대해 모두 답이 나온다.

$$
\begin{aligned}
&\mathbb{E}_{s_{0:\infty}, a_{0:\infty}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) Q_t(s_{0:\infty}, a_{0:\infty}) \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t}} \left[ \mathbb{E}_{s_{t+1:\infty}, a_{t+1:\infty}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) Q_t(s_{0:\infty}, a_{0:\infty}) \right] \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \mathbb{E}_{s_{t+1:\infty}, a_{t+1:\infty}} \left[ Q_t(s_{0:\infty}, a_{0:\infty}) \right] \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) Q^{\pi,\gamma}(s_t, a_t) \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \left(A^{\pi}(s_t, a_t)+ V^{\pi,\gamma}(s_t)\right) \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) A^{\pi}(s_t, a_t) \right]
\end{aligned}
$$

$$
\begin{aligned}
&\mathbb{E}_{s_{0:\infty}, a_{0:\infty}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) b_t(s_{0:t}, a_{0:t-1}) \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t-1}} \left[ \mathbb{E}_{s_{t+1:\infty}, a_{t+1:\infty}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) b_t(s_{0:t}, a_{0:t-1}) \right] \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t-1}} \left[ \mathbb{E}_{s_{t+1:\infty}, a_{t+1:\infty}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \right] b_t(s_{0:t}, a_{0:t-1}) \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t-1}} \left[ 0 \cdot b_t(s_{0:t}, a_{0:t-1}) \right]\\
&= 0.
\end{aligned}
$$


</div>
</details>
\
따라서 다음의 estimation들은 $\gamma$-just이다.

$$
\sum_{l=0}^{\infty} \gamma^l r_{t+l}, \qquad Q^{\pi, \gamma}(s_t, a_t), \qquad A^{\pi, \gamma}(s_t, a_t), \qquad r_t + \gamma V^{\pi, \gamma}(s_{t+1}) - V^{\pi, \gamma}(s_t)
$$

즉, $\gamma$-just라는 것은 estimation을 통해 계산한 gradient의 평균이 실제 gradient의 평균과 동일(unbiased)하다는 뜻이다. $\gamma$-just를 만족한다면 우리는 unbiased gradient update를 할 수 있게된다. 그리고 이를 만족하기 위해 unbiased에 가까운 $Q$를 고르는 것이 중요할 것이다.

## Advantage function estimation

그렇다면 이 $\hat{A}\_t^n$을 어떻게 해야 정확하게 estimate할 수 있을까? 우리는 에피소드로부터 다음과 같이 gradient의 estimation을 구하게 된다.

$$
\hat{g} = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=0}^{\infty} \hat{A}_t^n \nabla_{\theta} \log \pi_{\theta}(a_t^n \mid s_t^n)
$$

TD residual $\delta\_t^{V} =r\_t + \gamma V(s\_{t+1}) - V(s\_t)$을 정의하면, 그것이 $V=V^{\pi,\gamma}$를 만족하는 조건 하에서 평균을 취하면 실제 advantage function이 됨을 알 수 있다(unbiased).

$$
\begin{aligned}
\mathbb{E}_{s_{t+1}} \left[ \delta_t^{V, \pi, \gamma} \right]
&= \mathbb{E}_{s_{t+1}} \left[ r_t + \gamma V^{\pi, \gamma}(s_{t+1}) - V^{\pi, \gamma}(s_t) \right]\\
&= \mathbb{E}_{s_{t+1}} \left[ Q^{\pi, \gamma}(s_t, a_t) - V^{\pi, \gamma}(s_t) \right] = A^{\pi, \gamma}(s_t, a_t).
\end{aligned}
$$

이제 TD residual에 time discount를 적용하여 t 시점에서의 k step만큼의 TD residual을 고려하는 estimation을 생각할 수 있다.

$$
\begin{aligned}
    \hat{A}_t^{(1)} &:= \delta_t^V = -V(s_t) + r_t + \gamma V(s_{t+1}) \\
    \hat{A}_t^{(2)} &:= \delta_t^V + \gamma \delta_{t+1}^V = -V(s_t) + r_t + \gamma r_{t+1} + \gamma^2 V(s_{t+2}) \\
    \hat{A}_t^{(3)} &:= \delta_t^V + \gamma \delta_{t+1}^V + \gamma^2 \delta_{t+2}^V = -V(s_t) + r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 V(s_{t+3}) \\ \\
    \hat{A}_t^{(k)} &:= \sum_{l=0}^{k-1} \gamma^l \delta_{t+l}^V = -V(s_t) + r_t + \gamma r_{t+1} + \dots + \gamma^{k-1} r_{t+k-1} + \gamma^k V(s_{t+k})
\end{aligned}
$$

$\hat{A}\_t^{(k)}$는 세 부분으로 나누어진다. Baseline term $-V(s\_t)$와 $k$-step만큼의 return, 그리고 $\gamma^k V(s\_{t+k})$이다. $k$의 값에 따라 bias가 어떻게 변화하는지 생각해보면, baseline은 Proposition에서 임의의 함수이기 때문에 조건을 만족하고, return 또한 문제될 것이 없고, $\gamma^k V(s\_{t+k})$항이 unbiased이면 $\gamma$-just하게 된다. 당연하게도 $k$가 무한히 증가하면 이 값의 영향은 미미해지고 다음을 얻게 된다.

$$
\hat{A}_t^{(\infty)} = \sum_{l=0}^{\infty} \gamma^l \delta_{t+l}^V = -V(s_t) + \sum_{l=0}^{\infty} \gamma^l r_{t+l}
$$

GAE estimator는 k-step estimation of advantage function에 다시 exponentially weighted average을 취한다. Weight parameter는 $\lambda$를 사용한다.

$$
\begin{aligned}
    \hat{A}_t^{\text{GAE}(\gamma, \lambda)} &:= (1-\lambda) \left( \hat{A}_t^{(1)} + \lambda \hat{A}_t^{(2)} + \lambda^2 \hat{A}_t^{(3)} + \dots \right) \\
    &= (1-\lambda) \left( \delta_t^V + \lambda \delta_t^V + \gamma \delta_{t+1}^V + \lambda^2 (\delta_t^V + \gamma \delta_{t+1}^V + \gamma^2 \delta_{t+2}^V) + \dots \right) \\
    &= (1-\lambda) \left( \delta_t^V (1 + \lambda + \lambda^2 + \dots) + \gamma \delta_{t+1}^V (\lambda + \lambda^2 + \lambda^3 + \dots) + \dots \right) \\
    &= (1-\lambda) \left( \delta_t^V \frac{1}{1-\lambda} + \gamma \delta_{t+1}^V \frac{\lambda}{1-\lambda} + \gamma^2 \delta_{t+2}^V \frac{\lambda^2}{1-\lambda} + \dots \right) \\
    &= \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}^V
\end{aligned}
$$

그 결과는 신기하게도 매우 간단한 형태의 식이 나오는 것을 확인할 수 있다. TD($\lambda$)와 형태가 유사한데 차이점은 분명히 존재한다. TD($\lambda$)는 value function을 예측하는데에 사용된다. 반면, GAE는 advantage function을 예측하는데에 사용된다.

$$
GAE: \qquad \hat{A}_t^{\text{GAE}(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}^V\\
TD(\lambda): \qquad G_t^\lambda = (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)}, \quad \text{where } G_t^{(n)} = \sum_{k=0}^{n-1} \gamma^k r_{t+k} + \gamma^n V(s_{t+n})
$$

GAE의 특수한 상황, 즉 $\lambda=1,\lambda=0$일 때를 비교해보자.

$$
\begin{aligned}
    \text{GAE}(\gamma, 0) : \quad \hat{A}_t &:= \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) \\
    \text{GAE}(\gamma, 1) : \quad \hat{A}_t &:= \sum_{l=0}^{\infty} \gamma^l \delta_{t+l}^V = \sum_{l=0}^{\infty} \gamma^l r_{t+l} - V(s_t)
\end{aligned}
$$

$\text{GAE}(\gamma, 1)$의 경우 $k$에 의해 value function의 영향이 사라져버렸기 때문에 항상 $\gamma$-just하다. 반면 $\text{GAE}(\gamma, 0)$의 경우 $V=V^{\pi,\gamma}$를 만족해야 $\gamma$-just하다. 둘 사이에는 bias와 variance의 tradeoff가 존재한다.

$\gamma$와 $\lambda$또한 서도 다른 역할을 가지고 있다. $\gamma<1$의 경우 value function에 붙어 곱해지기 때문에 value function estimation의 정확도와 관계없이 bias를 달리 가져오게 된다. 반면 $\lambda<1$는 value function estimation이 정확한 경우에는 bias를 가져오지 않는다. Bias를 가져오는 정도도 차이를 보이는데 $\lambda$가 $\gamma$보다 더 적은 bias를 가져오는 것으로 보인다. 그 이유는 실험 결과를 통해서 알아본 최적 값에서 보통 $\lambda$가 $\gamma$보다 작은 값을 갖기 때문이다.

## Interpretation as Reward Shaping

$\lambda$를 reward shaping 후에 적용된 discount factor로 해석할 수 있다. Reward shaping의 정의는 다음과 같이 transformation을 적용한 것이다.

$$
\tilde{r}(s, a, s') = r(s, a, s') + \gamma \Phi(s') - \Phi(s),
$$

이 식을 t부터 쭉 summation하면

$$
\sum_{l=0}^{\infty} \gamma^l \tilde{r}(s_{t+l}, a_{t+l}, s_{t+l+1}) = \sum_{l=0}^{\infty} \gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \Phi(s_t).
$$

다시 이 식을 이용하면 가치함수류들을 다음과 같이 표현할 수 있다.

$$
\tilde{Q}^{\pi, \gamma}(s, a) = Q^{\pi, \gamma}(s, a) - \Phi(s)\\
\tilde{V}^{\pi, \gamma}(s) = V^{\pi, \gamma}(s) - \Phi(s)\\
\tilde{A}^{\pi, \gamma}(s, a) = (Q^{\pi, \gamma}(s, a) - \Phi(s)) - (V^{\pi, \gamma}(s) - \Phi(s)) = A^{\pi, \gamma}(s, a).
$$

이 아이디어를 바탕으로, GAE의 policy gradient에 적용한다. $\gamma\leftarrow \gamma\lambda, \Phi\leftarrow V$로 두면 reward shaping의 결과가 GAE estimation인 것을 확인할 수 있다.

$$
\sum_{l=0}^{\infty} (\gamma \lambda)^l \tilde{r}(s_{t+l}, a_t, s_{t+l+1}) = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}^{V} = \hat{A}_{t}^{\text{GAE}(\gamma, \lambda)}.
$$

추가적으로, $\gamma$와 GAE가 policy gradient estimation에 미치는 영향을 알아보기 위해 response function의 개념을 소개한다. Response function이란 특정 행동이 미래 보상에 미치는 영향의 정도를 수치화한 것이다. 강화학습에서는 행동의 영향을 오랜 시간 뒤에 알게 되는 경우가 있다. 이러한 long-range dependency를 파악하기 위해 response function은 중요한 역할을 한다.

$$
\chi(l; s_t, a_t) = \mathbb{E} \left[ r_{t+l} \mid s_t, a_t \right] - \mathbb{E} \left[ r_{t+l} \mid s_t \right].\\
A^{\pi, \gamma}(s, a) = \sum_{l=0}^{\infty} \gamma^l \chi(l; s, a)
$$

정의를 보면 advantage function이 response function의 summation으로 표현된다. 이제 policy gradient를 response function으로 표현해보면

$$
\nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) A^{\pi, \gamma}(s_t, a_t) = \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \sum_{l=0}^{\infty} \gamma^l \chi(l; s_t, a_t).
$$

따라서 $\gamma<1$일 때 $l \gg \frac{1}{(1 - \gamma)}$를 만족하면 이후의 response term의 영향은 무시할 수 있을 정도로 작아진다.

<details>
<summary>$l$의 기준</summary>
<div markdown="1">

네, 이 부분을 수학적으로 엄밀하게 살펴보자. 논문에서 말하는 **\\( l \gg 1/(1-\gamma) \\) 인 항들은 거의 무시됨**을 증명하기 위해, 할인율 \\(\gamma\\)가 정책 그래디언트 추정에 미치는 영향을 분석해보자.

---

## **✅ 수학적 분석: 왜 \\( l \gg 1/(1-\gamma) \\)이면 기여도가 거의 0이 되는가?**

우리는 정책 그래디언트 추정식에서 등장하는 **할인 계수**의 형태를 확인해야 함:

\\(
\sum_{l=0}^{\infty} \gamma^l \chi(l; s_t, a_t)
\\)

여기서 중요한 점은 **\\( \gamma^l \\)** 항이 **지수적으로 감소**한다는 것.

### **📌 지수 감소율 분석**
\\(\gamma^l\\) 항이 감소하는 속도를 분석하려면, 다음 부등식을 고려하자:

\\(
\gamma^l \leq e^{-l(1-\gamma)}
\\)

이 부등식이 성립하는 이유는 로그를 취해서 확인할 수 있음:

\\(
\ln(\gamma^l) = l \ln \gamma \approx l(- (1-\gamma)) = -l(1-\gamma),
\\)

즉,

\\(
\gamma^l \approx e^{-l(1-\gamma)}.
\\)

### **📌 \\( l \gg 1/(1-\gamma) \\) 인 경우**
- 만약 \\( l \gg 1/(1-\gamma) \\)라면,  
  \\(
  l(1-\gamma) \gg 1
  \\)
  이므로,
  \\(
  e^{-l(1-\gamma)} \approx 0.
  \\)

즉, **할인율이 1에 가까울수록, \\( \gamma^l \\)는 \\( l \approx 1/(1-\gamma) \\) 이후부터 매우 작아짐.**  
따라서 \\( l \gg 1/(1-\gamma) \\) 인 경우, 기여도가 사실상 0에 가까워짐.

---

## **✅ 결론**
- **할인율 \\( \gamma \\)가 1에 가까우면, 먼 미래의 보상도 중요하게 반영됨.**
- 하지만, **\\( l \gg 1/(1-\gamma) \\) 이후부터는 기여도가 거의 0이 됨.**
- **이것이 논문에서 "할인율 \\( \gamma < 1 \\)을 사용하면 \\( l \gg 1/(1-\gamma) \\) 항들은 무시된다"라고 말하는 수학적 근거!** 🚀

</div>
</details>
\
다시 reward shaping으로 돌아와서 $\Phi=V^{\pi,\gamma}$일 때, $\mathbb{E} [\tilde{r}\_{t+l} \mid s\_t, a\_t] = \mathbb{E} [\tilde{r}\_{t+l} \mid s\_t] = 0 \text{ for } l > 0$이다($V$의 정의를 생각해 보면 된다). 결과적으로 GAE를 이용해 reward shaping하게 되면 response function이 temporally extended인 형태에서 immediate인 형태로 변형된다(다루기가 용이해진다는 뜻). 거기에 $\lambda$를 이용하여 좀 더 가파른 discount를 제공하여 long delays에 대한 noise의 영향을 줄인다.

소결론 : GAE를 통해 advantage function estimation을 정의하면 reward shaping의 관점에서 response function이 즉각적인 형태로 바뀌게 되어 다루기 용이해진다. 이를 $\lambda$를 이용하여 다루게 된다.

## Value function estimation

전체 process에서는 policy update를 위한 GAE 말고도 가치 함수 update를 위한 대한 estimation도 해야 하는데 여기서는 trust region 방식을 사용한 QP를 풀게된다. 기존 풀고자 하는 문제는

$$
\text{minimize}_{\phi} \quad \sum_{n=1}^{N} \| V_{\phi}(s_n) - \hat{V}_n \|^2\\
\text{subject to} \quad \frac{1}{N} \sum_{n=1}^{N} \frac{\| V_{\phi}(s_n) - V_{\phi_{\text{old}}}(s_n) \|^2}{2\sigma^2} \leq \epsilon.
$$

인데, 아래의 QP 문제를 풀어 해결할 수 있다.

$$
\text{minimize}_{\phi} \quad g^T (\phi - \phi_{\text{old}})\\
\text{subject to} \quad \frac{1}{N} \sum_{n=1}^{N} (\phi - \phi_{\text{old}})^T H (\phi - \phi_{\text{old}}) \leq \epsilon.
$$


## Experiments

$$
\begin{align*}
    &\text{minimize }_{\theta} \quad L_{\theta_{old}}(\theta) \\
    &\text{subject to} \quad \overline{D}_{KL}^{\theta_{old}}(\pi_{\theta_{old}}, \pi_{\theta}) \leq \epsilon
\end{align*}
\\
\text{where }
L_{\theta_{old}}(\theta) \equiv \frac{1}{N} \sum_{n=1}^{N} \frac{\pi_{\theta}(a_n \mid s_n)}{\pi_{\theta_{old}}(a_n \mid s_n)} \hat{A}_n, \quad
\overline{D}_{KL}^{\theta_{old}}(\pi_{\theta_{old}}, \pi_{\theta}) = \frac{1}{N} \sum_{n=1}^{N} D_{KL}(\pi_{\theta_{old}}(\cdot \mid s_n) \parallel \pi_{\theta}(\cdot \mid s_n))
$$


Policy update는 TRPO를 사용한다.



<center>
<img src='{{"assets/images/GAE/gae4.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">TRPO update with GAE</figcaption>
</center>

다양한 $\lambda,\gamma$에 대해 결과를 확인할 수 있다. 가치 함수(TD error)를 사용하지 않고 time-dependent baseline(각 time step에서의 평균 리턴)을 사용한 경우(No VF) 성능이 현저히 떨어지는 것을 확인할 수 있다. No VF는 GAE 대신 다음의 advantage function estimation을 사용한다.

$$
\hat{A}_t=R_t-B_t
$$

$R\_t$는 Monte Carlo Return이고, $B\_t$는 time step별 평균 리턴이다.

<center>
<img src='{{"assets/images/GAE/gae1.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Experimental Results</figcaption>
</center>
<center>
<img src='{{"assets/images/GAE/gae2.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Experimental Results</figcaption>
</center>
<center>
<img src='{{"assets/images/GAE/gae3.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Experimental Results</figcaption>
</center>

## Discussion

Policy gradient의 정확도는 가치 함수 estimation의 오차(Bellman error, projected Bellman error)의 영향을 받는다. 만약 둘 사이의 관계를 수학적으로 공식화할 수 있다면 policy gradient에서 가치 함수를 더 효과적으로 학습할 수 있을 것이다.

또 다른 연구 방향은 policy와 value function에 대해 shared 구조를 갖도록 하는 것이다. 공통적인 표현 학습이 가능해져 학습이 빨라질 수 있다. 하지만 이 상태에서 최적화가 잘 이루어지는지, 수렴성이 보장되는지에 대한 부분은 검증할 필요가 있다.

아울러 Continuous 환경에서도 GAE가 잘 동작하는지 알아볼 필요가 있다.