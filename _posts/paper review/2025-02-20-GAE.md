---
title: "[GAE] HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION"
last_modified_at: 2025-02-20
categories:
  - paper_review
tags:
  - RL
  - Advantage function
  - TD learning
excerpt: "GAE paper review"
use_math: true
classes: wide
---

> ICLR 2016. [[Paper](https://arxiv.org/abs/1506.02438)]
> John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel  
> 8 Jun 2015

## Summary
ê°•í™”í•™ìŠµì—ì„œ ê²ªëŠ” ë‘ ê°€ì§€ ì£¼ëœ ì–´ë ¤ì›€ì€ ì²«ë²ˆì§¸ë¡œ ìš”êµ¬ë˜ëŠ” ë§ì€ ì–‘ì˜ ìƒ˜í”Œë“¤ì´ ìˆê³  ë‘ë²ˆì§¸ë¡œ ë³€í™”ë˜ëŠ” ë¶„í¬ë¡œë¶€í„° ì–»ì–´ì§€ëŠ” ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ì•ˆì •ì , ë‹¨ì¡°ì ìœ¼ë¡œ ì •ì±…ì„ ê°œì„ í•´ì•¼í•œë‹¤ëŠ” ê²ƒì´ ìˆë‹¤. ì²«ë²ˆì§¸ ì–´ë ¤ì›€ì€ TD($\lambda$)ê°™ì€ ë°©ì‹ìœ¼ë¡œ exponentially weighted estimatorë¥¼ ì‚¬ìš©í•˜ì—¬
bias-variance tradeoffë¥¼ í†µí•´ ìƒ˜í”Œì˜ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆë‹¤. ë‘ë²ˆì§¸ ì–´ë ¤ì›€ì€ trust region optimizationì„ policyì™€ value functionì— ë™ì‹œì— ì ìš©í•˜ì—¬ ì™„í™”í•  ìˆ˜ ìˆë‹¤.

ì•„ìš¸ëŸ¬ Exponentially weighted estimatorë¥¼ reward shaping, response functionì˜ ê´€ì ì—ì„œë„ ì„¤ëª…í•œë‹¤.

## Introduction

ê°•í™”í•™ìŠµì€ ê¸°ëŒ€ ë³´ìƒì„ ìµœëŒ€í™”í•˜ê¸° ìœ„í•´ policy gradientë¥¼ ì´ìš©í•˜ê±°ë‚˜ value function(Actor-critic)ë¥¼ ì´ìš©í•œë‹¤. Policy gradientëŠ” bias ì—†ì´ policyë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ë§Œ, varianceê°€ í¬ê¸° ë•Œë¬¸ì— ë§ì€ ìƒ˜í”Œì„ í•„ìš”ë¡œ í•œë‹¤. Actor-critic ë°©ë²•ì—ì„œëŠ” ë°˜ëŒ€ë¡œ varianceë¥¼ ì¤„ì´ì§€ë§Œ biasê°€ ìƒê¸´ë‹¤. ë”°ë¼ì„œ ìµœì í•´ë¡œ ìˆ˜ë ´í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆë‹¤. GAEì—ì„œëŠ” advantage functionì„ ì¶”ì •í•˜ë©° íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ variance, bias tradeoffë¥¼ í•œë‹¤. ë˜í•œ ê°€ì¹˜ í•¨ìˆ˜ í•™ìŠµì„ ìœ„í•´ trust region optimization methodë¥¼ ì œì‹œí•˜ì—¬ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•˜ë„ë¡ í•œë‹¤.

## Preliminaries
ê°•í™”í•™ìŠµì—ì„œëŠ” ë¯¸ë˜ ë³´ìƒì„ ìµœëŒ€í™”í•˜ê¸° ìœ„í•´ gradientë¥¼ ì‚¬ìš©í•œë‹¤. Policy gradientì˜ í˜•íƒœëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\begin{aligned}
g :&= \nabla_{\theta} \mathbb{E} \left[ \sum_{t=0}^{\infty} r_t \right]\\
&= \mathbb{E} \left[ \sum_{t=0}^{\infty} \Psi_t \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \right]
\end{aligned}
$$

ì—¬ê¸°ì„œ $\Psi_t$ì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ” ê²ƒë“¤ë¡œëŠ”

$$
\begin{aligned}
&\sum_{t=0}^{\infty} r_t: \text{total reward of the trajectory.}\\
&\sum_{t'=t}^{\infty} r_{t'}: \text{reward following action $a_t$.}\\
&\sum_{t'=t}^{\infty} r_{t'} - b(s_t): \text{baselined version of previous formula.}\\
&Q^{\pi}(s_t, a_t): \text{state-action value function.}\\
&A^{\pi}(s_t, a_t): \text{advantage function.}\\
&r_t + V^{\pi}(s_{t+1}) - V^{\pi}(s_t): \text{TD residual.}\\
\end{aligned}
$$

$$
\text{where}\quad V^{\pi}(s_t) := \mathbb{E}_{s_{t+1:\infty}, a_{t:\infty}} \left[ \sum_{l=0}^{\infty} r_{t+l} \right]\quad\quad Q^{\pi}(s_t, a_t) := \mathbb{E}_{s_{t+1:\infty}, a_{t+1:\infty}} \left[ \sum_{l=0}^{\infty} r_{t+l} \right]\\
A^{\pi}(s_t, a_t) := Q^{\pi}(s_t, a_t) - V^{\pi}(s_t),
\quad \text{(Advantage function).}
$$

ì´ ìˆë‹¤. ì´ì œ discount factor $\gamma$ë¥¼ ê³ ë ¤í•œ estimationê³¼ gradientë¥¼ ë‹¤ì‹œ ì •ì˜í•˜ë©´

$$
V^{\pi, \gamma}(s_t) := \mathbb{E}_{s_{t+1:\infty}, a_{t:\infty}} \left[ \sum_{l=0}^{\infty} \gamma^l r_{t+l} \right]\\
Q^{\pi, \gamma}(s_t, a_t) := \mathbb{E}_{s_{t+1:\infty}, a_{t+1:\infty}} \left[ \sum_{l=0}^{\infty} \gamma^l r_{t+l} \right]\\
A^{\pi, \gamma}(s_t, a_t) := Q^{\pi, \gamma}(s_t, a_t) - V^{\pi, \gamma}(s_t).\\
g^{\gamma} := \mathbb{E}_{s_{0:\infty}, a_{0:\infty}} \left[ \sum_{t=0}^{\infty} A^{\pi, \gamma}(s_t, a_t) \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \right]
$$

ì´ ë•Œ, advantage functionì€ $\gamma$-just estimatorê°€ ëœë‹¤.

> $\textbf{Definition 1.}$ $\textit{The estimator } \hat{A}_t \textit{ is } \gamma\textit{-just if}$
> <div align="center">
>
$$
\mathbb{E}_{s_{0:\infty}, a_{0:\infty}} \left[ \hat{A}_t(s_{0:\infty}, a_{0:\infty}) \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \right]
= \mathbb{E}_{s_{0:\infty}, a_{0:\infty}} \left[ A^{\pi, \gamma}(s_t, a_t) \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \right].
$$
> </div>

ë§Œì•½ $\hat{A}\_t$ê°€ $\gamma$-justë¼ë©´ ë‹¤ìŒì„ ë§Œì¡±í•œë‹¤.

$$
\mathbb{E}_{s_{0:\infty}, a_{0:\infty}} \left[ \sum_{t=0}^{\infty} \hat{A}_t(s_{0:\infty}, a_{0:\infty}) \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \right] = g^{\gamma}
$$

ë˜í•œ $\hat{A}\_t$ê°€ $\gamma$-justì´ê¸° ìœ„í•œ í•„ìš”ì¡°ê±´ì€ $\hat{A}\_t$ê°€ $Q\_t$ì™€ $b\_t$ë¡œ ë¶„í•´ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. $Q\_t$ëŠ” unbiased estimatorì´ê³  $b_t$ëŠ” ì„ì˜ì˜ baselineìœ¼ë¡œì¨ state history, action history(before $t$)ì˜ í•¨ìˆ˜ì´ë‹¤.

> $\textbf{Proposition 1.}$ Suppose that $\hat{A}\_t$ can be written in the form $\hat{A}\_t(s_{0:\infty}, a\_{0:\infty}) = Q\_t(s\_{t:\infty}, a\_{t:\infty}) - b\_t(s\_{0:t}, a\_{0:t-1})$ such that for all $(s\_t, a\_t)$, $\mathbb{E}\_{s\_{t+1:\infty}, a\_{t+1:\infty} \mid s\_t, a\_t} \left[ Q\_t(s\_{t:\infty}, a\_{t:\infty}) \right] = Q^{\pi, \gamma}(s\_t, a\_t)$. Then $\hat{A}$ is $\gamma$-just.

<details>
<summary>Proof of Proposition 1.</summary>
<div markdown="1">
$Q_t$ì™€ $b_t$ë¥¼ ë‚˜ëˆ„ì–´ ìƒê°í•œë‹¤. í™•ë¥ ë¶„í¬ì˜ pdfëŠ” í•­ìƒ í•©ì´ 1ì´ë¯€ë¡œ ë¯¸ë¶„í–ˆì„ ë•Œ 0ì´ ë‚˜ì˜¤ê²Œ ëœë‹¤.

$$
\mathbb{E}_{s_{0:t}, a_{0:t}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \right] = 0
$$

ì´ë¥¼ ê³ ë ¤í•˜ë©´ ë‘ ê²½ìš°ì— ëŒ€í•´ ëª¨ë‘ ë‹µì´ ë‚˜ì˜¨ë‹¤.

$$
\begin{aligned}
&\mathbb{E}_{s_{0:\infty}, a_{0:\infty}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) Q_t(s_{0:\infty}, a_{0:\infty}) \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t}} \left[ \mathbb{E}_{s_{t+1:\infty}, a_{t+1:\infty}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) Q_t(s_{0:\infty}, a_{0:\infty}) \right] \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \mathbb{E}_{s_{t+1:\infty}, a_{t+1:\infty}} \left[ Q_t(s_{0:\infty}, a_{0:\infty}) \right] \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) Q^{\pi,\gamma}(s_t, a_t) \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \left(A^{\pi}(s_t, a_t)+ V^{\pi,\gamma}(s_t)\right) \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) A^{\pi}(s_t, a_t) \right]
\end{aligned}
$$

$$
\begin{aligned}
&\mathbb{E}_{s_{0:\infty}, a_{0:\infty}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) b_t(s_{0:t}, a_{0:t-1}) \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t-1}} \left[ \mathbb{E}_{s_{t+1:\infty}, a_{t+1:\infty}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) b_t(s_{0:t}, a_{0:t-1}) \right] \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t-1}} \left[ \mathbb{E}_{s_{t+1:\infty}, a_{t+1:\infty}} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \right] b_t(s_{0:t}, a_{0:t-1}) \right]\\
&= \mathbb{E}_{s_{0:t}, a_{0:t-1}} \left[ 0 \cdot b_t(s_{0:t}, a_{0:t-1}) \right]\\
&= 0.
\end{aligned}
$$


</div>
</details>
\
ë”°ë¼ì„œ ë‹¤ìŒì˜ estimationë“¤ì€ $\gamma$-justì´ë‹¤.

$$
\sum_{l=0}^{\infty} \gamma^l r_{t+l}, \qquad Q^{\pi, \gamma}(s_t, a_t), \qquad A^{\pi, \gamma}(s_t, a_t), \qquad r_t + \gamma V^{\pi, \gamma}(s_{t+1}) - V^{\pi, \gamma}(s_t)
$$

ì¦‰, $\gamma$-justë¼ëŠ” ê²ƒì€ estimationì„ í†µí•´ ê³„ì‚°í•œ gradientì˜ í‰ê· ì´ ì‹¤ì œ gradientì˜ í‰ê· ê³¼ ë™ì¼(unbiased)í•˜ë‹¤ëŠ” ëœ»ì´ë‹¤. $\gamma$-justë¥¼ ë§Œì¡±í•œë‹¤ë©´ ìš°ë¦¬ëŠ” unbiased gradient updateë¥¼ í•  ìˆ˜ ìˆê²Œëœë‹¤. ê·¸ë¦¬ê³  ì´ë¥¼ ë§Œì¡±í•˜ê¸° ìœ„í•´ unbiasedì— ê°€ê¹Œìš´ $Q$ë¥¼ ê³ ë¥´ëŠ” ê²ƒì´ ì¤‘ìš”í•  ê²ƒì´ë‹¤.

## Advantage function estimation

ê·¸ë ‡ë‹¤ë©´ ì´ $\hat{A}\_t^n$ì„ ì–´ë–»ê²Œ í•´ì•¼ ì •í™•í•˜ê²Œ estimateí•  ìˆ˜ ìˆì„ê¹Œ? ìš°ë¦¬ëŠ” ì—í”¼ì†Œë“œë¡œë¶€í„° ë‹¤ìŒê³¼ ê°™ì´ gradientì˜ estimationì„ êµ¬í•˜ê²Œ ëœë‹¤.

$$
\hat{g} = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=0}^{\infty} \hat{A}_t^n \nabla_{\theta} \log \pi_{\theta}(a_t^n \mid s_t^n)
$$

TD residual $\delta\_t^{V} =r\_t + \gamma V(s\_{t+1}) - V(s\_t)$ì„ ì •ì˜í•˜ë©´, ê·¸ê²ƒì´ $V=V^{\pi,\gamma}$ë¥¼ ë§Œì¡±í•˜ëŠ” ì¡°ê±´ í•˜ì—ì„œ í‰ê· ì„ ì·¨í•˜ë©´ ì‹¤ì œ advantage functionì´ ë¨ì„ ì•Œ ìˆ˜ ìˆë‹¤(unbiased).

$$
\begin{aligned}
\mathbb{E}_{s_{t+1}} \left[ \delta_t^{V, \pi, \gamma} \right]
&= \mathbb{E}_{s_{t+1}} \left[ r_t + \gamma V^{\pi, \gamma}(s_{t+1}) - V^{\pi, \gamma}(s_t) \right]\\
&= \mathbb{E}_{s_{t+1}} \left[ Q^{\pi, \gamma}(s_t, a_t) - V^{\pi, \gamma}(s_t) \right] = A^{\pi, \gamma}(s_t, a_t).
\end{aligned}
$$

ì´ì œ TD residualì— time discountë¥¼ ì ìš©í•˜ì—¬ t ì‹œì ì—ì„œì˜ k stepë§Œí¼ì˜ TD residualì„ ê³ ë ¤í•˜ëŠ” estimationì„ ìƒê°í•  ìˆ˜ ìˆë‹¤.

$$
\begin{aligned}
    \hat{A}_t^{(1)} &:= \delta_t^V = -V(s_t) + r_t + \gamma V(s_{t+1}) \\
    \hat{A}_t^{(2)} &:= \delta_t^V + \gamma \delta_{t+1}^V = -V(s_t) + r_t + \gamma r_{t+1} + \gamma^2 V(s_{t+2}) \\
    \hat{A}_t^{(3)} &:= \delta_t^V + \gamma \delta_{t+1}^V + \gamma^2 \delta_{t+2}^V = -V(s_t) + r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 V(s_{t+3}) \\ \\
    \hat{A}_t^{(k)} &:= \sum_{l=0}^{k-1} \gamma^l \delta_{t+l}^V = -V(s_t) + r_t + \gamma r_{t+1} + \dots + \gamma^{k-1} r_{t+k-1} + \gamma^k V(s_{t+k})
\end{aligned}
$$

$\hat{A}\_t^{(k)}$ëŠ” ì„¸ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì§„ë‹¤. Baseline term $-V(s\_t)$ì™€ $k$-stepë§Œí¼ì˜ return, ê·¸ë¦¬ê³  $\gamma^k V(s\_{t+k})$ì´ë‹¤. $k$ì˜ ê°’ì— ë”°ë¼ biasê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ìƒê°í•´ë³´ë©´, baselineì€ Propositionì—ì„œ ì„ì˜ì˜ í•¨ìˆ˜ì´ê¸° ë•Œë¬¸ì— ì¡°ê±´ì„ ë§Œì¡±í•˜ê³ , return ë˜í•œ ë¬¸ì œë  ê²ƒì´ ì—†ê³ , $\gamma^k V(s\_{t+k})$í•­ì´ unbiasedì´ë©´ $\gamma$-justí•˜ê²Œ ëœë‹¤. ë‹¹ì—°í•˜ê²Œë„ $k$ê°€ ë¬´í•œíˆ ì¦ê°€í•˜ë©´ ì´ ê°’ì˜ ì˜í–¥ì€ ë¯¸ë¯¸í•´ì§€ê³  ë‹¤ìŒì„ ì–»ê²Œ ëœë‹¤.

$$
\hat{A}_t^{(\infty)} = \sum_{l=0}^{\infty} \gamma^l \delta_{t+l}^V = -V(s_t) + \sum_{l=0}^{\infty} \gamma^l r_{t+l}
$$

GAE estimatorëŠ” k-step estimation of advantage functionì— ë‹¤ì‹œ exponentially weighted averageì„ ì·¨í•œë‹¤. Weight parameterëŠ” $\lambda$ë¥¼ ì‚¬ìš©í•œë‹¤.

$$
\begin{aligned}
    \hat{A}_t^{\text{GAE}(\gamma, \lambda)} &:= (1-\lambda) \left( \hat{A}_t^{(1)} + \lambda \hat{A}_t^{(2)} + \lambda^2 \hat{A}_t^{(3)} + \dots \right) \\
    &= (1-\lambda) \left( \delta_t^V + \lambda \delta_t^V + \gamma \delta_{t+1}^V + \lambda^2 (\delta_t^V + \gamma \delta_{t+1}^V + \gamma^2 \delta_{t+2}^V) + \dots \right) \\
    &= (1-\lambda) \left( \delta_t^V (1 + \lambda + \lambda^2 + \dots) + \gamma \delta_{t+1}^V (\lambda + \lambda^2 + \lambda^3 + \dots) + \dots \right) \\
    &= (1-\lambda) \left( \delta_t^V \frac{1}{1-\lambda} + \gamma \delta_{t+1}^V \frac{\lambda}{1-\lambda} + \gamma^2 \delta_{t+2}^V \frac{\lambda^2}{1-\lambda} + \dots \right) \\
    &= \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}^V
\end{aligned}
$$

ê·¸ ê²°ê³¼ëŠ” ì‹ ê¸°í•˜ê²Œë„ ë§¤ìš° ê°„ë‹¨í•œ í˜•íƒœì˜ ì‹ì´ ë‚˜ì˜¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. TD($\lambda$)ì™€ í˜•íƒœê°€ ìœ ì‚¬í•œë° ì°¨ì´ì ì€ ë¶„ëª…íˆ ì¡´ì¬í•œë‹¤. TD($\lambda$)ëŠ” value functionì„ ì˜ˆì¸¡í•˜ëŠ”ë°ì— ì‚¬ìš©ëœë‹¤. ë°˜ë©´, GAEëŠ” advantage functionì„ ì˜ˆì¸¡í•˜ëŠ”ë°ì— ì‚¬ìš©ëœë‹¤.

$$
GAE: \qquad \hat{A}_t^{\text{GAE}(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}^V\\
TD(\lambda): \qquad G_t^\lambda = (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)}, \quad \text{where } G_t^{(n)} = \sum_{k=0}^{n-1} \gamma^k r_{t+k} + \gamma^n V(s_{t+n})
$$

GAEì˜ íŠ¹ìˆ˜í•œ ìƒí™©, ì¦‰ $\lambda=1,\lambda=0$ì¼ ë•Œë¥¼ ë¹„êµí•´ë³´ì.

$$
\begin{aligned}
    \text{GAE}(\gamma, 0) : \quad \hat{A}_t &:= \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) \\
    \text{GAE}(\gamma, 1) : \quad \hat{A}_t &:= \sum_{l=0}^{\infty} \gamma^l \delta_{t+l}^V = \sum_{l=0}^{\infty} \gamma^l r_{t+l} - V(s_t)
\end{aligned}
$$

$\text{GAE}(\gamma, 1)$ì˜ ê²½ìš° $k$ì— ì˜í•´ value functionì˜ ì˜í–¥ì´ ì‚¬ë¼ì ¸ë²„ë ¸ê¸° ë•Œë¬¸ì— í•­ìƒ $\gamma$-justí•˜ë‹¤. ë°˜ë©´ $\text{GAE}(\gamma, 0)$ì˜ ê²½ìš° $V=V^{\pi,\gamma}$ë¥¼ ë§Œì¡±í•´ì•¼ $\gamma$-justí•˜ë‹¤. ë‘˜ ì‚¬ì´ì—ëŠ” biasì™€ varianceì˜ tradeoffê°€ ì¡´ì¬í•œë‹¤.

$\gamma$ì™€ $\lambda$ë˜í•œ ì„œë„ ë‹¤ë¥¸ ì—­í• ì„ ê°€ì§€ê³  ìˆë‹¤. $\gamma<1$ì˜ ê²½ìš° value functionì— ë¶™ì–´ ê³±í•´ì§€ê¸° ë•Œë¬¸ì— value function estimationì˜ ì •í™•ë„ì™€ ê´€ê³„ì—†ì´ biasë¥¼ ë‹¬ë¦¬ ê°€ì ¸ì˜¤ê²Œ ëœë‹¤. ë°˜ë©´ $\lambda<1$ëŠ” value function estimationì´ ì •í™•í•œ ê²½ìš°ì—ëŠ” biasë¥¼ ê°€ì ¸ì˜¤ì§€ ì•ŠëŠ”ë‹¤. Biasë¥¼ ê°€ì ¸ì˜¤ëŠ” ì •ë„ë„ ì°¨ì´ë¥¼ ë³´ì´ëŠ”ë° $\lambda$ê°€ $\gamma$ë³´ë‹¤ ë” ì ì€ biasë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ê·¸ ì´ìœ ëŠ” ì‹¤í—˜ ê²°ê³¼ë¥¼ í†µí•´ì„œ ì•Œì•„ë³¸ ìµœì  ê°’ì—ì„œ ë³´í†µ $\lambda$ê°€ $\gamma$ë³´ë‹¤ ì‘ì€ ê°’ì„ ê°–ê¸° ë•Œë¬¸ì´ë‹¤.

## Interpretation as Reward Shaping

$\lambda$ë¥¼ reward shaping í›„ì— ì ìš©ëœ discount factorë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤. Reward shapingì˜ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ transformationì„ ì ìš©í•œ ê²ƒì´ë‹¤.

$$
\tilde{r}(s, a, s') = r(s, a, s') + \gamma \Phi(s') - \Phi(s),
$$

ì´ ì‹ì„ të¶€í„° ì­‰ summationí•˜ë©´

$$
\sum_{l=0}^{\infty} \gamma^l \tilde{r}(s_{t+l}, a_{t+l}, s_{t+l+1}) = \sum_{l=0}^{\infty} \gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \Phi(s_t).
$$

ë‹¤ì‹œ ì´ ì‹ì„ ì´ìš©í•˜ë©´ ê°€ì¹˜í•¨ìˆ˜ë¥˜ë“¤ì„ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$
\tilde{Q}^{\pi, \gamma}(s, a) = Q^{\pi, \gamma}(s, a) - \Phi(s)\\
\tilde{V}^{\pi, \gamma}(s) = V^{\pi, \gamma}(s) - \Phi(s)\\
\tilde{A}^{\pi, \gamma}(s, a) = (Q^{\pi, \gamma}(s, a) - \Phi(s)) - (V^{\pi, \gamma}(s) - \Phi(s)) = A^{\pi, \gamma}(s, a).
$$

ì´ ì•„ì´ë””ì–´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, GAEì˜ policy gradientì— ì ìš©í•œë‹¤. $\gamma\leftarrow \gamma\lambda, \Phi\leftarrow V$ë¡œ ë‘ë©´ reward shapingì˜ ê²°ê³¼ê°€ GAE estimationì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

$$
\sum_{l=0}^{\infty} (\gamma \lambda)^l \tilde{r}(s_{t+l}, a_t, s_{t+l+1}) = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}^{V} = \hat{A}_{t}^{\text{GAE}(\gamma, \lambda)}.
$$

ì¶”ê°€ì ìœ¼ë¡œ, $\gamma$ì™€ GAEê°€ policy gradient estimationì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì•Œì•„ë³´ê¸° ìœ„í•´ response functionì˜ ê°œë…ì„ ì†Œê°œí•œë‹¤. Response functionì´ë€ íŠ¹ì • í–‰ë™ì´ ë¯¸ë˜ ë³´ìƒì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì˜ ì •ë„ë¥¼ ìˆ˜ì¹˜í™”í•œ ê²ƒì´ë‹¤. ê°•í™”í•™ìŠµì—ì„œëŠ” í–‰ë™ì˜ ì˜í–¥ì„ ì˜¤ëœ ì‹œê°„ ë’¤ì— ì•Œê²Œ ë˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤. ì´ëŸ¬í•œ long-range dependencyë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ response functionì€ ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤.

$$
\chi(l; s_t, a_t) = \mathbb{E} \left[ r_{t+l} \mid s_t, a_t \right] - \mathbb{E} \left[ r_{t+l} \mid s_t \right].\\
A^{\pi, \gamma}(s, a) = \sum_{l=0}^{\infty} \gamma^l \chi(l; s, a)
$$

ì •ì˜ë¥¼ ë³´ë©´ advantage functionì´ response functionì˜ summationìœ¼ë¡œ í‘œí˜„ëœë‹¤. ì´ì œ policy gradientë¥¼ response functionìœ¼ë¡œ í‘œí˜„í•´ë³´ë©´

$$
\nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) A^{\pi, \gamma}(s_t, a_t) = \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \sum_{l=0}^{\infty} \gamma^l \chi(l; s_t, a_t).
$$

ë”°ë¼ì„œ $\gamma<1$ì¼ ë•Œ $l \gg \frac{1}{(1 - \gamma)}$ë¥¼ ë§Œì¡±í•˜ë©´ ì´í›„ì˜ response termì˜ ì˜í–¥ì€ ë¬´ì‹œí•  ìˆ˜ ìˆì„ ì •ë„ë¡œ ì‘ì•„ì§„ë‹¤.

<details>
<summary>$l$ì˜ ê¸°ì¤€</summary>
<div markdown="1">

ë„¤, ì´ ë¶€ë¶„ì„ ìˆ˜í•™ì ìœ¼ë¡œ ì—„ë°€í•˜ê²Œ ì‚´í´ë³´ì. ë…¼ë¬¸ì—ì„œ ë§í•˜ëŠ” **\\( l \gg 1/(1-\gamma) \\) ì¸ í•­ë“¤ì€ ê±°ì˜ ë¬´ì‹œë¨**ì„ ì¦ëª…í•˜ê¸° ìœ„í•´, í• ì¸ìœ¨ \\(\gamma\\)ê°€ ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì •ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•´ë³´ì.

---

## **âœ… ìˆ˜í•™ì  ë¶„ì„: ì™œ \\( l \gg 1/(1-\gamma) \\)ì´ë©´ ê¸°ì—¬ë„ê°€ ê±°ì˜ 0ì´ ë˜ëŠ”ê°€?**

ìš°ë¦¬ëŠ” ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì •ì‹ì—ì„œ ë“±ì¥í•˜ëŠ” **í• ì¸ ê³„ìˆ˜**ì˜ í˜•íƒœë¥¼ í™•ì¸í•´ì•¼ í•¨:

\\(
\sum_{l=0}^{\infty} \gamma^l \chi(l; s_t, a_t)
\\)

ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ì ì€ **\\( \gamma^l \\)** í•­ì´ **ì§€ìˆ˜ì ìœ¼ë¡œ ê°ì†Œ**í•œë‹¤ëŠ” ê²ƒ.

### **ğŸ“Œ ì§€ìˆ˜ ê°ì†Œìœ¨ ë¶„ì„**
\\(\gamma^l\\) í•­ì´ ê°ì†Œí•˜ëŠ” ì†ë„ë¥¼ ë¶„ì„í•˜ë ¤ë©´, ë‹¤ìŒ ë¶€ë“±ì‹ì„ ê³ ë ¤í•˜ì:

\\(
\gamma^l \leq e^{-l(1-\gamma)}
\\)

ì´ ë¶€ë“±ì‹ì´ ì„±ë¦½í•˜ëŠ” ì´ìœ ëŠ” ë¡œê·¸ë¥¼ ì·¨í•´ì„œ í™•ì¸í•  ìˆ˜ ìˆìŒ:

\\(
\ln(\gamma^l) = l \ln \gamma \approx l(- (1-\gamma)) = -l(1-\gamma),
\\)

ì¦‰,

\\(
\gamma^l \approx e^{-l(1-\gamma)}.
\\)

### **ğŸ“Œ \\( l \gg 1/(1-\gamma) \\) ì¸ ê²½ìš°**
- ë§Œì•½ \\( l \gg 1/(1-\gamma) \\)ë¼ë©´,  
  \\(
  l(1-\gamma) \gg 1
  \\)
  ì´ë¯€ë¡œ,
  \\(
  e^{-l(1-\gamma)} \approx 0.
  \\)

ì¦‰, **í• ì¸ìœ¨ì´ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡, \\( \gamma^l \\)ëŠ” \\( l \approx 1/(1-\gamma) \\) ì´í›„ë¶€í„° ë§¤ìš° ì‘ì•„ì§.**  
ë”°ë¼ì„œ \\( l \gg 1/(1-\gamma) \\) ì¸ ê²½ìš°, ê¸°ì—¬ë„ê°€ ì‚¬ì‹¤ìƒ 0ì— ê°€ê¹Œì›Œì§.

---

## **âœ… ê²°ë¡ **
- **í• ì¸ìœ¨ \\( \gamma \\)ê°€ 1ì— ê°€ê¹Œìš°ë©´, ë¨¼ ë¯¸ë˜ì˜ ë³´ìƒë„ ì¤‘ìš”í•˜ê²Œ ë°˜ì˜ë¨.**
- í•˜ì§€ë§Œ, **\\( l \gg 1/(1-\gamma) \\) ì´í›„ë¶€í„°ëŠ” ê¸°ì—¬ë„ê°€ ê±°ì˜ 0ì´ ë¨.**
- **ì´ê²ƒì´ ë…¼ë¬¸ì—ì„œ "í• ì¸ìœ¨ \\( \gamma < 1 \\)ì„ ì‚¬ìš©í•˜ë©´ \\( l \gg 1/(1-\gamma) \\) í•­ë“¤ì€ ë¬´ì‹œëœë‹¤"ë¼ê³  ë§í•˜ëŠ” ìˆ˜í•™ì  ê·¼ê±°!** ğŸš€

</div>
</details>
\
ë‹¤ì‹œ reward shapingìœ¼ë¡œ ëŒì•„ì™€ì„œ $\Phi=V^{\pi,\gamma}$ì¼ ë•Œ, $\mathbb{E} [\tilde{r}\_{t+l} \mid s\_t, a\_t] = \mathbb{E} [\tilde{r}\_{t+l} \mid s\_t] = 0 \text{ for } l > 0$ì´ë‹¤($V$ì˜ ì •ì˜ë¥¼ ìƒê°í•´ ë³´ë©´ ëœë‹¤). ê²°ê³¼ì ìœ¼ë¡œ GAEë¥¼ ì´ìš©í•´ reward shapingí•˜ê²Œ ë˜ë©´ response functionì´ temporally extendedì¸ í˜•íƒœì—ì„œ immediateì¸ í˜•íƒœë¡œ ë³€í˜•ëœë‹¤(ë‹¤ë£¨ê¸°ê°€ ìš©ì´í•´ì§„ë‹¤ëŠ” ëœ»). ê±°ê¸°ì— $\lambda$ë¥¼ ì´ìš©í•˜ì—¬ ì¢€ ë” ê°€íŒŒë¥¸ discountë¥¼ ì œê³µí•˜ì—¬ long delaysì— ëŒ€í•œ noiseì˜ ì˜í–¥ì„ ì¤„ì¸ë‹¤.

ì†Œê²°ë¡  : GAEë¥¼ í†µí•´ advantage function estimationì„ ì •ì˜í•˜ë©´ reward shapingì˜ ê´€ì ì—ì„œ response functionì´ ì¦‰ê°ì ì¸ í˜•íƒœë¡œ ë°”ë€Œê²Œ ë˜ì–´ ë‹¤ë£¨ê¸° ìš©ì´í•´ì§„ë‹¤. ì´ë¥¼ $\lambda$ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ë£¨ê²Œ ëœë‹¤.

## Value function estimation

ì „ì²´ processì—ì„œëŠ” policy updateë¥¼ ìœ„í•œ GAE ë§ê³ ë„ ê°€ì¹˜ í•¨ìˆ˜ updateë¥¼ ìœ„í•œ ëŒ€í•œ estimationë„ í•´ì•¼ í•˜ëŠ”ë° ì—¬ê¸°ì„œëŠ” trust region ë°©ì‹ì„ ì‚¬ìš©í•œ QPë¥¼ í’€ê²Œëœë‹¤. ê¸°ì¡´ í’€ê³ ì í•˜ëŠ” ë¬¸ì œëŠ”

$$
\text{minimize}_{\phi} \quad \sum_{n=1}^{N} \| V_{\phi}(s_n) - \hat{V}_n \|^2\\
\text{subject to} \quad \frac{1}{N} \sum_{n=1}^{N} \frac{\| V_{\phi}(s_n) - V_{\phi_{\text{old}}}(s_n) \|^2}{2\sigma^2} \leq \epsilon.
$$

ì¸ë°, ì•„ë˜ì˜ QP ë¬¸ì œë¥¼ í’€ì–´ í•´ê²°í•  ìˆ˜ ìˆë‹¤.

$$
\text{minimize}_{\phi} \quad g^T (\phi - \phi_{\text{old}})\\
\text{subject to} \quad \frac{1}{N} \sum_{n=1}^{N} (\phi - \phi_{\text{old}})^T H (\phi - \phi_{\text{old}}) \leq \epsilon.
$$


## Experiments

$$
\begin{align*}
    &\text{minimize }_{\theta} \quad L_{\theta_{old}}(\theta) \\
    &\text{subject to} \quad \overline{D}_{KL}^{\theta_{old}}(\pi_{\theta_{old}}, \pi_{\theta}) \leq \epsilon
\end{align*}
\\
\text{where }
L_{\theta_{old}}(\theta) \equiv \frac{1}{N} \sum_{n=1}^{N} \frac{\pi_{\theta}(a_n \mid s_n)}{\pi_{\theta_{old}}(a_n \mid s_n)} \hat{A}_n, \quad
\overline{D}_{KL}^{\theta_{old}}(\pi_{\theta_{old}}, \pi_{\theta}) = \frac{1}{N} \sum_{n=1}^{N} D_{KL}(\pi_{\theta_{old}}(\cdot \mid s_n) \parallel \pi_{\theta}(\cdot \mid s_n))
$$


Policy updateëŠ” TRPOë¥¼ ì‚¬ìš©í•œë‹¤.



<center>
<img src='{{"assets/images/GAE/gae4.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">TRPO update with GAE</figcaption>
</center>

ë‹¤ì–‘í•œ $\lambda,\gamma$ì— ëŒ€í•´ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ê°€ì¹˜ í•¨ìˆ˜(TD error)ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  time-dependent baseline(ê° time stepì—ì„œì˜ í‰ê·  ë¦¬í„´)ì„ ì‚¬ìš©í•œ ê²½ìš°(No VF) ì„±ëŠ¥ì´ í˜„ì €íˆ ë–¨ì–´ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. No VFëŠ” GAE ëŒ€ì‹  ë‹¤ìŒì˜ advantage function estimationì„ ì‚¬ìš©í•œë‹¤.

$$
\hat{A}_t=R_t-B_t
$$

$R\_t$ëŠ” Monte Carlo Returnì´ê³ , $B\_t$ëŠ” time stepë³„ í‰ê·  ë¦¬í„´ì´ë‹¤.

<center>
<img src='{{"assets/images/GAE/gae1.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Experimental Results</figcaption>
</center>
<center>
<img src='{{"assets/images/GAE/gae2.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Experimental Results</figcaption>
</center>
<center>
<img src='{{"assets/images/GAE/gae3.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Experimental Results</figcaption>
</center>

## Discussion

Policy gradientì˜ ì •í™•ë„ëŠ” ê°€ì¹˜ í•¨ìˆ˜ estimationì˜ ì˜¤ì°¨(Bellman error, projected Bellman error)ì˜ ì˜í–¥ì„ ë°›ëŠ”ë‹¤. ë§Œì•½ ë‘˜ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ê³µì‹í™”í•  ìˆ˜ ìˆë‹¤ë©´ policy gradientì—ì„œ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

ë˜ ë‹¤ë¥¸ ì—°êµ¬ ë°©í–¥ì€ policyì™€ value functionì— ëŒ€í•´ shared êµ¬ì¡°ë¥¼ ê°–ë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤. ê³µí†µì ì¸ í‘œí˜„ í•™ìŠµì´ ê°€ëŠ¥í•´ì ¸ í•™ìŠµì´ ë¹¨ë¼ì§ˆ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì´ ìƒíƒœì—ì„œ ìµœì í™”ê°€ ì˜ ì´ë£¨ì–´ì§€ëŠ”ì§€, ìˆ˜ë ´ì„±ì´ ë³´ì¥ë˜ëŠ”ì§€ì— ëŒ€í•œ ë¶€ë¶„ì€ ê²€ì¦í•  í•„ìš”ê°€ ìˆë‹¤.

ì•„ìš¸ëŸ¬ Continuous í™˜ê²½ì—ì„œë„ GAEê°€ ì˜ ë™ì‘í•˜ëŠ”ì§€ ì•Œì•„ë³¼ í•„ìš”ê°€ ìˆë‹¤.