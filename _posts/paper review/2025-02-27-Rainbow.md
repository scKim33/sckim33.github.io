---
title: "[Rainbow] Rainbow: Combining Improvements in Deep Reinforcement Learning"
last_modified_at: 2025-02-27
categories:
  - paper_review
tags:
  - Rainbow
  - DQN
  - Reinforcement Learning
  - Off policy
excerpt: "Rainbow paper review"
use_math: true
classes: wide
---

> AAAI 2018. [[Paper](https://arxiv.org/abs/1710.02298)]
> Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, David Silver  
> 6 Oct 2017

## Summary

DQN의 다양한 variation들을 합쳐서 가장 좋은 성능을 보여준 논문이다.

## Introduction

DQN 논문 이후 다양한 variation들이 등장한다. Double DQN은 Q value overestimation 문제를 다룬다. PER은 더 많은 양을 배운 데이터를 우선 배치하여 데이터 효율성을 높인다. Dueling DQN은 value와 advantage를 분리시켜 action에 대한 일반화를 유도한다. A3C는 n-step Q-learning을 통해 bias-variance tradeoff를 한다. Distributional Q-learning은 Q value를 확률 분포 형태로 예측한다. Noisy DQN은 신경망의 가중치에 noise를 추가하여 exploration을 유도한다.

## Background

## Extensions to DQN
### DQN

<center>
<img src='{{"assets/images/Rainbow/rainbow1.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">DQN algorithm</figcaption>
</center>

### Double DQN
기존의 DQN 방식은 어떤 이유에서든지 간에(environmental noise, function approximation, non-stationarity, etc.) Q value estimation의 오차가 있을 때, upward bias를 유발할 수 있다.

> $\textbf{Theorem 1.}$ Consider a state s in which all the true optimal action values are equal at $Q\_\*(s,a) = V\_\*(s)$ for some $V\_\*(s)$. Let $Q\_t$ be arbitrary value estimates that are on the whole unbiased in the sense that $\sum\_a(Q\_t(s,a) - V\_\*(s)) = 0$, but that are not all correct, such that $\frac{1}{m}\sum\_a(Q\_t(s,a) - V\_\*(s))^2 = C$ for some $C > 0$, where $m\geq 2$ is the number of actions in $s$. Under these conditions, $\max\_a Q\_t(s,a) \geq V\_\*(s) + \sqrt{\frac{C}{m-1}}.$ This lower bound is tight. Under the same conditions, the lower bound on the absolute error of the Double Q-learning estimate is zero.

<details>
<summary>Proof of Theorem 1.</summary>
<div markdown="1">
귀류법을 통해 증명한다.


다음과 같이 estimation error를 action마다 정의할 수 있다.

$$
\epsilon_a = Q_t(s,a)-V_*(s)
$$

그리고 어떤 임의의 값보다 작은 $\epsilon\_a$가 존재한다고 가정한다. ($C$의 값을 조정하면 임의의 값으로 만들 수 있다.)

$$
\max_a \epsilon_a < \sqrt{\frac{C}{m-1}}
$$

Strictly positive인 error를 $\epsilon\_j^-$라 하고 개수를 $n$이라 하자. $Q\_t$가 unbiased이기 때문에 error의 합은 0을 만족한다. 잘 생각해보면 strictly positive만으로(혹은 strictly negative만으로) error가 이루어질 수 없음을 알 수 있다.

$$
n\leq m-1\\
\text{Then, } \sum_{i=1}^{n} \epsilon_i^{+} \leq n \max_i \epsilon_i^{+} < n \sqrt{\frac{C}{m-1}},\\
\text{also, } \sum_{j=1}^{m-n} |\epsilon_j^{-}| < n \sqrt{\frac{C}{m-1}}\Rightarrow\max_j |\epsilon_j^{-}| < n \sqrt{\frac{C}{m-1}}.
$$

Holder's inequality를 이용한다.

$$
\textbf{Hölder's inequality: }\sum_{i=1}^{n} |a_i b_i| \leq \left( \sum_{i=1}^{n} |a_i|^p \right)^{\frac{1}{p}} \left( \sum_{i=1}^{n} |b_i|^q \right)^{\frac{1}{q}}
\quad\text{such that}\quad \frac{1}{p} + \frac{1}{q} = 1
$$

$$
\begin{align*}
\sum_{j=1}^{m-n} (\epsilon_j^{-})^2 &\leq \sum_{j=1}^{m-n} |\epsilon_j^{-}| \cdot \max_j |\epsilon_j^{-}|\\
&< n \sqrt{\frac{C}{m-1}} \sqrt{\frac{C}{m-1}}.
\end{align*}
$$

이제 positive와 negative term을 합치면

$$
\begin{align*}
  \sum_{a=1}^{m} (\epsilon_a)^2 &= \sum_{i=1}^{n} (\epsilon_i^{+})^2 + \sum_{j=1}^{m-n} (\epsilon_j^{-})^2\\
  &< n \frac{C}{m-1} + n \sqrt{\frac{C}{m-1}} \sqrt{\frac{C}{m-1}}\\
  &= C \frac{n(n+1)}{m-1} \leq mC.
\end{align*}
$$

하지만 가정에 의해 $\sum\_{a=1}^{m} \epsilon\_a^2 < mC$이고 모순이다. 따라서 

$$
\max_a \epsilon_a \geq \sqrt{\frac{C}{m-1}}\quad\forall\epsilon_a
$$

을 만족한다. 따라서 증명이 완료된다.

</div>
</details>
\
그림을 통해서도 알아볼 수 있다. $\max$연산이 근본적으로 overestimation을 유발하게 된다. 연두색 선은 하나의 action에 대한 Q estimation이다. DQN을 그대로 쓰면 연두색 선 여러개 중 max값만을 취하게 되어 가끔 튀는 값들이 있으면 매우 부정확해질 수 있다. 반면 Double DQN은 하나의 신경망에서 과대평가가 일어났다고 하더라도 다른 신경망에서 과대평가가 일어나지 않으면 올바른 Q로 학습하게 된다.

<center>
<img src='{{"assets/images/Rainbow/rainbow2.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Over-estimation of Q-value</figcaption>
</center>

<center>
<img src='{{"assets/images/Rainbow/rainbow3.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Over-estimation in Atari</figcaption>
</center>

DQN과 비교하여 딱 한가지만 바꿔주면 된다. Target value를 추정할 때 사용되는 $Q$와 maximum을 취할때 사용되는 $Q$에 서로 다른 network를 적용하기만 하면 된다.

$$
Y_t^{\text{DoubleDQN}} \equiv R_{t+1} + \gamma Q(S_{t+1}, \arg\max_{a} Q(S_{t+1}, a; \theta_t); \theta^-)
$$

### Prioritized replay

기존 DQN은 replay buffer에서 uniform하게 transition tuple을 샘플링한다. Prioritized replay에서는 uniform 분포가 아닌 TD error에 비례하도록 확률을 조정한다. 즉, 업데이트가 크게 발생한 transition을 중요한 것으로 보고 더 많이 샘플링하게 된다는 것이다. $\omega$는 hyperparameter로 높을 수록 TD error의 영향을 키우는 역할을 한다(0이면 uniform 분포).

$$
p_t \propto \left| R_{t+1} + \gamma_{t+1} \max_{a'} q_{\theta'}(S_{t+1}, a') - q_{\theta}(S_t, A_t) \right|^{\omega}
$$

원 논문에서 제공하는 알고리즘은 다음과 같다.

<center>
<img src='{{"assets/images/Rainbow/rainbow4.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">DDQN with Prioritized replay buffer</figcaption>
</center>

### Dueling Netowrks

<center>
<img src='{{"assets/images/Rainbow/rainbow5.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Vanilla network vs Dueling network</figcaption>
</center>

Dueling network는 기존 DQN이 value 추정에 사용하던 network 구조를 변경한다. 기존에는 Q value estimation이 output으로 나오기 때문에 action의 수만큼 estimation을 얻는다. 하지만 dueling network는 value estimation + advantage function estimation으로 구분하여 1개 + action 수만큼의 output을 얻는다. 두 개를 합치면 쉽게 Q value estimation을 구할 수 있다.

<center>
<img src='{{"assets/images/Rainbow/rainbow6.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Each dueling network output concentrates on different sections of the image</figcaption>
</center>

Saliency map을 통해 알아본 결과 advantage function estimation의 경우 눈 앞의 장애물에 더 집중하여 효과적인 action을 이끌어 낼 수 있다.

네트워크 구조가 다른 것 말고는 DQN과 동일하다. 알고리즘도 같다.

### Distributional DQN(Categorical DQN(C51))

기존에는 Q estimation을 고정된 값으로 학습한다. 하지만 이런 상황이 있다고 가정하자.
> Action을 취했을 때 50% 확률로 0의 리워드, 나머지 50% 확률로 10의 리워드를 받는다.

이런 경우에 Q value를 5로 추정하는 것은 어색하다. 이렇게 보상의 분포는 스칼라 값처럼 단순한 형태가 아닐 수 있다. 따라서 이를 표현하기 위해 distributional DQN이 제시된다.



원 논문에서는 기존의 Bellman operator를 distribution에 맞게 바꾸는 과정을 거친다. Wasserstein metric과 같이 분포에서의 적절한 metric으로 distribution Bellman operator가 수렴함을 보인다(optimality는 아니다). 그리고 이를 근사하기 위해 구간을 나누어 이산확률분포로 두고 구간의 개수를 51개로 정하였다. 따라서 모델의 output은 51개의 확률값으로 나타난다. Bellman operator에 의해 구간이 찌그러지거나 할 수 있다. 이를 적절히 projection하여 등간격의 구간에 확률을 분배한다. 또한 Wasserstein metric은 샘플링 기반에서는 적절하지 않기 때문에 KL divergence로 변경한다.

<center>
<img src='{{"assets/images/Rainbow/rainbow9.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Distribution of Q estimate</figcaption>
</center>

알고리즘은 다음과 같다.

<center>
<img src='{{"assets/images/Rainbow/rainbow8.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Categorical DQN</figcaption>
</center>

### Noisy DQN

기존에는 탐색을 유도하기 위해서 epsilon greedy 같은 방식을 사용한다. 하지만 네트워크의 가중치에 noise를 첨가하여 자연스러운 학습을 유도할 수 있다.

기존의 input-output의 형태

$$
y=wx+b
$$

는 다음과 같이 불확실성을 포함한 항으로 변경된다.

$$
y \stackrel{\text{def}}{=} (\mu^w + \sigma^w \odot \varepsilon^w) x + \mu^b + \sigma^b \odot \varepsilon^b
$$

$\mu,\sigma$는 학습가능한 파라미터이다. Noisy network가 epsilon greedy 방식과 비교하여 갖는 장점은 $\varepsilon$과 같이 튜닝해야 할 파라미터가 없다는 점이다. 또한 고정된 비율로 exploration을 하는 epsilon greedy와 다르게 noisy network는 학습 초반에 부정확한 Q estimate로 인해 더욱 왕성하게 탐색하다가 학습 후반에는 Q estimate가 어느정도 정확해지면서 탐색이 점진적으로 줄어든다. 알고리즘에 제약을 두지 않는다는 장점도 있다. 알고리즘도 DQN과 비교하여 네트워크 파라미터를 샘플링하는 과정 하나가 추가된다.

<center>
<img src='{{"assets/images/Rainbow/rainbow7.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">NoisyNet DQN</figcaption>
</center>

## The integrated agent

모든 방법론을 다 사용한다. 먼저 DQN의 1-step return을 multi-step distributional return으로 변경한다. $\mathbf{p}\_{\hat{\theta}}$는 확률밀도함수이고 $z$는 discrete support, $d\_t$는 그에 해당하는 확률분포이다.

$$
R_t^{(n)} \equiv \sum_{k=0}^{n-1} \gamma_t^{(k)} R_{t+k+1}\\
\mathbf{d}_t^{(n)} = \left(R_t^{(n)} + \gamma_t^{(n)} \mathbf{z}, \mathbf{p}_{\hat{\theta}}(S_{t+n}, a^*_{t+n})\right)
$$

DQN의 MSE loss 대신 KL divergence를 사용한다.

$$
D_{\text{KL}} \left( \Phi_{\mathbf{z}} \mathbf{d}_t^{(n)} \middle\| d_t \right)
$$

Double Q-learning을 적용하기 위해 n-step 이후 state $S\_{t+n}$에서 greedy action $a^\*\_{t+n}$을 선택하고, 이에 대응되는 $\mathbf{p}\_{\hat{\theta}}$을 target network가 평가하게 된다.

Prioritized experience replay에서는 TD error에 비례하는 priority를 제시하였으나 여기서는 distribution의 유사성을 판단할 수 있도록 KL divergence를 기준으로 한다.

$$
p_t \propto \left( D_{\text{KL}} \left( \Phi_{\mathbf{z}} \mathbf{d}_t^{(n)} \middle\| d_t \right) \right)^{\omega}
$$

Dueling network의 경우 return distribution에 맞게끔 조정한다. $\phi$는 shared된 네트워크, 분자 분모 각각 앞의 첫 항은 value function estimation, 뒤의 두 항은 advantage function estimation(Q function estimation - value function estimation)이다. value function estimation은 나눠진 구간의 개수만큼의 차원을, advantage function estimation은 나눠진 구간 x action의 수만큼의 차원을 갖게 된다.

$$
p_{\theta}^i(s,a) = \frac{\exp(v_{\eta}^i(\phi) + a_{\psi}^i(\phi,a) - \bar{a}_{\psi}^i(s))}{\sum_j \exp(v_{\eta}^j(\phi) + a_{\psi}^j(\phi,a) - \bar{a}_{\psi}^j(s))},\\
\text{where } \phi = f_{\xi}(s) \text{ and } \bar{a}_{\psi}^i(s) = \frac{1}{N_{\text{actions}}} \sum_{a'} a_{\psi}^i(\phi, a').
$$

마지막으로 Noisy network를 위해 linear의 파라미터에 gaussian noise를 추가한다.

## Experimental methods

## Analysis

<center>
<img src='{{"assets/images/Rainbow/rainbow10.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Results</figcaption>
</center>

<center>
<img src='{{"assets/images/Rainbow/rainbow11.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Ablation study</figcaption>
</center>

전범위적으로 가장 크게 영향을 미친 요소는 n-step training, priority experience replay이다. 장기적인 학습 결과도 포함한다면 distributional estimate도 포함할 수 있다.

## Discussion