---
title: "[SAC] Soft Actor-Critic Algorithms and Applications"
last_modified_at: 2025-03-03
categories:
  - paper_review
tags:
  - SAC
  - Reinforcement Learning
  - Off policy
  - Model free
  - Google
excerpt: "SAC paper review"
use_math: true
classes: wide
---

> ICML 2018. [[Paper](https://arxiv.org/abs/1812.05905)]  
> Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, Sergey Levine  
> 13 Dec 2018

## Summary

Model-free RL이 요구하는 많은 양의 samples, 그리고 hyperparameter tuning에 민감한 부분은 real world application을 어렵게한다. SAC는 maximum entropy RL을 이용하여 미래 보상을 추구하면서도 동시에 탐색하도록 한다. 그 결과 안정적인 성능을 보인다.

## Introduction

기존 model-free 알고리즘들은 샘플 효율성이 낮다. 간단한 task에도 수백만 개의 데이터가 요구될 수 있다. 또한 하이퍼파라미터에 민감하다. Learning rate, exploration constants 등에 의해 성능이 크게 달라질 수 있다. On policy의 경우 policy가 업데이트 될 때마다 데이터를 새로 얻어야 하므로 샘플 수집 비용이 크다. Off policy의 경우 샘플을 재사용할 수 있지만 신경망을 사용한 nonlinear function approximation이 학습을 불안정하게 만든다.

SAC에서는 기존의 목적함수에 Entropy 항을 추가하여 행동의 다양성을 유지한다. 하지만 temperature hyperparameter 설정이 어렵다는 문제가 있다. 논문에는 이를 보완하여 자동적으로 temperature를 조정하는 방식을 추가적으로 제시한다.

## Related Work

이전의 연구들은 on policy 상에서 entropy maximization을 적용하여 안정성을 추구하지만 샘플의 효율성이 낮은 편이다. TRPO, PPO의 경우 entropy를 policy regularization을 위해 사용한다. 샘플 효율성을 높이기 위해 off policy 샘플을 활용하는 방법이 있었지만 완전히 off policy로 학습하지는 못하거나, 복잡한 보정 방법을 사용해야 하는 한계가 있다. 

이전 off policy 연구 중에는 DDPG가 있다. DDPG는 학습 안정성이 떨어지고, 하이퍼파라미터에 민감한 단점이 있다. 이는 고차원 문제에서 성능이 떨어지는 원인이 된다. SVG(0)도 SAC와 같이 policy gradient와 Q learning을 결합하였지만 최대화하는 목적 함수는 여전히 미래 보상만을 다룬다.

Maximum entorpy RL 측면에서는 inverse RL, optimal control, MAP-based RL, GPS(guided policy search) 등으로 연구되고 있다. 기존 연구들은 discrete action space를 다루거나, Gaussian 근사를 이용하여 continuous action space에 적용하거나, sampling network를 사용하여 optimal policy의 샘플을 뽑는 soft Q learning이 있지만 true actor-critic 구조는 아니다.


## Preliminaries

### Maximum Entropy Reinforcement Learning

Maximum entropy RL에서 목적 함수는 기대 보상 + entropy의 형태이다.

$$
\pi^* = \arg\max_{\pi} \sum_{t} \mathbb{E}_{(s_t, a_t) \sim \rho_{\pi}} \left[ r(s_t, a_t) + \alpha \mathcal{H} (\pi (\cdot \mid s_t)) \right]
$$

여기서 $\alpha$는 하이퍼파라미터로 stochasticity를 조절하는 역할을 한다.

## From Soft Policy Iteration to Soft Actor-Critic

### Soft Policy Iteration

Tabular setting, fixed temperature의 가정에서 시작한다. Soft policy iteration에서는 다음과 같이 변경된 value function을 사용한다.

$$
\mathcal{T}^{\pi} Q(s_t, a_t) \triangleq r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p} \left[ V(s_{t+1}) \right],\\
\text{where,}\quad V(s_t) = \mathbb{E}_{a_t \sim \pi} \left[ Q(s_t, a_t) - \alpha \log \pi(a_t \mid s_t) \right]
$$

Soft policy evaluation을 통해 soft Q function을 얻을 수 있다. 기존 policy evaluation과 같이 soft policy evaluation도 수렴한다.

> $\textbf{Lemma 1.}$ (Soft Policy Evaluation). Consider the soft Bellman backup operator $\mathcal{T}^{\pi}$ and a mapping $Q^0 : \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ with $\|\mathcal{A}\| < \infty$ and define $Q^{k+1} = \mathcal{T}^{\pi} Q^k$. Then the sequence $Q^k$ will converge to the soft Q-function of $\pi$ as $k \rightarrow \infty$.

이제 policy improvement 과정을 살펴보면, soft Q function의 exponential에 대해 policy를 업데이트한다.

$$
\pi_{\text{new}}(a \mid s) \propto \exp(Q_{\text{soft}}(s, a))
$$

하지만 이 경우 intractable 하기 때문에 tractable한 형태로 바꾸어 준다. 여기서는 KL divergence를 이용한 information projection을 이용한다.

$$
\pi_{\text{new}} = \arg \min_{\pi' \in \Pi} \text{D}_{\text{KL}} \left( \pi' (\cdot \mid s_t) \parallel \frac{\exp \left( \frac{1}{\alpha} Q_{\pi_{\text{old}}} (s_t, \cdot) \right)}{Z_{\pi_{\text{old}}} (s_t)} \right)
$$

$Z\_{\pi\_{\text{old}}} (s\_t)$는 normalization term인데 new policy의 gradient에 영향을 주지 않으므로 무시할 수 있다. 이렇게 업데이트된 policy는 policy improvement를 만족하게 된다.

> $\textbf{Lemma 2}$ (Soft Policy Improvement). Let $\pi\_{\text{old}} \in \Pi$ and let $\pi\_{\text{new}}$ be the optimizer of the minimization problem defined in Equation 4. Then $Q^{\pi\_{\text{new}}} (s\_t, a\_t) \geq Q^{\pi\_{\text{old}}} (s\_t, a\_t)$ for all $(s\_t, a\_t) \in \mathcal{S} \times \mathcal{A}$ with $\|\mathcal{A}\| < \infty$.

위 과정을 반복하면 optimal maximum entropy policy로 수렴한다.

> $\textbf{Theorem 1}$ (Soft Policy Iteration). Repeated application of soft policy evaluation and soft policy improvement from any $\pi \in \Pi$ converges to a policy $\pi^\*$ such that $Q^{\pi^\*} (s\_t, a\_t) \geq Q^{\pi} (s\_t, a\_t)$ for all $\pi \in \Pi$ and $(s\_t, a\_t) \in \mathcal{S} \times \mathcal{A}$, assuming $\|\mathcal{A}\| < \infty$.


### Soft Actor-Critic

Soft policy iteration의 경우 discrete space에서만 그 수렴성이 보장되어 있다. 이를 large continuous domain에 적용하려면 실질적으로 approximation을 할 수 밖에 없다. 따라서 soft Q function과 policy에 approximator를 사용하고 policy evaluation과 improvement를 수렴할때까지 적용하는 대신에 stochastic gradient descent를 사용한다. soft Q function 업데이트를 위해 다음의 soft Bellman residual을 사용한다.

$$
J_Q(\theta) = \mathbb{E}_{(s_t, a_t) \sim \mathcal{D}} \left[ \frac{1}{2} \left( Q_{\theta} (s_t, a_t) - \left( r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p} [V_{\theta} (s_{t+1})] \right) \right)^2 \right],\\
\hat{\nabla}_{\theta} J_Q (\theta) = \nabla_{\theta} Q_{\theta} (a_t, s_t) \left( Q_{\theta} (s_t, a_t) - \left( r(s_t, a_t) + \gamma (Q_{\theta} (s_{t+1}, a_{t+1}) - \alpha \log \pi_{\phi} (a_{t+1} \mid s_{t+1})) \right) \right)
$$

Target value를 구할 때는 target soft Q-function network를 따로 두어 학습 안정성을 높인다. 이제 policy 업데이트 식을 살펴보면

$$
J_{\pi} (\phi) = \mathbb{E}_{s_t \sim \mathcal{D}} \left[ \mathbb{E}_{a_t \sim \pi_{\phi}} \left[ \alpha \log \pi_{\phi} (a_t \mid s_t) - Q_{\theta} (s_t, a_t) \right] \right]
$$

이대로 gradient를 계산하는 것은 변화하는 분포에 대한 action sampling을 고려해야 하므로 번거롭다. 또한 high variance 문제가 있을 수 있다. 따라서 action sampling에 reparameterization trick을 사용한다.

$$
a_t = f_{\phi} (\epsilon_t; s_t)
$$

이를 이용하여 spherical Gaussian과 같이 고정된 분포로부터 noise vector를 샘플링하여 variance를 줄일 수 있다.

$$
J_{\pi} (\phi) = \mathbb{E}_{s_t \sim \mathcal{D}, \epsilon_t \sim \mathcal{N}} \left[ \alpha \log \pi_{\phi} (f_{\phi} (\epsilon_t; s_t) \mid s_t) - Q_{\theta} (s_t, f_{\phi} (\epsilon_t; s_t)) \right],\\
\hat{\nabla}_{\phi} J_{\pi} (\phi) = \nabla_{\phi} \alpha \log (\pi_{\phi} (a_t \mid s_t)) + \left( \nabla_{a_t} \alpha \log (\pi_{\phi} (a_t \mid s_t)) - \nabla_{a_t} Q(s_t, a_t) \right) \nabla_{\phi} f_{\phi} (\epsilon_t, s_t).
$$

> Q: Policy 업데이트 시 importance sampling을 요구하지 않는 이유?\\
> Policy 업데이트 시 importance sampling이 필요한 경우는 이전의 policy로부터 얻은 데이터를 통해 업데이트 할 때이다. SAC의 경우 soft Q value에 따른 exponential값을 policy가 학습할 때 action을 필요로 한다. 하지만 그 action은 이전 policy에서 얻은 transition tuple에 있는 action이 아니라 current policy로 뽑아낸 action이 된다. 그렇기 때문에 이전 policy의 영향은 받지 않고 IS도 필요로 하지 않는다.

> Q: 그러면 replay buffer에서 action은 왜 존재하나요?\\
> Critic을 학습할 때에는 replay buffer에 있는 값을 그대로 사용한다.

## Automating Entropy Adjustment for Maximum Entropy RL

온도를 적절하게 조절하기 위해 다음과 같이 특정 엔트로피 값 이상의 제약조건을 둔다.

$$
\max_{\pi_{0:T}} \mathbb{E}_{\rho_{\pi}} \left[ \sum_{t=0}^{T} r(s_t, a_t) \right] 
\quad \text{s.t.} \quad \mathbb{E}_{(s_t, a_t) \sim \rho_{\pi}} \left[ -\log (\pi_t (a_t \mid s_t)) \right] \geq \mathcal{H} \quad \forall t
$$

현재 시점의 policy는 미래 방향의 시간축으로밖에 영향을 주지 못하므로 DP 형태로 문제를 바꿀 수 있다.

$$
\max_{\pi_0} \left( \mathbb{E} [r(s_0, a_0)] + \max_{\pi_1} \left( \mathbb{E} [\dots] + \max_{\pi_T} \mathbb{E} [r(s_T, a_T)] \right) \right),
$$

마지막 time step부터 시작하여 엔트로피 제약조건과 함께 이를 dual problem으로 재정의할 수 있다.

$$
\max_{\pi_T} \mathbb{E}_{(s_t, a_t) \sim \rho_{\pi}} \left[ r(s_T, a_T) \right] = \min_{\alpha_T \geq 0} \max_{\pi_T} \mathbb{E} \left[ r(s_T, a_T) - \alpha_T \log \pi (a_T \mid s_T) \right] - \alpha_T \mathcal{H},
$$

이제 $\alpha\_T$는 dual variable이다. Objective가 linear하고 constraint(entropy)가 $\pi\_T$에서 convex하므로 strong duality가 성립한다. Optimal dual variable을 $\alpha^\*\_T$라 했을때

$$
\arg\min_{\alpha_T} \mathbb{E}_{s_t, a_t \sim \pi_t} \left[ -\alpha_T \log \pi_T^* (a_T \mid s_T; \alpha_T) - \alpha_T \mathcal{H} \right].
$$

이제 $T$ 이외의 time step에도 이 식이 적용될 수 있는지 알아보자. Soft Q function의 정의를 이용하면

$$
Q_t^* (s_t, a_t; \pi_{t+1:T}^*, \alpha_{t+1:T}^*) = \mathbb{E} \left[ r(s_t, a_t) \right] + \mathbb{E}_{\rho_{\pi}} \left[ Q_{t+1}^* (s_{t+1}, a_{t+1}) - \alpha_{t+1}^* \log \pi_{t+1}^* (a_{t+1} \mid s_{t+1}) \right],
$$

with $Q\_{T}^\* (s\_{T}, a\_{T})$ 이고, $T-1$인 경우에 objective function을 정의함과 동시에 위에 나왔던 식들을 참고하면

$$
\begin{align*}
  \max_{\pi_{T-1}} &\left( \mathbb{E} [r(s_{T-1}, a_{T-1})] + \max_{\pi_T} \mathbb{E} [r(s_T, a_T)] \right)\\
  &= \max_{\pi_{T-1}} \left( \mathbb{E} [r(s_{T-1}, a_{T-1})] + \min_{\alpha_T \geq 0} \max_{\pi_T} \mathbb{E} \left[ r(s_T, a_T) - \alpha_T \log \pi (a_T \mid s_T) \right] - \alpha_T \mathcal{H} \right)\\
  &= \max_{\pi_{T-1}} \left( \mathbb{E} [r(s_{T-1}, a_{T-1})] +  \mathbb{E}_{\pi^*_T} \left[ Q_{T}^* (s_{T}, a_{T}) - \alpha_T^* \log \pi_T^* (a_T \mid s_T) \right] - \alpha_T^* \mathcal{H} \right)\\
  &= \max_{\pi_{T-1}} \left( Q_{T-1}^* (s_{T-1}, a_{T-1}) - \alpha_T^* \mathcal{H} \right)\\
  &= \min_{\alpha_{T-1} \geq 0} \max_{\pi_{T-1}} \left( \mathbb{E} \left[ Q_{T-1}^* (s_{T-1}, a_{T-1}) \right] - \mathbb{E} \left[ \alpha_{T-1} \log \pi (a_{T-1} \mid s_{T-1}) \right] - \alpha_{T-1} \mathcal{H} \right) - \alpha_T^* \mathcal{H}.
\end{align*}
$$

즉, time step $T$일 때와 동일한 형태의 dual problem이 유도된다. 이를 recursively 반복하면 모든 time step에 대해 $\alpha^\*\_t$를 구할 수 있다.

$$
\alpha_t^* = \arg\min_{\alpha_t} \mathbb{E}_{a_t \sim \pi_t^*} \left[ -\alpha_t \log \pi_t^* (a_t \mid s_t; \alpha_t) - \alpha_t \bar{\mathcal{H}} \right].
$$

## Practical Algorithm

<center>
<img src='{{"assets/images/SAC/sac1.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">SAC Algorithm</figcaption>
</center>

Q value overestimation을 막기 위해 double Q function을 이용한다. Dual objective를 최적화하기 위해 온도 하이퍼파라미터도 같이 최적화한다. 

## Experiments

<center>
<img src='{{"assets/images/SAC/sac2.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Results</figcaption>
</center>

<center>
<img src='{{"assets/images/SAC/sac3.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Application to robot walking</figcaption>
</center>

실제 로봇을 평지에서 걷기 학습을 시킨 결과, 2시간 학습(400 episodes, 160k steps)로도 unseen stiuation에 대해 잘 일반화하였다.

<center>
<img src='{{"assets/images/SAC/sac4.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Application to dexterous hand manipulation</figcaption>
</center>

<center>
<img src='{{"assets/images/SAC/sac5.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Results of various input source</figcaption>
</center>

9-DOF DClaw를 이용하여 밸브를 회적시키는 작업을 수행한다. 학습 속도는 다소 느리지만 이미지만 제공한 경우에도 학습이 이루어지는 것을 알 수 있다. 밸브 위치 정보를 입력으로 주었을 때에는 PPO와 비교하여 2배 이상의 학습 속도를 보였다.(3h vs 7.4h)

## Conclusion