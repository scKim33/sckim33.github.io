---
title: "[RT-2] RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"
last_modified_at: 2025-02-02
categories:
  - paper_review
tags:
  - Reinforcement Learning
  - Robotics
  - CoRL
  - multi-modal
  - VLA
  - VLM
  - Google
excerpt: "RT-2 paper review"
use_math: true
classes: wide
---

> CoRL 2023 Poster. [[Paper](https://arxiv.org/abs/2307.15818)] [[Github](https://robotics-transformer.github.io/)]  
> Google DeepMind  
> 28 Jul 2023

## Summary

Vision-language 모델을 Internet-scale의 방대한 데이터로 학습시킨 연구이다. Single end-to-end 방식으로 학습시키는 것을 목표로 한다. 로봇의 action 또한 text token으로 처리하여 language model이 학습하는 방식과 동일하게 학습하게 한다.

## Introduction

Language 혹은 vision-language 모델로 로봇 데이터를 학습하는 것은 어려운 일이다. 그도 그럴 것이, 이미지의 semantics, 물체 인식, textual prompts를 이해함과 동시에 end-effector 제어와 같은 low-level action 또한 필요하기 때문이다. 앞선 연구들은 물체 집기, 놓기와 같은 high-level planning에 대해서만 언어 모델이 역할을 하였고, 그와 분리된 low-level controller가 작업을 수행함으로써 Internet-scale이 제공하는 풍부한 semantic을 활용하기가 어려웠다. 이 연구는 pretrained된 vision-language 모델이 low-level robotic control에 directly integrated 될 수 있는지를 다룬다.

## Related work

관련 연구로는 vision-language 모델, 로봇 학습 일반화, 로봇 manipulation pre-training이 있다.

## Vision-language-action models

### Pre-Trained Vision-Language Models

<center>
<img src='{{"assets/images/RT2/rt2_9.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">RT-2 model overview</figcaption>
</center>

PaLI-X, PaLM-E를 pre-treained 모델로 사용한다. Parameter 수는 수십억~수백억 수준으로 설정하여 학습한다. 

### Robot-Action Fine-tuning

6-DOF manipulator를 위해 action space를 새로이 정의한다. 각 positional, rotational displacement는 256개로 discretization된다. 아울러 robot gripper의 오픈 여부, terminating episode를 의미하는 binary 값 또한 존재한다. 따라서 action space dimension은 8이다. PaLI-X는 정수 1000까지에 대해 token 값이 존재하여 그대로 대응시키고, PaLM-E는 존재하지 않아 가장 덜 사용되는 256개의 token으로 덮어씌웠다. 이러한 방식은 symbol tuning이라고 해서 이미 VLM에 적용해도 문제가 없음이 알려져 있다.

이를 종합하면 fine-tuning에서 입력의 형태는 로봇의 카메라 이미지와 textual task description이 토큰화 된 형태(VQA format으로, "Q: what action should the robot take to [task instruction]? A:")이고, 출력은 정수 token(혹은 덜 쓰이는 256개의 token)의 string이다.

아울러 학습의 특징은 다음과 같다. Co-fine-tuning으로 웹 데이터와 로봇 데이터를 함께 사용하여 generalization 성능을 높인다. 그리고 일반적인 VLM과 다르게 유효한 token만을 출력해야 한다는 점도 있다.

### Real-Time Inference

VLM 모델이 55B parameter까지 가능한데 이 모델을 on-robot GPU에서 사용하는 것은 real-time으로 불가능하기 때문에 구글에서는 RT-2를 가동하기 위한 multi-TPU cloud service를 구축해서 네트워크 통신을 하는 방식으로 진행했다. 이 세팅으로 1~3Hz control로 구동가능하고, 좀 더 작은 5B 모델에서는 5Hz control로 구동가능하다.

## Experiments

<center>
<img src='{{"assets/images/RT2/rt2_1.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">학습 결과</figcaption>
</center>
<center>
<img src='{{"assets/images/RT2/rt2_8.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">evaluation 항목</figcaption>
</center>

여러 데이터셋을 혼합하여 사용한다. 그 중 하나로 WebLI 데이터셋을 포함한다. 원래는 109개국 언어의 100억개 image-text쌍인데, cross-modal similarity가 높은 10억개만 사용한다. 로봇 데이터셋은 observations(image, robot state), actions, task annotation("pick", "open", "place into" 등 행동 중 하나 + 대상으로 이루어진 자연어 demonstration)로 구성된다.

모델은 여러 가지의 prediction task를 수행한다.
1) predict the action, given two consecutive image frames and a text instruction
2) predict the instruction, given image frames
3) predict the robot arm position, given image frames
4) predict the number of timesteps between given image frames
5) predict whether the task was successful, given image frames and the instruction.

<center>
<img src='{{"assets/images/RT2/rt2_2.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">Unseen condition에서 평가</figcaption>
</center>
<center>
<img src='{{"assets/images/RT2/rt2_3.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">Baselines과 비교</figcaption>
</center>

일반화 성능을 보지 못한 물체, 배경, 환경에 대해 테스트한다. 인터넷으로 학습한 방대한 양의 데이터가 VLM에 학습되어 전작인 RT-1과 비교해도 일반화 성능이 좋다.

<center>
<img src='{{"assets/images/RT2/rt2_4.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">Opne source 모델과의 비교</figcaption>
</center>

오픈 소스 baseline과도 비교한다. Language-Table simulation environment에서 3B 모델을 오픈 소스 모델의 주파수와 비슷하게 맞추어 진행한다. 이전에 학습한 적 없는 push 명령에 대해서도 잘 수행한다.

<center>
<img src='{{"assets/images/RT2/rt2_5.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">Co-fine-tune의 효과</figcaption>
</center>

Co-fine-tune이 일반화 성능 향상에 기여하는 것으로 보인다.

<center>
<img src='{{"assets/images/RT2/rt2_6.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">CoT가 잘 수행되는 것이 확인된다.</figcaption>
</center>

Chain-of-thought가 RT-2의 VLM에서도 나타나는지 알아보기 위해 추가적으로 variant를 학습한다. "Plan" 단계에서는 동작을 수행하기 전 동작의 목적을 자연어로 나타낸다. 학습 결과를 통해 VLM이 VLA 내 planner로써의 역할을 잘 수행하는 것을 확인할 수 있다. (사견이긴 한데, CoT 문구 자체를 학습시킨 거라면 결국 reasoning이라기 보다는 그냥 학습해서 그런게 아닌가..? 나중에 CoT 논문을 살펴보아야겠다.)

## Limitations

<center>
<img src='{{"assets/images/RT2/rt2_7.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">수행하지 못하는 task 예시</figcaption>
</center>

두 가지 한계점을 제시한다. 먼저 Web-scale의 pretraining이 완전히 새로운 동작을 수행할 수 있는 능력을 주지는 않는다. 펜과 같이 물리적 특성이 다른 경우(굴러갈 수 있음), 바나나와 같이 center of mass가 특이한 경우에 실패하는 모습을 보인다. 다른 실패 예시로는 다음이 있다.
- Grasping objects by specific parts, such as the handle
- Novel motions beyond what was seen in the robot data, such as wiping with a towel or tool use
- Dexterous or precise motions, such as folding a towel
- Extended reasoning requiring multiple layers of indirection

그리고 로봇 제어가 real-time으로 돌아가기 위해 요구되는 computation cost가 크다는 단점이 있다.

