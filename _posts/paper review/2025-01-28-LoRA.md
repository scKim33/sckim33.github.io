---
title: "[LoRA] LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS"
last_modified_at: 2025-01-28
categories:
  - paper_review
tags:
  - LoRA
  - Fine-tuning
  - Efficiency
  - ICLR
excerpt: "LoRA paper review"
use_math: true
classes: wide
---

> ICLR 2022 Poster. [[Paper](https://arxiv.org/abs/2106.09685)] [[Github](https://github.com/microsoft/LoRA)]  
> Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen  
> 17 Jun 2021

## Summary

Pre-trained model의 fine-tuning 과정에 모든 parameter를 재학습하는 것은 굉장히 비용이 많이 든다. Low-Rank Adaptation(LoRA)를 통해 pre-trained model의 parameter를 freeze시킨 상태에서 학습가능한 rank decomposition matrices를 각 transformer layer에 삽입하는 것은 학습되는 parameter 수를 획기적으로 줄일 수 있다. GPT-3 175B의 경우 LoRA가 1만 배 적은 parameter로 학습할 수 있으며 GPU memory 요구량도 3배 감소한다. 심지어는 fully fine-tuning model과 비교했을 때 비슷하거나 더 좋은 성능을 보이기도 한다.

## Introduction

<center><img src='{{"assets/images/LoRA/lora1.png" | relative_url}}' style="width:30%;">
<figcaption style="text-align: center;">LoRA에서는 오직 A, B의 parameter만 학습시킨다.</figcaption>
</center>

거대 LLM들은 몇 달에 한번씩 학습된 모델을 출시한다. Task-specific한 language model을 fine-tuning하기 위해 거대 모델의 일부의 parameter만 학습하거나 새로운 task에 대한 외부 모듈을 학습시키는 방식으로 거대 모델을 fully fine-tuning 하는 비싼 작업을 대신한다. 지금까지의 방법들은 inference latency를 유발하거나 usable sequence length를 제한하는 등의 단점이 있다. 근본적으로 효율성을 높이는 대가로 성능의 감소를 일으키는 것이 가장 큰 문제이다.  
파라미터 수가 과도한 모델은 사실 낮은 내재적 차원에 위치한다는 연구결과를 통해 model adaptation에서도 이러한 아이디어를 바탕으로 접근하게 된다. 구체적으로 intrinsic dimension은 input dimension이 12,288인 경우에서 1 내지 2까지도 낮아질 수 있다.  
LoRA는 다음과 같은 장점을 가진다.
- 여러 다양한 task에 대해 동일한 pre-trained model을 사용할 수 있다.
- 하드웨어 성능 요구량을 3배까지 줄일 수 있다.
- 추론의 latency를 유발하지 않는다.
- 구조상 다른 method들과 독립적이기 때문에 호환성이 높다.

## Problem statement

일반적인 full fine-tuning에서는 다음과 같이 conditional language objective에 대한 parameter update가 진행된다.

$$
\max_{\Phi} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left( P_{\Phi}(y_t \mid x, y_{<t}) \right)
$$

하지만 full fine-tuning은 $\mid\Delta\Phi\mid$가 $\mid\Delta_0\mid$를 그대로 따르기 때문에 parameter를 업데이트 하는 것이 어렵다. 따라서 기존의 parameter를 encode하여 차원을 줄인 뒤($\Delta\Phi=\Delta\Phi(\Theta)$) 이를 이용하여 업데이트한다.

$$
\max_{\Theta} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left( p_{\Phi_0 + \Delta \Phi(\Theta)}(y_t \mid x, y_{<t}) \right)
$$

GPT-3 175B 모델을 pre-trained model로 사용한 경우, $\mid\Theta\mid$의 크기는 $\mid\Phi_0\mid$의 0.01% 수준으로 설정할 수 있다.

## Aren't existing solutions good enough?

사실 앞서 이야기한 fine-tuning에 대해서는 여러 방식으로 이미 적용되고 있는 부분이다. Adapter layer를 추가하거나, input layer activations의 형태를 optimizing하기도 한다. 이러한 방법들은 대규모 모델에서 문제점을 갖는다. Adapter layer의 경우, transformer block 사이에 두개의 adapter layer를 삽입하는 방식으로 구성된다. 학습되는 parameter의 수는 기존의 1% 수준이지만 bottleneck을 형성한다. 이 bottleneck은 sequential하게 연산의 길이를 증가시킨다. 이는 LLM에서 hardware parallelism의 이점을 충분히 이용하지 못하게 한다. 특히나 batch size가 작은 online inference에서는 더욱 그 차이가 두드러진다. 게다가 모델이 공유되는 경우 각 GPU 계산이 공유되는 시간 등이 추가적인 depth로 인해 더 자주 발생하게 된다.

<center>
<img src='{{"assets/images/LoRA/lora2.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">batch size가 작아질수록 Adapter layer방식과 LoRA 방식과의 latency 차이가 커진다.</figcaption>
</center>

이 외에도 prefix tuning이 있다. 더 나은 prompt를 찾기 위한 방법으로 fine-tuning 과정에서 input 앞에 prefix를 두고, prefix를 학습가능한 parameter로 구성하여 학습시킨다. 이 방식의 단점은 prefix가 기존 sequence의 일부분을 차지하게 되면서 downstream task에 가용될 sequence length를 줄이게 된다. 

## Our method

### Low-rank-parameterized update matrices

Pre-trained model의 parameter matrix를 $W_0\in \mathbb{R}^{d\times k}$라 할 때,
$W_0 + \Delta W = W_0 + BA$ 로 행렬을 분해한다. 이때 $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$, intrinsic dimension $r$에 대해 $r \ll \min(d, k)$이 되도록 작은 값으로 구성한다. $W_0$는 학습되지 않는 고정된 값이다. Forward pass는 병렬적으로 구성할 수 있다.

$$
h = W_0 x + \Delta W x = W_0 x + BAx
$$

LoRA 방식은 크게 두 가지의 이점을 갖는다
- Full fine-tuning의 generalization이다.  
  adapter based method가 MLP로 수렴하고, prefix based method가 긴 input sequence를 다루지 못하는 것과 달리 LoRA는 실제 모델로 수렴하게 된다. $r$을 증가시킴에 따라 pre-trained model의 weight matrices와 동일한 구조에 근접하며, 결국에는 실제 모델을 학습시키는 것과 동등하기 때문이다.  
- 추가적인 inference latency가 없다.  
  아울러 다른 task로 교체할 때 원본 모델은 그대로 유지되므로 input이 같다면 $B'A'$의 결과를 더한 뒤 $BA$ 값을 빼주면 되기 때문에 빠르고 메모리를 적게 사용한다.

### Applying LoRA to transformer

Transformer에는 크게 4종류의 weight matrices($W_q, W_k, W_v, W_o$)가 self-attention module에 있고, 2종류가 MLP module에 있다. Attention head 연산을 $d_{model}\times d_{model}$차원을 갖는 하나의 matrix로 두고 이 matrix에 대해서만 adapting을 수행한다.  
LoRA는 VRAM 사용량을 1/3 수준까지 낮추었으며, checkpoint size 또한 1만배 감소하였다. 다만 한계도 존재하는데, 여러 task에 대해 forward pass input을 batch로 구성하기가 까다롭다. 즉, inference latency를 최소화하기 위해 $W$에 $BA$를 미리 더하여 사용하게 되는데, task 1에 대한 $W_1$, task 2에 대한 $W_2$가 각각 존재하는 상황에서 batch 내에 두 task의 데이터가 섞여 있는 경우 처리하기가 까다로운 것이다. Latency가 중요하지 않은 상황에서는 batch의 sample 마다 동적으로 $BA$를 지정하여 연산할 수 있다.

## Empirical experiments

<center>
<img src='{{"assets/images/LoRA/lora3.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">GPT-3 175B pre-trained model에서의 성능 비교</figcaption>
</center>

<center>
<img src='{{"assets/images/LoRA/lora4.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">Trainable parameters에 따른 validation accuracy</figcaption>
</center>

Prefix fine-tuning의 경우 prefix에 할당되는 token의 수가 너무 많아지면 오히려 성능이 저하되는 모습을 보인다. 아울러 LoRA는 적은 parameter를 학습했음에도 full fine-tuning과 정확도 면에서 큰 차이를 보이지 않는다.

## Related works

생략

## Understanding the low-rank updates

### Which weight matrices in transformer should we apply LoRA to?

<center>
<img src='{{"assets/images/LoRA/lora5.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">고정된 trainable parameter에 대한 validation accuracy 비교</figcaption>
</center>

학습되는 parameter 수를 제한시킨 경우, 단일 weight matrix를 높은 $r$로 학습시키는 것보다 여러 weight matices를 낮은 $r$로 학습시키는 것의 성능이 더 좋았다.

### What is the optimal rank $r$ for LoRA?

<center>
<img src='{{"assets/images/LoRA/lora6.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">다양한 $r$에 대한 validation accuracy 비교</figcaption>
</center>

매우 작은 $r$에 대해서도 충분히 좋은 성능을 보여준다. 이를 뒷받침하기 위해 서도 다른 $r$에 대해 subspace similarity를 확인한다. rank $r=k$에서 학습된 LoRA matrix를 $A_{r=k}$라 정의하고, singular value decomposition한다. Right singular unitary matrix를 $U_{A_{r=k}}$라 정의한다. 두 행렬 간의 similarity를 Grassmann distance로 측정한다.

$$
\phi(A_{r=8}, A_{r=64}, i, j) = \frac{\|U_{A_{r=8}}^{i^\top} U_{A_{r=64}}^j\|_F^2}{\min(i, j)} \in [0, 1]
$$

$\lVert\cdot\rVert_F^2$는 Frobenius norm으로 행렬 간의 유클리드 거리를 나타낸다. 여기서는 첫번째 행렬의 $i$번째 singular vectors까지 span하는 영역이 두번째 행렬의 $j$번째 singular vectors까지 span하는 영역에 얼마만큼 포함되어 있는지를 나타낸 값으로 해석할 수 있다. 이 metric은 1에 가까울수록 두 subspace가 overlap 된다는 것을 의미하며 0에 가까울수록 separation 된다는 것을 의미한다.

<center>
<img src='{{"assets/images/LoRA/lora7.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">r=8, r=64에 대한 Grassmann distance. 우측 두 그림은 좌측 두 그림의 왼쪽 하단에 있는 회색 부분을 확대한 것이다.</figcaption>
</center>

$i,j=1$인 경우 similarity $>0.5$인 것을 확인할 수 있다. 이는 $r=1$에 대해서도 fine-tuning의 성능이 충분히 좋은 이유를 설명해 준다.

이번에는 동일한 $r$에 대하여 서로 다른 seed에서의 결과를 비교한다.

<center>
<img src='{{"assets/images/LoRA/lora8.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;">r=64의 서로 다른 seed에 대한 Grassmann distance.</figcaption>
</center>

그 결과도 비슷한 양상을 보임을 알 수 있다.

### How does the adaptation matrix $\Delta W$ compared to $W$?

$\Delta W$와 $W$의 관계에 대해서 알아본다. 만약 $\Delta W$의 top singular vector가 $W$의 top singular vector에 포함된다면 두 행렬은 서로 highly correlated 되었다고 볼 수 있다. 수치적으로 나타내기 위해 $W$를 rank $r$의 $\Delta W$에 projection한다. 즉, $\Delta W$의 SVD 분해 $U,V$ matrices를 이용하여 정사영 $\lVert U^TWV^T\rVert_F$를 구하고 $\lVert W\rVert_F^2$과의 Frobenius norm을 구한다. 정사영의 크기(norm의 크기)가 작다는 것은 두 행렬의 singular vector space가 orthogonal에 가깝다는 뜻이고 less correlated라고 볼 수 있다. 반대로 정사영의 크기가 크다는 것은 highly correlated라고 볼 수 있다.

<center>
<img src='{{"assets/images/LoRA/lora9.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;"></figcaption>
</center>

<center>
<img src='{{"assets/images/LoRA/lora10.png" | relative_url}}' width="70%">
<figcaption style="text-align: center;"></figcaption>
</center>

Random matrix와 비교하였을 때, $\Delta W$와 $W$는 stronger correlation을 보인다. 아울러 $\Delta W$는 실제로 $W$의 top singular vectors가 아닌, $W$에서는 특별히 강조되지 않는 features를 증폭시킨 것으로 해석된다. 이러한 경향은 그림에서와 같이 $r$이 작을 때 더 강하다.
