---
title: "[I-JEPA] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture"
last_modified_at: 2025-02-22
categories:
  - paper_review
tags:
  - JEPA
  - Meta
  - Self-supervised learning
excerpt: "I-JEPA paper review"
use_math: true
classes: wide
---

> CVPR 2023. [[Paper](https://arxiv.org/abs/2301.08243)]
> Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas  
> 19 Jan 2023

## Summary
이미지의 semantic representation을 수작업의 데이터 증강 방식 없이도 학습할 수 있도록 한다. Context block과 target block을 정의하여 하나의 context block을 통해 같은 이미지의 다양한 target block을 예측하게 된다. JEPA 구조를 ViT에 적용했을 때 확장성이 뛰어난 것을 확인할 수 있다.

## Introduction

자기 지도 학습은 두 가지 접근 방식을 지닌다. 첫번째, invariance-based 방식은 같은 이미지의 다양한 view에 대해 비슷한 embedding을 갖도록 학습한다. 이 방식은 높은 수준의 semantic representation을 학습할 수 있다. 그리고 data augmentation이 필요하다. 하지만 data augmentation은 bias를 초래할 수 있다는 한계를 갖는다. 이것이 downstream tasks에서 성능 저하를 초래할 수 있다. 또한 이미지 분류와 instance segmentation에서 다른 invariance를 요구하기도 한다. 이미지가 아닌 audio를 사용한다면 augmentation 자체가 어려울 수 있다.

두번째 방식은 생물학적 학습 이론에 기반하여 이미지의 일부를 삭제하거나 손상시킨 후 복원하는 generative 방식을 사용한다. 이 방식은 비교적 낮은 수준의 semantic을 학습(즉, 학습한 representation이 실제 semantic으로 연결되지 않을 수 있다)하고 이미지 분류에서는 invariance 방법보다 성능이 떨어진다. 하지만 멀티모달에 강하고 사전 지식이 덜 필요한 편이다.

## Background

<center>
<img src='{{"assets/images/I-JEPA/jepa2.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Self-supervised learning methods</figcaption>
</center>

자기 지도 학습은 energy based model의 관점에서 적절한 입력 쌍에 대해서는 낮은 에너지를, 부적절한 입력 쌍에 대해서는 높은 에너지를 할당하는 방식으로 학습이 이루어진다. 구체적으로는 3가지 방식으로 나뉜다.

Joint-embedding architecture는 입력 쌍 x,y에 대해 encoder를 거친 representation을 비교하게 된다. 데이터 증강을 통한 학습이 가능하다. 이 방식의 문제점은 모든 output을 동일하게 출력해버리는 representation collapse가 일어날 수 있다는 점이다. 이를 방지하기 위해 contrastive loss, non-contrastive loss, clustering-based learning, asymmetric architecture등의 방법이 사용된다.

Generative architecture는 x를 embedding으로 변환한 뒤 이를 다시 decoder를 통해 복원한다. 마스킹을 통하여 제거된 이미지 패치를 복원하는 방식으로 학습된다. Representation collapse는 일어나지 않지만 semantic 특징 학습으로 연결되지 않을 수도 있다.

Joint-embedding predictive architecture(JEPA)는 예측이 representation space에서 이루어진다. 추가적인 데이터 증강이 필요하지 않고 마스크 정보나 위치 정보 등을 이용하여 예측하게 된다. Representation collapse를 막기 위해 x,y encoder 비대칭 구조를 활용한다.


## Method

<center>
<img src='{{"assets/images/I-JEPA/jepa3.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">I-JEPA structure</figcaption>
</center>

### Targets
입력 이미지 $y$에 대해, $N$개의 패치로 나눈 후 target-encoder를 통과시켜 $N$개의 representation을 얻는다. 이를 $s\_y = \\{ \mathbf{g}\_{y\_1}, \dots, \mathbf{g}\_{y\_N} \\}$라 하자. 이 representation에서 $M$개의 block을 선택한다(block들은 패치를 overlap할 수 있다). $i$번째 블록을 $B\_i$라 할 때 블록에 속한 패치를 $\mathbf{s}\_{y}(i) = \\{ \mathbf{s}\_{y\_j} \\}\_{j \in B\_i}$로 표현한다. 논문에서는 $M=4$, aspect ratio $\[0.75, 1.5\]$, scale $\[0.15, 0.2\]$를 사용한다.

<center>
<img src='{{"assets/images/I-JEPA/jepa4.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">How target masks are applied</figcaption>
</center>

### Context
입력 이미지 $y$에서 scale $\[0.85, 1\]$의 unit aspect ratio single block $x$를 sample한다. 거기에 target block 영역을 제거해 준다. 이 상태로 context encoder에 전달하여 representation $\mathbf{s}\_\{x\} = \\{ \mathbf{s}\_{x\_j} \\}\_{j \in B\_x}$을 얻는다.


### Prediction
하나의 context representation과 $M$개의 target representation이 있다. 각각의 target에 대한 mask 정보 또한 패치별로 learnable한 parameter로 구성되어 decoder에 condition으로 준다. 최종 output은 다음과 같다.

$$
\hat{s}_{y}(i) = \{\hat{\mathbf{s}}_{y_j}\}_{j \in B_i} = g_{\phi}(s_{x}, \{ m_j \}_{j \in B_i})
$$


### Loss
Loss는 각 target block에 대해 context와 target의 JEPA 출력값의 summation이다.

$$
\frac{1}{M} \sum_{i=1}^{M} D\left( \hat{s}_{y}(i), s_{y}(i) \right) =
\frac{1}{M} \sum_{i=1}^{M} \sum_{j \in B_{t}} \left\| \hat{s}_{y_j} - s_{y_j} \right\|_2^2.
$$

Decoder와 context encoder는 gradient-based optimization으로 학습하되, target encoder는 exponential moving average를 사용한다. 이는 학습의 안정성과 표현 붕괴 방지를 위한 것인데, 다른 연구들에서도 효과성을 보인 방법이다.

## Related Work
손실, 변형된 이미지에 대한 예측 연구에 대해 설명한다. 마스킹을 활용한 연구(MAE, BEiT, SimMIM)들이 있다. JEPA와 유사하게는 data2vec, context autoencoder가 있다. JEA(DINO, MSN, iBOT) 기반 연구와도 관련이 있다. 


## Image Classification

이미지 분류 문제를 linear probing(pretrained 모델의 가중치 고정 + linear classifier 추가), partial fine-tuning(pretrained 모델의 일부 층을 learnable 하게)의 방식으로 나누어 테스트한다. 학습 이미지는 224x224 크기 고정이다.

<div style="display: flex; justify-content: center; align-items: center;">
  <div style="text-align: center; margin-right: 20px;">
    <img src='{{"assets/images/I-JEPA/jepa5.png" | relative_url}}' width="80%">
    <figcaption>Image classification results</figcaption>
  </div>
  <div style="text-align: center;">
    <img src='{{"assets/images/I-JEPA/jepa6.png" | relative_url}}' width="80%">
    <figcaption>Low-shot Image classification results</figcaption>
  </div>
</div>

I-JEPA는 생성 기반 방법(MAE, CAE, data2vec)과 같은 기존의 생성 기반 방법보다 성능이 우수하고, 데이터 증강 없이도 불변성 기반 방법과 견줄 만한 성능을 보인다. Low-shot에서도 같은 경향성을 보인다.

## Local Prediction Tasks

<center>
<img src='{{"assets/images/I-JEPA/jepa7.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Transfer learning image classification & local prediction results</figcaption>
</center>

이미지 분류 Transfer learning에서는 불변성 기반 방법을 넘진 못했으나 성능 격차를 줄였다. Low-level task(개수 세기, depth 예측) transfer learning에서도 좋은 성능을 보인다.

## Scalability

<center>
<img src='{{"assets/images/I-JEPA/jepa8.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Pretraining dataset size results</figcaption>
</center>

<center>
<img src='{{"assets/images/I-JEPA/jepa9.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Model scaling results</figcaption>
</center>

Scalability를 데이터 개수와 모델 크기에 대해 확인한다. 


## Predictor Visualizations

<center>
<img src='{{"assets/images/I-JEPA/jepa12.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Visualization outputs</figcaption>
</center>

## Ablations

<center>
<img src='{{"assets/images/I-JEPA/jepa10.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Prediction of embedding vs pixel</figcaption>
</center>

픽셀 단위 예측과 embedding 단위 예측의 성능 비교이다. 픽셀 단위 예측은 불필요한 픽셀 예측까지 학습하기 때문에 성능이 오히려 감소하게 된다.

<center>
<img src='{{"assets/images/I-JEPA/jepa11.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Various target block generation results</figcaption>
</center>

Target block을 만드는 방법을 다양하게 시도한다. 좀 더 단순한 target block에 대해서는 성능이 떨어지는 편이다.

## Conclusion