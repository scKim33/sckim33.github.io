---
title: "[DPG] Deterministic Policy Gradient Algorithms"
last_modified_at: 2025-02-28
categories:
  - paper_review
tags:
  - DPG
  - Reinforcement Learning
excerpt: "DPG paper review"
use_math: true
classes: wide
---

> ICML 2014. [[Paper](https://proceedings.mlr.press/v32/silver14.pdf)]  
> David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller  
> 6 Oct 2017

## Summary

Stochastic actor critic보다 효율적으로 학습할 수 있는 deterministic actor critic에 대해 소개한다.

## Introduction

## Background

On-policy Stochastic policy gradient는 다음과 같다.

$$
\begin{align*}
  \nabla_{\theta} J(\pi_{\theta}) &= \int_{S} \rho^{\pi}(s) \int_{A} \nabla_{\theta} \pi_{\theta}(a | s) Q^{\pi}(s, a) \, da \, ds\\
  &= \mathbb{E}_{s \sim \rho^{\pi}, a \sim \pi_{\theta}} \left[ \nabla_{\theta} \log \pi_{\theta}(a | s) Q^{\pi}(s, a) \right]
\end{align*}
$$

Off-policy Stochastic policy gradient는 다음과 같다.

$$
\begin{align*}
  \nabla_{\theta} J_{\beta}(\pi_{\theta}) &\approx \int_{S} \int_{A} \rho^{\beta}(s) \nabla_{\theta} \pi_{\theta}(a | s) Q^{\pi}(s, a) \, da \, ds\\
  &= \mathbb{E}_{s \sim \rho^{\beta}, a \sim \beta} \left[ \frac{\pi_{\theta}(a | s)}{\beta_{\theta}(a | s)} \nabla_{\theta} \log \pi_{\theta}(a | s) Q^{\pi}(s, a) \right]
\end{align*}
$$

이 gradient의 문제점은 state와 action에 모두 적분 계산을 취하게 되면서 계산량이 많아지고, action의 차원이 클 경우 샘플 효율성이 낮아지게 된다.

## Gradients of Deterministic Policies

Greedy policy improvement에서 next action을 정할 때 continuous action space에서는 연산량이 문제가 될 수 있다. 따라서 보다 간단하고 연산량이 적은 deterministic policy를 이용할 수 있다.

$$
a_{t+1}=\mu_\theta(s_t)
$$

Policy gradient는 다음과 같다.

$$
\begin{align*}
  \nabla_{\theta} J(\mu_{\theta}) &= \int_{S} \rho^{\mu}(s) \nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^{\mu}(s, a) \Big|_{a = \mu_{\theta}(s)} \, ds\\
  &= \mathbb{E}_{s \sim \rho^{\mu}} \left[ \nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^{\mu}(s, a) \Big|_{a = \mu_{\theta}(s)} \right]
\end{align*}
$$

> Q: 증명 과정?\\
> Policy gradient의 증명 과정과 같은 방법으로 증명할 수 있다. Stochastic policy와 다르게 deterministic policy의 경우 $V^\mu(s)=Q^\mu(s,\mu(s))$이므로 증명 첫 부분에 이 부분으로 시작하면 된다. 아래 증명에서의 $\mu$는 $\pi$의 state distribution이므로(policy 아님) 그 부분은 주의할 것.
> <center>
<img src='{{"assets/images/DPG/dpg1.png" | relative_url}}' style="max-width: 100%; width: auto;"><figcaption style="text-align: center;">Policy gradient proof</figcaption></center>
> DPG 버전의 증명은 따로 suppliment에 있다. 다만 suppliement의 증명에서는 state distribution으로 target policy $\mu$를 기준으로 증명하고, off policy actor critic에서는 이것을 behaviour policy $\beta$로 바꾸어 approximation을 적용하고 있다.

> Q: 위 질문에 이어서, off-policy learning에서 behaviour policy의 state distribution을 통해 objective function을 정의하는 것이 target policy의 state distribution으로 정의하는 것과 동일한 optimal policy를 이끌어내는가?\\
> 아직 해답을 못 찾았다. 그냥 approximation으로 놔둔건지..? 이 부분은 offline learning같이 distribution이 많이 차이나게 되는 경우에 중요할 것 같은데...

논문에서는 이 form이 stochastic policy gradient의 variation임을 보인다. 아이디어만 설명하면 파라미터 $\sigma$를 정의하여 $\sigma\rightarrow 0$일 때 delta function으로 수렴하는 분포에 대해 그 극한이 결국 deterministic policy gradient의 형태와 같다는 것이다.


## Deterministic Actor-Critic Algorithms

알고리즘적으로는 on/off-policy 각각에 대해 적용할 수 있다. On-policy의 경우

$$
\begin{align*}
\delta_t &= r_t + \gamma Q^w(s_{t+1}, a_{t+1}) - Q^w(s_t, a_t)\\
w_{t+1} &= w_t + \alpha_w \delta_t \nabla_w Q^w(s_t, a_t)\\
\theta_{t+1} &= \theta_t + \alpha_\theta \nabla_{\theta} \mu_{\theta}(s_t) \nabla_a Q^w(s_t, a_t) \Big|_{a = \mu_{\theta}(s)}
\end{align*}
$$

이다. Trajectory가 deterministic하게 정해지기 때문에 exploration이 부족하지만, 환경 자체에 noise가 포함된 경우 이를 완화할 수 있다.


Off-policy의 경우

$$
\begin{align*}
  \nabla_{\theta} J_{\beta}(\mu_{\theta}) &\approx \int_{S} \rho^{\beta}(s) \nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^{\mu}(s, a) \, ds\\
  &= \mathbb{E}_{s \sim \rho^{\beta}} \left[ \nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^{\mu}(s, a) \Big|_{a = \mu_{\theta}(s)} \right]
\end{align*}
$$

아까 deterministic policy gradient의 식과 비교했을 때, 여기서는 approximation($\approx$)를 쓴 것을 보면 그냥 behaviour policy로 근사시키는게 문제없다고 판단한 것 같다. 논문에서는 deterministic policy로 인해 action에 대한 적분이 필요 없기 때문에 그로 인한 importance sampling 연산도 없어진다고 설명하고 있다.


$$
\begin{align*}
\delta_t &= r_t + \gamma Q^w(s_{t+1}, \mu_{\theta}(s_{t+1})) - Q^w(s_t, a_t)\\
w_{t+1} &= w_t + \alpha_w \delta_t \nabla_w Q^w(s_t, a_t)\\
\theta_{t+1} &= \theta_t + \alpha_\theta \nabla_{\theta} \mu_{\theta}(s_t) \nabla_a Q^w(s_t, a_t) \Big|_{a = \mu_{\theta}(s)}
\end{align*}
$$

이다. Target policy는 TD error를 구할 때만 사용되고, 나머지는 stochastic behaviour policy가 생성한 trajectory를 바탕으로 구하게 된다.

## Experiments

## Discussion and Related Work

Stochastic policy를 사용하게 되면 학습이 진행될 수록 점차 mean에 쏠리는 형태의 분포를 보이게 된다. 그렇게 되면 mean 주변에서는 급격한 gradient가 형성되어 policy gradient estimation을 원활히 할 수 없게 만든다. 또한 high-dimensional action space에서는 stochastic policy gradient formula의 action에 대한 적분을 action sampling을 통해 구하게 되는데, deterministic policy의 경우 