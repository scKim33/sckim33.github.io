---
title: "[ViT] AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"
last_modified_at: 2025-02-20
categories:
  - paper_review
tags:
  - Transformer
  - CV
  - Google
excerpt: "ViT paper review"
use_math: true
classes: wide
---

> ICLR 2021 Oral. [[Paper](https://arxiv.org/abs/2010.11929)][[Github](https://github.com/google-research/vision_transformer)]   
> Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby  
> 22 Oct 2020

## Summary

당시 언어 모델에 국한되었던 트랜스포머 구조를 vison 영역에 적용하여 SOTA 성능을 달성한다. 이전에도 트랜스포머 구조를 vision에 사용한 연구들이 있었지만 순수하게 트랜스포머만으로 이미지 분류를 하였고, 다른 CNN 모델의 성능을 넘었다는 점에서 의의가 있다.

## Introduction

트랜스포머는 NLP의 표준 모델로써 자리매김했다. 그리고 100B 이상의 크기를 가진 모델들도 등장하면서 성능 또한 계속해서 높아지고 있다. CV에서는 CNN이 여전히 지배적이다. 트랜스포머를 결합하거나 완전히 대체한 일부 연구들이 있지만 SOTA 급 성능은 여전히 CNN 기반이다.

트랜스포머를 이미지 분류에 사용하기 위해 이미지를 패치 단위로 나누고 이를 NLP의 token처럼 다룬다. ViT의 경우 CNN이 사용되지 않은 순수 트랜스포머 기반이다.

ViT는 ResNet과 비교하여 적은 데이터를 사용하는 ImageNet에서는 성능이 떨어진다. 많은 학습 데이터를 사용하는(14M~300M) 환경에서는 ResNet을 뛰어넘는 성능을 보인다. 이는 inductive bias가 성능에 미치는 긍정적인 영향을 large scale training이 뛰어넘었고 ViT가 대규모 학습에 효과적임을 의미한다. 

> 참고: inductive bias란? \\
> 모델이 학습 데이터 이외에도 일반화할 수 있도록 미리 짜여진 구조적인 제약을 의미한다. CNN의 경우 inductive bias가 높은 편이라 볼 수 있다. Convolutional filter가 공간 상관없이 동일한 특징을 감지하는 transition invariant, 작은 지역의 패턴을 파악하는 locality, weight sharing의 특징 때문이다. 반면 ViT의 경우 토큰 간의 key, query 관계만을 파악하기 때문에 inductive bias가 조금 약한 편이다. Fully-connected는 더더욱 약할 것이다. Inductive bias가 강할 수록 적은 데이터로도 효과적인 학습이 가능하다.

## Related work

트랜스포머가 machine translation을 위해 등장하고, BERT나 GPT와 같이 대규모로 사전학습을 한 뒤 fine tune하는 방식이 NLP에 사용되었다. Self-attention 구조를 이미지에 사용하기 위해 픽셀 단위로 self-attention을 적용하기에는 $O(N^2)$로 과도한 연산을 요구한다. 이를 해결하기 위해 여러 연구들이 진행되었다. ViT는 이에 대해 ImageNet-21k, JFT-300M과 같은 거대 데이터셋에서 pretrain시킨 후 fine tune한다.

## Method

<center>
<img src='{{"assets/images/ViT/vit1.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Model overview</figcaption>
</center>

이미지 $x\in\mathbb{R}^{H\times W\times C}$를 가로 세로 크기가 $P$인 $N$개의 2D 패치 $x\_p\in\mathbb{R}^{N\times (P^2\times C)}$로 변환한다. 각 패치를 flatten하고 $D$차원으로 mapping 시키면 patch embedding이 된다. Patch embedding 앞에는 BERT의 \[class\] token과 같은 개념의 token을 사용한다. 이 token은 이미지 전체를 대표하는 정보로 사용된다. 그리고 각각의 embedding에는 positional embedding을 더해주어 최종적인 input이 구성된다.

$$
z_0 = [x_{\text{class}}; \, x_p^1 \mathbf{E}; \, x_p^2 \mathbf{E}; \, \dots ; \, x_p^N \mathbf{E}] + \mathbf{E}_{\text{pos}}, \quad
\mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}, \, \mathbf{E}_{\text{pos}} \in \mathbb{R}^{(N+1) \times D}
$$

그리고 input은 $L$개의 transformer encoder를 통과하게 된다. 마지막 encoder를 통과한 값의 첫번째 성분(class token)인 $z_L^0$에 one hidden layer MLP를 달아 분류 class를 최종적으로 출력한다. $y$는 class를 예측한다.

$$
\begin{aligned}
z'_\ell &= \text{MSA}(\text{LN}(z_{\ell-1})) + z_{\ell-1}, \quad \ell = 1 \dots L \\
z_{\ell} &= \text{MLP}(\text{LN}(z'_\ell)) + z'_\ell, \quad \ell = 1 \dots L \\
y &= \text{LN}(z_L^0)
\end{aligned}
$$

ViT에서 확인할 수 있는 inductive bias관련 정보는 다음과 같다.
- $z\_L^0$에 연결된 MLP layer는 local하다.
- self-attention은 모든 patch embedding 간 연산이 이루어지므로 global하다.
- 이미지가 패치로 변환될 때 2d neighborhood structure가 사용된다.
- 해상도가 다른 이미지의 위치 임베딩은 이미지 비율에 맞게 조정되어야 하는데, 위치 임베딩을 패치 좌표를 기준으로 보간하여 적용하게 되고 이 때 2d neighborhood structure가 사용된다.
- 초기 position embedding은 학습 가능한 파라미터를 랜덤 초기화를 하여 2D 위치 정보를 가지고 있지 않다. 이후에 학습을 통해 가까운 위치의 패치끼리는 강한 attention score의 position embedding을 가지게 된다.

<details>
<summary>왜 굳이 transformer에 사용되는 sinusoidal position embedding을 random initialize & learnable parameter화 한 것일까?</summary>
<div markdown="1">

### **왜 ViT에서는 Sinusoidal Position Embedding 대신 학습 가능한 랜덤 초기화를 사용할까?**  

너의 질문은 아주 핵심적인 지점을 찌르고 있어!  
Sinusoidal Position Embedding을 사용하면 **2D 공간적 관계를 학습할 필요 없이 처음부터 명확한 위치 정보를 부여할 수 있을 것 같은데**,  
그럼에도 불구하고 ViT는 **랜덤 초기화를 사용하고 학습을 통해 위치 정보를 학습하도록 설계**했어.  

💡 **그 이유는 크게 3가지가 있어!**  

---

## **1️⃣ Sinusoidal Position Embedding은 NLP에서는 효과적이지만, 2D 이미지에서는 제한적**
### ✅ **(1) NLP에서 Sinusoidal Encoding이 쓰이는 이유**  
- NLP에서는 단어들이 1D 시퀀스 형태로 나열되므로, 단순한 **위치 정보(1D 순서 정보)**만 제공해 주면 충분함.  
- **Sinusoidal Position Embedding은 "단어 간 상대적 거리"를 유지하는 데 효과적**임.  
- 예를 들어, "The cat sat on the mat."에서 **"cat"과 "sat"이 가까운지 먼지를 구분할 수 있음.**  

### ✅ **(2) 하지만 2D 이미지에서는 단순한 1D 순서 정보만으로 충분하지 않음!**  
- ViT는 이미지를 패치(Patch) 단위로 쪼개지만, **패치들의 위치 관계는 단순한 1D 순서로 표현할 수 없음.**  
- 예를 들어,  
  - **패치 (0,1)과 (1,0)은 원래 이미지에서 물리적으로 가까움**.  
  - 하지만 **1D로 순서만 부여하면 (0,1)과 (5,5)도 비슷한 거리로 인식될 수 있음.**  
- **즉, Sinusoidal Position Embedding은 1D 순서를 잘 반영하지만, 2D 공간적 관계를 잘 반영하지 못할 가능성이 큼.**  

🚨 **결론:**  
➡ Sinusoidal Position Embedding을 사용하면 1D 순서는 반영할 수 있지만,  
➡ **2D 공간 구조(위-아래, 좌-우 관계)를 직접적으로 반영하기 어려움.**  
➡ **즉, 학습이 진행될수록 원래 2D 위치 정보와는 다르게 변할 가능성이 높음.**  

---

## **2️⃣ 학습 가능한 Position Embedding은 데이터에 맞게 최적화될 수 있음**
### ✅ **(1) ViT는 특정한 데이터셋에 맞게 최적화된 위치 정보를 학습해야 함.**  
- Sinusoidal Embedding은 **고정된 수학적 함수 기반으로 위치 정보를 생성**함.  
- 하지만 **이미지 데이터에서 위치 정보는 단순한 수학적 관계 이상으로 중요한 역할을 할 수 있음!**  

💡 예제 1:  
- "고양이" 사진에서는 **눈, 코, 귀가 항상 특정한 패턴으로 배열됨.**  
- 하지만 **풍경 이미지(예: 하늘, 나무, 건물)에서는 패치 간 위치 정보의 역할이 다를 수 있음.**  
- **그러므로, 모델이 데이터에 맞는 최적의 위치 정보를 학습하는 것이 더 유리할 수 있음!**  

💡 예제 2:  
- Sinusoidal Encoding은 위치를 고정된 방식으로 표현하기 때문에,  
  **해상도가 달라지거나 패치 개수가 변하면 그대로 적용하기 어려움.**  
- **하지만 학습 가능한 Position Embedding은 해상도 변화에 맞춰 조정 가능.**  

🚨 **결론:**  
➡ **Sinusoidal Encoding은 "고정된 규칙"을 따르지만, 학습 가능한 Position Embedding은 데이터에 맞게 최적화될 수 있음.**  
➡ **특정 데이터셋(예: 얼굴 인식, 자연 풍경 등)에 따라 위치 관계의 중요성이 다를 수 있음.**  
➡ **따라서, 학습 가능한 임베딩을 사용하면 데이터 특성에 맞게 공간 정보를 효과적으로 학습할 수 있음.**  

---

## **3️⃣ ViT는 위치 정보를 "처음부터 학습"하는 것이 아니라, "필요한 만큼 학습"하는 것이 중요**
### ✅ **(1) ViT의 핵심 아이디어는 "위치 정보가 필요할 때만 학습하도록 설계"**
- CNN과 달리, ViT는 **고정된 공간적 필터(Convolution)를 사용하지 않음.**  
- 따라서, **위치 정보가 모델의 성능에 얼마나 중요한지 학습을 통해 결정할 수 있도록 설계됨.**  

💡 **예제:**  
- 어떤 이미지 태스크에서는 **위치 정보가 매우 중요할 수 있음.**  
  - 예: 객체 탐지(Object Detection) → "사람의 눈이 얼굴의 위쪽에 있어야 한다"는 정보가 필요.  
- 하지만 **어떤 태스크에서는 위치 정보가 덜 중요할 수도 있음.**  
  - 예: 스타일 변환(Style Transfer) → "이미지 전체적인 특징만 반영하면 되므로 위치 정보가 덜 중요할 수 있음."  
- 즉, **ViT는 필요한 만큼만 위치 정보를 학습할 수 있도록 설계됨.**  

🚨 **결론:**  
➡ **고정된 위치 정보를 넣으면 필요하지 않은 정보까지 강제로 반영될 가능성이 있음.**  
➡ **학습 가능한 위치 임베딩을 사용하면, ViT가 "위치 정보가 필요할 때만" 이를 활용할 수 있도록 최적화할 수 있음.**  

---

## **📌 최종 요약**

| | **Sinusoidal Position Embedding** | **학습 가능한 Position Embedding (ViT 사용 방식)** |
|-------------------|--------------------------------|--------------------------------|
| **1D 시퀀스 표현** | ✅ NLP에서 효과적 (1D 순서 반영 가능) | ❌ 2D 공간 구조 반영 어려움 |
| **2D 공간 정보** | ❌ 1D 순서만 표현 가능, 공간 관계 반영 어려움 | ✅ 학습을 통해 2D 구조를 최적화 가능 |
| **데이터 최적화** | ❌ 고정된 규칙으로 적용됨 | ✅ 데이터에 따라 최적의 위치 정보 학습 가능 |
| **해상도 변화 대응** | ❌ 패치 개수가 변하면 그대로 적용하기 어려움 | ✅ 보간(Interpolation) 등을 통해 조정 가능 |
| **위치 정보의 필요성** | ❌ 항상 적용됨 (불필요한 경우도 포함) | ✅ 필요한 만큼만 학습 가능 |

---

## **📌 결론: ViT가 학습 가능한 Position Embedding을 사용하는 이유**
1️⃣ **Sinusoidal Embedding은 1D 시퀀스에는 효과적이지만, 2D 공간적 관계를 제대로 표현하기 어려움.**  
2️⃣ **ViT는 위치 정보를 처음부터 강제로 반영하는 것이 아니라, 필요한 만큼만 학습할 수 있도록 설계됨.**  
3️⃣ **학습 가능한 Position Embedding은 특정 데이터셋에 맞게 최적화될 수 있으며, 해상도 변화에도 유연하게 대응 가능.**  

💡 **즉, Sinusoidal Embedding을 사용하면 ViT가 원래 의도한 "자유롭게 패치 관계를 학습하는 방식"이 제한될 수 있음!**  

➡ **그래서 ViT는 랜덤 초기화된 학습 가능한 Position Embedding을 사용하여, 모델이 공간적 관계를 직접 학습하도록 만든 것!** 🚀  

---

이제 완전히 이해됐어? 더 궁금한 거 있으면 질문해줘! 😊🔥
</div>
</details>
\
Hybrid architecture라고 이미지 패치 대신 CNN의 feature map을 대신 사용하는 방식도 있다.

## Experiments

<center>
<img src='{{"assets/images/ViT/vit7.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">ViT variation</figcaption>
</center>

ImageNet, ImageNet-21K, JFT-300M을 이용하여 사전학습을 진행하고, 이후 벤치마크에서 평가한다. ViT의 경우 3가지 variation을 둔다. Hybrid 모델은 CNN의 중간 feature을 ViT의 입력으로 넣어준다.

<center>
<img src='{{"assets/images/ViT/vit2.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Results</figcaption>
</center>

ViT가 성능 면에서 ResNet 계열을 뛰어넘었고, 연산량(TPUv3-core-days)을 25% 수준으로 적게 사용하는 것을 확인할 수 있다.

### Pre-training data requirements

<center>
<img src='{{"assets/images/ViT/vit3.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Pretraining data size vs Accuracy</figcaption>
</center>

사전학습 데이터 크기가 작은 경우 ResNet 기반 모델이 우수했지만, 사전학습 데이터 크기가 커짐에 따라 ViT가 ResNet을 넘어서는 성능을 보인다. 거대 데이터에 대해서는 inductive bias를 극복할 수 있다. 

### Scaling study

<center>
<img src='{{"assets/images/ViT/vit4.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Computation vs Accuracy</figcaption>
</center>

CNN feature를 일부 사용하는 Hybrid 모델과 기존 ViT와의 차이를 확인한다. 적은 사전학습 계산량 대비로는 Hybrid 모델이 더 우수했지만 사전학습 계산량이 늘어남에 따라 ViT와의 차이가 사라졌다. CNN 기반 모델은 데이터 당 연산량이 많기 때문에 계산량 대비 성능은 떨어지는 모습을 보인다.

### Inspecting vision transformer

<center>
<img src='{{"assets/images/ViT/vit5.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">ViT filter, position embedding and attention distance</figcaption>
</center>

<center>
<img src='{{"assets/images/ViT/vit6.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Attention examples</figcaption>
</center>

Embedding을 생성하는 projection에서의 filter를 조사해본다. Filter들의 principal components를 시각화한 결과 CNN의 기본 필터와 유사한 패턴을 보인다. 즉, projection의 filter는 이미지의 기본적인 구조를 학습한 것으로 보인다.

초기에 랜덤하게 초기화된 Position embedding 또한 2D 위치 정보를 잘 학습한다. 각자의 token 위치에 대해 근접한 token에서 similarity가 높게 측정된다. 따라서 굳이 2D-aware embedding을 설계하여 넣어줄 필요가 없다.

또한 attention distance(다른 patch와의 attention score를 평균냄, CNN에서 receptive field와 비슷한 개념)을 측정하여 self-attention 구조가 이미지를 어떻게 처리하는지 확인한다. Head의 Attention distance가 낮다면 거리상으로 가까운 patch와의 attention이 이루어진다는 것이고, Attention distance가 높다면 멀리 있는 patch와 attention이 이루어진다는 것을 의미한다. ViT의 경우 낮은 depth layer에 대해서는 local 정보와 global 정보를 동시에 학습하는 편이고 높은 detph layer에 대해서는 global 정보를 위주로 학습하는 것으로 보인다. CNN과 비교했을 때 low level에서 주로 local한 정보만을 학습하는 것과는 차이를 보인다.

### Self-supervision

NLP에서 트랜스포머의 성공은 scalability 뿐만 아니라 자기 지도 학습의 역할도 크다. Vision에서도 자기 지도 학습이 잘 이루어지는지 확인하기 위해 (BERT의 방식을 따라) masked patch prediction을 수행한다. 패치의 절반을 손상시키는데 [mask]로 바꾸거나 다른 랜덤한 patch embedding으로 교체하거나 그대로 둔다. 그리고 손상된 패치의 평균 색상을 예측하도록 학습한다(RGB 각 값을 3-bit로 예측하여 $2^{3\times3}=512$개의 색상 조합). 이후 fine tuning을 거친 후 테스트한다. 그 결과 pre-training을 거치지 않은 scratch보다는 성능이 좋았으나 pretraining을 거친 지도 학습 보다는 성능이 떨어졌다.


## Conclusion

ViT에서는 이미지 분류 문제만 다루었지만 detection이나 segmentation에 대해서도 연구 필요성을 말한다. 여전히 pretraining 지도 학습으로 진행한 경우와 자기 지도 학습으로 진행한 경우의 성능 차이가 존재하기 때문에 이에 대한 추가적인 연구가 필요해 보인다.