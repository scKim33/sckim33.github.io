---
title: "[DDPG] CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING"
last_modified_at: 2025-02-28
categories:
  - paper_review
tags:
  - DDPG
  - Reinforcement Learning
  - Off policy
  - Model free
  - Google
excerpt: "DDPG paper review"
use_math: true
classes: wide
---

> ICLR 2016. [[Paper](https://arxiv.org/abs/1509.02971)]  
> Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra  
> 9 Sep 2015

## Summary

DQN의 아이디어를 continuous action domain으로 확장한다. DQN에서 $\max$ 연산 대신 state에서 action으로 가는 deterministic policy를 이용하여 continuous domain에 적용가능하게 된다.

## Introduction

DQN은 강화학습과 신경망을 이용하여 Atari 게임을 인간 수준으로 학습하는데 성공했다. 하지만 discrete, low-dimensional action space로 한정되었다는 점에서 한계가 있다. Continuous space를 이산화하는 방법은 몇가지 문제점이 존재한다.
 - 차원의 저주를 겪을 수 있다. 가령 6-DOF의 state에 대해 3가지 값으로만 이산화해도 $3^7=2,187$에 달하는 space를 고려해야 한다.
 - action space를 이산화하는 경우 정밀한 제어가 어렵다. 그 결과, 샘플 효율성을 감소시키고 학습 속도가 느려진다.
 - state의 이산화가 구조적인 정보를 잃게 만들어 결과적으로 problem solving이 어렵게 한다.

이 논문에서는 DPG 알고리즘에 DQN의 아이디어를 적용하여 학습의 안정성을 높인다.
- replay buffer 사용
- target network 사용
- batch normalization

## Background

Stochastic policy $\pi$에 대해서는 다음과 같은 Bellman equation을 갖는다.

$$
Q^\pi(s_t, a_t) = \mathbb{E}_{r, s_{t+1} \sim \mathcal{E}} \left[ r(s_t, a_t) + \gamma \mathbb{E}_{a_{t+1} \sim \pi} \left[ Q^\pi(s_{t+1}, a_{t+1}) \right] \right]
$$

Deterministic policy $\mu$의 경우에 대해서는 Bellman equation의 형태가 변화한다.

$$
Q^\mu(s_t, a_t) = \mathbb{E}_{r, s_{t+1} \sim \mathcal{E}} \left[ r(s_t, a_t) + \gamma Q^\mu(s_{t+1}, \mu(s_{t+1})) \right]
$$

Q-Learning에서는 deterministic한 greedy policy를 이용하여 Q estimation의 target value를 정하고 이를 MSE loss를 이용하여 optimize한다.

$$
L(\theta^Q) = \mathbb{E}_{s_t \sim \rho^\beta, a_t \sim \beta, r \sim \mathcal{E}} \left[ \left( Q(s_t, a_t | \theta^Q) - y_t \right)^2 \right]\\
\text{where } y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, \mu(s_{t+1}) | \theta^Q).
$$


## Algorithm

DQN에서 사용하는 알고리즘을 곧바로 continuous space에 들여오는 건 불가능하다. 그 이유는 target value를 구하는 과정에서 $\max\_{a'} Q(s',a')$ 연산을 수행하기 위해 수많은 action space를 탐색해야 하기 때문이다. 이를 해결하기 위해 DPG 알고리즘을 베이스로 변경한다. DPG 알고리즘에서는 state에서 action으로 가는 deterministic한 함수, 즉 parameterized action function $\mu(s\|\theta^\mu)$를 정의한다. Policy gradient의 형태에 $\mu$를 넣어주면

$$
\begin{align*}
  \nabla_{\theta^\mu} J &\approx \mathbb{E}_{s_t \sim \rho^\beta} \left[ \nabla_a Q(s, a | \theta^Q) \Big|_{s_t, a_t = \mu(s_t | \theta^\mu)} \right]\\
  &= \mathbb{E}_{s_t \sim \rho^\beta} \left[ \nabla_a Q(s, a | \theta^Q) \Big|_{s_t, a_t = \mu(s_t)} \nabla_{\theta^\mu} \mu(s | \theta^\mu) \Big|_{s_t} \right]
\end{align*}

$$

이다. DPG 논문에서 이 형태 또한 policy gradient임을 증명하였다.

Non-linear function approximation을 이용하여 Q learning을 하게 되면 더 이상 수렴성이 보장되지 않는다. DDPG와 유사한 연구인 NFQCA는 학습 안정성을 위해 배치 학습(데이터 전체를 이용하여 학습)이 필요하기 때문에 미니배치를 통해 학습하는 DDPG와 차이가 있다.

DDPG에서 강화학습이 겪는 몇가지 어려움을 해결하기 위해 각각의 해결책을 제시한다.

- i.i.d. 가정을 위반한다: 최적화 알고리즘은 독립적이고 동일한 분포 가정을 한다. 그러나 강화학습에서는 agent가 환경을 순차적으로 탐색하면 데이터를 생성하기 때문에 샘플 간의 상관관계가 높다.
- 온라인 학습: 신경망 학습을 효과적으로 하기 위해서는 미니배치 단위의 학습을 필요로 한다. 하지만 강화학습의 경우 경험을 통해 순차적으로 데이터가 수집되는 형태로, 이를 직접적으로 적용하게 되면 학습이 불안정해질 수 있다.
  
**>> DQN과 같이 Replay buffer를 사용한다.**

- DQN에서는 target network를 agent network와 분리하여 target value가 발산하는 것을 막는다.

**>> DQN에서 사용한 target network 뿐만 아니라 soft target update**

$$
\theta' \leftarrow \tau\theta+(1-\tau)\theta'
$$

**를 적용한다.**

- 서로 다른 물리량의 observation이 학습에 부정적인 영향을 줄 수 있다.

**>> Batch normalization을 사용하여 수동적으로 조작할 필요를 없앤다.**

- Continuous space에서 exploration이 어려운 경우가 발생한다.

**>> Actor network에 noise**

$$
\mu'(s_t) = \mu(s_t | \theta_t^\mu) + \mathcal{N}
$$

**를 추가한다.** 노이즈는 Ornstein-Uhlenbeck process를 이용하여 생성되며 물리적인 제어 문제에 적합하다.

<details>
<summary>Ornstein-Uhlenbeck process 상세</summary>
<div markdown="1">
### **Ornstein-Uhlenbeck 과정(Ornstein-Uhlenbeck Process)란?**
강화학습에서 **연속적(continuous) 행동 공간**을 다룰 때, **탐색(Exploration)이 어려운 문제**가 존재한다.  
DDPG에서는 이 문제를 해결하기 위해 **오른스타인-울렌벡 과정(Ornstein-Uhlenbeck Process, OU Process)** 을 활용하여 **탐색 정책**을 설계하였다.

---
## **1. OU 과정의 개념**
오른스타인-울렌벡 과정은 **시간에 따라 변화하는 노이즈를 생성하는 확률 과정(stochastic process)** 이다.  
이 과정은 **물리학과 금융 공학**에서 자주 사용되며, 특히 **이전 값과의 상관관계를 유지하는 노이즈**를 생성하는 데 유용하다.

OU 과정은 일반적으로 다음 **확률 미분 방정식(Stochastic Differential Equation, SDE)** 로 정의된다.

\\(
dX_t = \theta (\mu - X_t) dt + \sigma dW_t
\\)

여기서:
- \\( X_t \\)= 현재 상태 (OU 노이즈 값)
- \\( \mu \\)= 평균 (노이즈가 수렴할 목표 값)
- \\( \theta \\)= 평균으로 되돌아가는 속도 (Reversion Rate)
- \\( \sigma \\)= 노이즈의 크기 (Volatility)
- \\( W_t \\)= 표준 브라운 운동(Standard Wiener Process), 즉 무작위 확률적 변동 요소

이 방정식을 풀어서 보면, OU 과정은 다음과 같은 특징을 갖는다.

1. **평균으로 회귀하는 성질(Mean Reversion)**
   - 노이즈 값이 평균 \\( \mu \\)로 **천천히 되돌아가도록 설정**된다.
   - 즉, 노이즈가 무작위로 움직이지만, **일정한 평균을 중심으로 변동**한다.

2. **시간적으로 연관된 노이즈(Temporal Correlation)**
   - 기존의 가우시안 노이즈(White Noise)는 **서로 독립적(i.i.d.)** 인 반면, OU 과정에서 생성된 노이즈는 **이전 값과 일정한 상관관계를 유지**한다.
   - 즉, **이전 노이즈 값과 현재 노이즈 값이 일정한 관계를 가지며, 부드럽게 변화**한다.

3. **랜덤 노이즈 요소 추가(Exploration Noise)**
   - OU 과정은 노이즈 크기 \\( \sigma \\)를 조절할 수 있어, 탐색 과정에서 **적절한 수준의 무작위성(Randomness)을 유지**할 수 있다.

---
## **2. DDPG에서 OU 과정 적용**
강화학습에서 OU 과정이 필요한 이유는 다음과 같다.

1. **물리 제어(Physical Control) 문제에서 더 효과적인 탐색 가능**
   - 물리 환경에서는 **관성이 있는 행동(Inertia)** 이 존재한다.
   - 즉, **이전 행동이 다음 행동에도 영향을 미친다**.
   - 예를 들어, 로봇이 움직일 때, 순간적으로 완전히 새로운 행동을 하는 것이 아니라 **점진적으로 변화**해야 한다.

2. **기존의 무작위 탐색 방식(White Noise)보다 부드러운 탐색 가능**
   - 일반적인 가우시안 노이즈(White Noise)를 사용하면 **각 행동이 독립적으로 변화**하기 때문에 행동이 급격히 변할 수 있다.
   - OU 과정을 사용하면 **이전 행동과 유사한 행동을 더 많이 선택**하게 되어, 더욱 자연스러운 탐색이 가능하다.

---
## **3. DDPG에서의 OU 탐색 정책**
DDPG에서는 **OU 과정으로 생성된 노이즈를 액터(Actor) 정책에 추가하여 탐색을 수행**한다.

탐색 정책:

\\(
\pi'(s_t) = \pi(s_t | \theta^\pi_t) + \mathcal{N}
\\)

여기서 **\(\mathcal{N}\)** 은 **OU 과정으로 생성된 노이즈**이며, 이를 액터 정책에 추가함으로써 **탐색 성능을 향상**시킨다.

DDPG에서 사용된 OU 과정의 수식은 다음과 같다.

\\(
dx_t = \theta (\mu - x_t) dt + \sigma dW_t
\\)

이 방정식을 **이산화(discretization)** 하면 다음과 같이 표현할 수 있다.

\\(
x_{t+1} = x_t + \theta (\mu - x_t) \Delta t + \sigma \epsilon_t \sqrt{\Delta t}
\\)

여기서:
- \\( x_t \\)= OU 노이즈 값
- \\( \mu \\)= 평균 (일반적으로 0으로 설정)
- \\( \theta \\)= 평균 복귀 계수 (일반적으로 0.15 정도)
- \\( \sigma \\)= 노이즈 크기 (일반적으로 0.2 정도)
- \\( \epsilon_t \\)= 표준 가우시안 노이즈 \( N(0,1) \)
- \\( \Delta t \\)= 시간 증가량 (보통 1로 설정)

---
## **4. OU 과정이 없는 경우와 비교**
OU 과정을 사용하지 않고 단순히 가우시안 노이즈(White Noise)를 추가하는 경우, 행동 탐색이 불안정할 수 있다.

### **비교 예시**

| 노이즈 방식 | 특징 | 문제점 |
|------------|----------------|------------|
| **가우시안 노이즈** | 각 행동이 독립적으로 무작위 | 급격한 행동 변화 발생 가능 |
| **OU 과정 노이즈** | 이전 행동과 연속적인 탐색 수행 | 더 부드럽고 현실적인 행동 생성 |

즉, OU 노이즈를 사용하면 **시간적으로 상관된 행동을 생성하여 더욱 자연스러운 탐색이 가능**하다.

---
## **5. 실험 및 적용 사례**
DDPG에서 OU 과정을 적용한 결과:
- **물리적 환경(예: 로봇 제어, 드론 제어)에서 효과적인 탐색을 수행**할 수 있었다.
- **이산적 행동 공간에서는 필요 없지만, 연속적 행동 공간에서는 필수적인 탐색 방법**으로 작용한다.
- **특히 로봇이 관성을 가지고 있는 경우, OU 과정이 탐색 성능을 향상**시키는 것이 확인되었다.

---
## **6. 결론**
OU 과정은 **시간적으로 상관된 노이즈를 생성하여, 강화학습에서 더욱 부드럽고 효과적인 탐색을 가능하게 하는 기법**이다.

- **DQN과 같은 이산적 행동 공간에서는 불필요하지만, DDPG와 같은 연속적 행동 공간을 다루는 알고리즘에서는 필수적인 요소**이다.
- OU 과정은 **이전 행동과 연속적인 관계를 유지하면서 탐색을 수행**하도록 도와준다.
- **특히 로봇 제어와 같은 환경에서 더욱 효과적인 탐색 성능을 보인다.**

즉, DDPG에서 OU 과정을 활용함으로써 **무작위성이 필요하지만 연속성이 유지되어야 하는 문제에서 더욱 효과적인 정책 학습이 가능해진다.** 🚀
</div>
</details>
\
위 사항들을 적용한 DDPG 알고리즘은 다음과 같다.

<center>
<img src='{{"assets/images/DDPG/ddpg1.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">DDPG algorithm</figcaption>
</center>

Target network는 target value $y\_i$를 구하는 데에만 사용되는 것을 알 수 있다.

> Q: Behavior policy와 target policy는 $\mu$로 같은데 왜 off-policy인가?\\
> 탐색 시에는 noise가 더해져 다른 distribution을 갖는다.



## Results

<center>
<img src='{{"assets/images/DDPG/ddpg2.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Test environments</figcaption>
</center>

TORCS와 MUJOCO 환경에서 테스트한다. MUJOCO의 경우 저차원 환경(joint angles, positions 등이 제공)과 고차원 환경(frame을 observation으로 사용) 두 경우 모두 테스트한다.

<center>
<img src='{{"assets/images/DDPG/ddpg3.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">DDPG results</figcaption>
</center>

lowd(저차원 환경), pix(고차원 환경), cntrl(DPG + replay buffer + batchnorm, DDPG의 no target network, no soft update 버전이라 보면 될듯?)의 결과이다. 0은 random agent의 점수와 같고 1은 iLQG planning algorithm이 기록한 점수이다. 일부 task에 대해서는 planning algorithm의 기록을 넘어서는 것을 확인할 수 있다. TORCS의 경우 raw reward값을 기록하였다.

<center>
<img src='{{"assets/images/DDPG/ddpg4.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Ablations</figcaption>
</center>

Target network가 없는 경우 성능이 급격히 저하(연한 회색)되는 것으로 보아 target network가 학습에 중요한 영향을 미침을 알 수 있다.

<center>
<img src='{{"assets/images/DDPG/ddpg5.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Q estimation vs Actual return</figcaption>
</center>

비교적 간단한 task에서는 거의 return과 비슷한 Q value estimation을 확인할 수 있다. 복잡한 task에 대해서는 Q value가 과대평가 되는 부분이 있으나 그래도 높은 Q estimation이 실제로 더 많은 return을 받도록 하는 경향성은 유지하고 있다.

## Related work

DDPG와 비교될 수 있는 model-free policy gradient 알고리즘은 TRPO, SVG(0), Guided policy search가 있다. Model-based 알고리즘은 PILCO 등이 있다. Evolutionary strategy 기반 강화학습도 있다.

## Conclusion