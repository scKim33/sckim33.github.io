---
title: "[TRPO] Trust Region Policy Optimization"
last_modified_at: 2025-03-04
categories:
  - paper_review
tags:
  - TRPO
  - Reinforcement Learning
  - On policy
  - Model free
excerpt: "TRPO paper review"
use_math: true
classes: wide
---

> ICML 2015. [[Paper](https://arxiv.org/abs/1502.05477)]  
> John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel  
> 19 Feb 2015

## Summary

Natural policy gradient의 아이디어를 이용하여 local approximation of expected return의 monotonic improvement를 보장하는 on policy 알고리즘을 제시한다.

## Introduction

Policy optimization의 방법에는 크게 policy iteration, policy gradient, derivative-free optimization이 있다. 본 연구에서는 surrogate objective function을 minimizing하는 것이 policy improvement를 보장한다는 것을 증명한다. 이를 approximation하여 TRPO 알고리즘을 두 가지 variant로 제시한다.

## Preliminaries

MDP setting에서 목적 함수는 다음과 같이 정의된다.

$$
\eta(\pi) = \mathbb{E}_{s_0, a_0, \dots} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t) \right], \text{ where } \\
s_0 \sim \rho_0(s_0), \quad a_t \sim \pi(a_t | s_t), \quad s_{t+1} \sim P(s_{t+1} | s_t, a_t).
$$

그리고 state distribution을 정의하면

$$
\rho_{\pi}(s) = P(s_0 = s) + \gamma P(s_1 = s) + \gamma^2 P(s_2 = s) + \dots,
$$

이제 다음 식이 성립함을 보일 수 있다. 이 식은 policy update 전후로 policy improvement가 이루어졌는지 확인하는 용도로 사용할 수 있다.

$$
\begin{align}
    \eta(\tilde{\pi}) &= \eta(\pi) + \mathbb{E}_{s_0, a_0, \dots \sim \tilde{\pi}} \left[ \sum_{t=0}^{\infty} \gamma^t A_{\pi}(s_t, a_t) \right], \\
    &= \eta(\pi) + \sum_{t=0}^{\infty} \sum_{s} P(s_t = s | \tilde{\pi}) \sum_{a} \tilde{\pi}(a | s) \gamma^t A_{\pi}(s, a) \\
    &= \eta(\pi) + \sum_{s} \sum_{t=0}^{\infty} \gamma^t P(s_t = s | \tilde{\pi}) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a) \\
    &= \eta(\pi) + \sum_{s} \rho_{\pi}(s) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a).
\end{align}
$$

식을 살펴보면 policy improvement가 이루어지는 조건을 확인할 수 있다.$\sum_{a} \tilde{\pi}(a \| s) A_{\pi}(s, a)\geq 0$가 모든 $s$에 대해 성립한다면 policy improvement가 이루어진 것이다. Greedy policy를 취한다면 각 $s$마다 positive advantage function을 갖는 $(s,a)$ 쌍이 하나씩만 존재하면 된다(존재할 수 밖에 없는게, 어떤 쌍에 대해 advantage가 negative이면 반드시 다른 쌍에 대해서는 positive가 된다). 그러나 approximation setting에서는 이러한 가정이 지켜지지 않을 수 있다. 게다가 $\tilde{\pi}$는 변하기 때문에 이로 인해 발생하는 복잡한 dependency가 optimize하기 어렵게 만든다. 따라서 $\pi$의 state distribution을 사용하는 다음의 local approximation을 고려한다.

$$
L_{\pi}(\tilde{\pi}) = \eta(\pi) + \sum_{s} \rho_{\pi}(s) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a).
$$

이제 state distribution은 고정되었다. 여기서 approximation으로 \\(\pi_\theta\\)를 사용하고 특정 값 \\(\theta_0\\)를 사용하면 RHS의 두번째 항은 사라지고(sum of $A$ = 0), local approximation과 policy gradient가 1차 미분이 동일한 값을 가진다.

$$
\begin{align*}
    L_{\pi_{\theta_0}}(\pi_{\theta_0}) &= \eta(\pi_{\theta_0}), \\
    \nabla_{\theta} L_{\pi_{\theta_0}}(\pi_{\theta}) \Big|_{\theta = \theta_0} &= \nabla_{\theta} \eta(\pi_{\theta}) \Big|_{\theta = \theta_0}.
\end{align*}
$$

위 식을 통해 충분히 작은 정도로 policy를 업데이트 하면 local approximation의 gradient로부터 true objective의 gradient를 구할 수 있다. 그런데 충분히 작은 값에 대한 명확한 기준이 없다. 이를 Improvement의 lower bound를 보장하는 방법으로 conservative policy iteration을 사용할 수 있다. Conservative policy iteration을 진행하기 위해 \\(\pi' = \arg\max_{\pi'} L_{\pi_{\text{old}}}(\pi').\\)라고 하자. 다음과 같이 policy 업데이트를 진행하는 경우 objective에 lower bound를 만족하며 업데이트된다.

$$
\pi_{\text{new}}(a | s) = (1 - \alpha) \pi_{\text{old}}(a | s) + \alpha \pi'(a | s).\\
\eta(\pi_{\text{new}}) \geq L_{\pi_{\text{old}}}(\pi_{\text{new}}) - \frac{2 \epsilon \gamma}{(1 - \gamma)^2} \alpha^2, \\
\text{where} \quad \epsilon = \max_s \left| \mathbb{E}_{a \sim \pi'(a | s)} \left[ A_{\pi}(s, a) \right] \right|.
$$

이 lower bound는 위와 같은 방식으로 policy update가 이루어져야만 만족한다는 단점이 있다. $\arg\max$를 만족하는 policy를 찾는 것이 번거롭기 때문에 좀 더 practical한 방식을 제안한다.

## Monotonic Improvement Guarantee for General Stochastic Policies

$\alpha$ 대신 total variance \\(D_{\text{TV}}(p \parallel q) = \frac{1}{2} \sum_i \|p_i - q_i\|\\)를 업데이트 되는 정도로 정한다. 

$$
D_{\text{TV}}^{\max} (\pi, \tilde{\pi}) = \max_s D_{\text{TV}} (\pi(\cdot | s) \parallel \tilde{\pi}(\cdot | s)).
$$

> \\\(\textbf{Theorem 1.}\\\)Let \\( \alpha = D_{\text{TV}}^{\max}(\pi_{\text{old}}, \pi_{\text{new}}) \\). Then the following bound holds: \\
> \\[
    \eta(\pi_{\text{new}}) \geq L_{\pi_{\text{old}}}(\pi_{\text{new}}) - \frac{4 \epsilon \gamma}{(1 - \gamma)^2} \alpha^2,
\\]
\\[
    \text{where} \quad \epsilon = \max_{s, a} |A_{\pi}(s, a)|.
\\]

Total variation divergence와 KL divergence 간의 관계 \\(D_{\text{TV}}(p \parallel q)^2 = D_{\text{KL}}(p \parallel q)\\)를 이용하면 Theorem 1.을 다음과 같이 변형할 수 있다. \\(D_{\text{KL}}^{\max} (\pi, \tilde{\pi}) = \max_s D_{\text{KL}} (\pi(\cdot \| s) \parallel \tilde{\pi}(\cdot \| s))\\)라 하면

$$
\eta(\tilde{\pi}) \geq L_{\pi}(\tilde{\pi}) - C D_{\text{KL}}^{\max} (\pi, \tilde{\pi}),\\
\text{where} \quad C = \frac{4 \epsilon \gamma}{(1 - \gamma)^2}.
$$

이를 통해 policy가 monotonically improving한다는 사실을 보일 수 있다. \\( M_i(\pi) = L_{\pi_i}(\pi) - C D_{\text{KL}}^{\max} (\pi_i, \pi) \\)라 하면

$$
\eta(\pi_{i+1}) \geq M_i(\pi_{i+1}) \quad \text{by Equation above}\\
\eta(\pi_i) = M_i(\pi_i), \text{ therefore,}\\  
\eta(\pi_{i+1}) - \eta(\pi_i) \geq M_i(\pi_{i+1}) - M(\pi_i).\\
\text{Finally, }\eta(\pi_0) \leq \eta(\pi_1) \leq \eta(\pi_2) \leq \dots
$$

이를 알고리즘으로 나타내면 다음과 같다.

<center>
<img src='{{"assets/images/TRPO/trpo1.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Policy iteration in TRPO</figcaption>
</center>

## Optimization of Parameterized Policies

이제 parameterized된 policy에 대해서도 알고리즘을 적용하면 다음을 계산해야 한다.

$$
\max_{\theta} \left[ L_{\theta_{\text{old}}}(\theta) - C D_{\text{KL}}^{\max} (\theta_{\text{old}}, \theta) \right].
$$

$C$의 경우 앞서 보았던 $C$의 조건에 맞추어 값을 넣게 되면 policy update 정도가 너무 작아지는 경우가 생긴다. 따라서 $C$를 없애는 대신 KL divergence 값에 제한을 두는 방식을 사용한다.

$$
\max_{\theta} L_{\theta_{\text{old}}}(\theta)\\
\text{subject to} \quad D_{\text{KL}}^{\max} (\theta_{\text{old}}, \theta) \leq \delta.
$$

한편 constraint는 모든 $s$에 대해 고려해야 하므로 너무 많은 constraint가 존재한다. 이를 평균 KL divergence로 바꾸면 좀 더 practical한 알고리즘을 구할 수 있다.

$$
\overline{D}_{\text{KL}}^{\rho}(\theta_1, \theta_2) := \mathbb{E}_{s \sim \rho} \left[ D_{\text{KL}} (\pi_{\theta_1}(\cdot | s) \parallel \pi_{\theta_2}(\cdot | s)) \right].\\
\max_{\theta} L_{\theta_{\text{old}}}(\theta)\\
\text{subject to} \quad \overline{D}_{\text{KL}}^{\rho_{\theta_{\text{old}}}} (\theta_{\text{old}}, \theta) \leq \delta.
$$

## Sample-Based Estimation of the Objective and Constraint

위에서 살펴보았던 최적화 문제를 데이터 샘플에 맞추어 학습하기 위해 추가적인 식의 변형이 필요하다. 데이터 샘플의 경우 우리는 old policy의 데이터만 가지고 업데이트를 하게 된다. 따라서 importance sampling을 이용하여 objective를 보정해 준다.

$$
\sum_{a} \pi_{\theta}(a | s_n) A_{\theta_{\text{old}}} (s_n, a) = \mathbb{E}_{a \sim q} \left[ \frac{\pi_{\theta}(a | s_n)}{q(a | s_n)} A_{\theta_{\text{old}}} (s_n, a) \right].
$$

이제 최적화 문제는 다음과 같이 변형된다.

$$
\max_{\theta} \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}, a \sim q} \left[ \frac{\pi_{\theta}(a | s)}{q(a | s)} Q_{\theta_{\text{old}}} (s, a) \right]\\
\text{subject to} \quad \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}} \left[ D_{\text{KL}} (\pi_{\theta_{\text{old}}}(\cdot | s) \parallel \pi_{\theta}(\cdot | s)) \right] \leq \delta.
$$

<center>
<img src='{{"assets/images/TRPO/trpo2.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Single path vs Vine</figcaption>
</center>

이제 estimation을 수행하는 두 가지 방법인 single path, vine(나뭇가지라는 뜻)에 대해 소개한다. TRPO의 경우 KL divergence 제약을 vine을 이용해서 샘플링 과정에 적용한다.
- Single path: 기존과 같이 \\(\pi_{\theta_{\text{old}}}(a\|s)\\)를 이용하여 전체 episode에 대한 trajectory를 생성하고 이를 이용하여 policy update한다.
- Vine: 전체 episode에 대한 trajectory 대신 $N$ step까지만 \\(\pi_{\theta_{i}}(a\|s)\\)를 이용해서 뽑는다. 이를 rollout set이라 한다. Rollout set의 각 \\(s_n\\)에서 \\(\pi_{\theta_{i}}(a\|s)\\)이 $K$개의 action을 샘플링한다. 생성된 $K$개의 $(s,a)$ pair에서 시작하는 rollout을 통해 \\(\hat{Q}\_{\theta_i}(s_n,a_{n,k})\\)를 추정하여 objective function을 estimate한다. 이를 이용하여 이를 통해 variance를 감소시켜 better estimate하는 효과가 있다. 다만 연산량의 증가 및 특정 state로의 초기화가 여러 번 반복될 수 있는 환경을 필요로 한다.

\\[
L_n(\theta) = \frac{\sum_{k=1}^{K} \frac{\pi_{\theta} (a_{n,k} | s_n)}{\pi_{\theta_{\text{old}}} (a_{n,k} | s_n)} Q(s_n, a_{n,k})}
{\sum_{k=1}^{K} \frac{\pi_{\theta} (a_{n,k} | s_n)}{\pi_{\theta_{\text{old}}} (a_{n,k} | s_n)}}.
\\]


## Practical algorithm

1. Single path 또는 vine 방식으로 데이터를 수집한다.
2. State, action에 대해 average하여 objective function을 계산한다.
3. Constrained optimization problem을 approximate하여 gradient를 구한다. Conjugate gradient algorithm 이후 line search를 수행하여 구할 수 있다.

3의 과정에서 KL divergence의 quadratic approximation을 Fisher matrix라 정의한다.

$$
Ax=g\\
D_{\text{KL}} (\theta_{\text{old}}, \theta) \approx \frac{1}{2} (\theta - \theta_{\text{old}})^T A (\theta - \theta_{\text{old}}).\\
A_{ij} = \frac{\partial}{\partial \theta_i} \frac{\partial}{\partial \theta_j} \overline{D}_{\text{KL}}(\theta_{\text{old}}, \theta).
$$

Large scale problem에서는 Fisher matrix의 inverse를 직접 계산하기 힘들다. 따라서 Fisher-vector product를 이용하여 연산량을 줄인다. 구체적인 방법은 다음과 같다. KL divergence를 두번 미분한다.

$$
D_{\text{KL}} (\pi_{\theta_{\text{old}}} (\cdot | x) \parallel \pi_{\theta} (\cdot | x)) = \text{kl}(\mu_{\theta}(x), \mu_{\text{old}})\\
\frac{\partial \mu_a (x)}{\partial \theta_i} \frac{\partial \mu_b (x)}{\partial \theta_j} \text{kl}''_{ab} (\mu_{\theta}(x), \mu_{\text{old}})
+ \frac{\partial^2 \mu_a (x)}{\partial \theta_i \partial \theta_j} \text{kl}'_a (\mu_{\theta}(x), \mu_{\text{old}})
$$

자코비안 \\(J := \frac{\partial \mu_a (x)}{\partial \theta_i}\\)을 이용하여 KL divergence를 분해할 수 있다.

$$
F=J^T M J, \quad \text{where} \quad M = \text{kl}''_{ab} (\mu_{\theta}(x), \mu_{\text{old}})\\
\text{Now, Fisher-vector product is }y \to J^T M J y.
$$

이제 conjugate gradient를 이용하여 역행렬 계산 대신 iteration 방법으로 numerical optimization을 진행한다.

<details>
<summary>Conjugate gradient method에 대해</summary>
<div markdown="1">
## **📌 Conjugate Gradient (CG) 상세 설명**
Conjugate Gradient(CG) 방법은 **큰 규모의 선형 시스템 \\(Ax = b\\)를 풀기 위한 효율적인 최적화 알고리즘**이야.  
특히, **고차원 Fisher Information Matrix의 역행렬을 직접 구하는 대신 근사적으로 계산하는 데 유용**해.

---

## **1. Conjugate Gradient가 필요한 이유**
TRPO 같은 알고리즘에서 **자연 정책 그래디언트(Natural Gradient) 업데이트**를 수행하려면 아래 공식을 풀어야 해.

\\(
F^{-1} g
\\)

여기서:
- \\( F \\)= **Fisher Information Matrix (피셔 정보 행렬)**
- \\( g \\)= **일반적인 정책 그래디언트** \\( \nabla_{\theta} L(\theta) \\)

하지만 **F의 크기가 너무 크면 직접 역행렬을 계산하는 것은 현실적으로 불가능**해.
- 일반적인 **행렬 역행렬 계산 비용은 \\(O(n^3)\\)** (너무 비싸!)
- \\( F \\)의 크기가 수만 개 이상의 파라미터일 경우, **메모리 부족 문제도 발생할 수 있음.**

💡 **해결책?**  
**Conjugate Gradient(CG) 방법을 사용하면 \\( F^{-1} g \\)를 직접 구하지 않고도 선형 시스템을 근사적으로 풀 수 있음!**

---

## **2. Conjugate Gradient 알고리즘이란?**
Conjugate Gradient는 **크고 희소한(Sparse) 대칭 양의 정부호(SPD) 행렬 \\( A \\)에 대한 선형 방정식**:

\\(
Ax = b
\\)

을 푸는 방법이야.

### **✅ 일반적인 방법과의 비교**
- **고전적인 방법 (직접 계산)**
  - 행렬 \\( A \\)의 역행렬을 직접 구해서 \\( x = A^{-1} b \\)를 계산.
  - 계산 비용이 \\( O(n^3) \\)이라 현실적으로 불가능.

- **CG 방법 (근사적으로 푸는 방법)**
  - \\( A \\)의 역행렬을 구하지 않고 \\( x \\)를 점진적으로 개선하는 방식.
  - **행렬-벡터 곱 \\( Av \\)만 필요**하므로 계산량이 대폭 감소.
  - 일반적으로 **\\( O(n) \\)~\\( O(n^2) \\)로 해결 가능**.

### **✅ 핵심 아이디어**
1. **최적의 방향을 찾아 반복적으로 해를 개선**하는 방식.
2. 단순한 경사 하강법(Gradient Descent)보다 훨씬 빠르게 수렴.
3. **역행렬 없이 선형 방정식을 효율적으로 해결 가능**.

---

## **3. Conjugate Gradient 알고리즘 작동 방식**
목표는 \\( Ax = b \\)를 푸는 것이야.

### **✅ 기본적인 경사 하강법 (Gradient Descent)**
가장 단순한 방법은 **경사 하강법**을 사용하는 것인데:

\\(
x_{k+1} = x_k + \alpha \cdot r_k
\\)

여기서:
- \\( r_k = b - Ax_k \\)는 **현재 오차(residual)**
- \\( \alpha \\)는 **스텝 크기**

하지만, 경사 하강법은 **모든 방향에서 동일한 비율로 이동**하기 때문에 수렴 속도가 느려질 수 있어.

### **✅ CG의 개선점: 직교(conjugate) 방향을 활용**
Conjugate Gradient는 **기존의 경사 방향과 독립적인(=직교하는) 방향을 사용하여 최적화**하는 방식이야.

💡 **CG는 기존 Gradient Descent보다 훨씬 빠르게 수렴하는 이유?**
- **새로운 방향을 선택할 때, 이전 방향과 직교(Conjugate)하도록 보장함**.
- 이렇게 하면 **한 번 업데이트한 방향으로 다시 돌아올 필요가 없기 때문에, 최소한의 반복으로 수렴 가능**.

---

## **4. Conjugate Gradient 알고리즘 단계별 과정**
### **🔹 Step 1: 초기화**
- 초기값 \\( x_0 \\)을 임의로 설정.
- 초기 오차(residual): \\( r_0 = b - Ax_0 \\).
- 첫 번째 탐색 방향을 **오차 방향으로 설정**: \\( p_0 = r_0 \\).

### **🔹 Step 2: 반복 업데이트**
반복문을 수행하면서 다음과 같이 업데이트:

1. **스텝 크기(learning rate) 계산**:

   \\(
   \alpha_k = \frac{r_k^T r_k}{p_k^T A p_k}
   \\)

2. **새로운 해 업데이트**:

   \\(
   x_{k+1} = x_k + \alpha_k p_k
   \\)

3. **새로운 오차 계산**:

   \\(
   r_{k+1} = r_k - \alpha_k A p_k
   \\)

4. **새로운 방향 벡터 설정**:

   \\(
   \beta_k = \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}
   \\)

   \\(
   p_{k+1} = r_{k+1} + \beta_k p_k
   \\)

5. 수렴 조건이 만족될 때까지 반복.

---

## **5. TRPO에서 CG를 어떻게 활용하는가?**
TRPO에서는 **자연 정책 그래디언트**를 계산할 때 다음 문제를 풀어야 해:

\\(
F v = g
\\)

여기서:
- \\( F \\)= **Fisher Information Matrix**
- \\( g \\)= **일반적인 정책 그래디언트**
- \\( v = F^{-1} g \\)를 찾아야 함 → **CG를 사용하여 근사적으로 해결!**

💡 **CG의 장점**
- **F의 역행렬을 직접 구하지 않고도 자연 그래디언트를 계산 가능!**
- **\\( F v \\)연산만 필요하므로 계산량이 대폭 감소!**
- **고차원에서도 효율적인 학습 가능!**

### **TRPO에서 CG 최적화 기법**
- **CG 반복 횟수 \\( k \\)를 10으로 설정** (너무 많이 반복하면 비효율적)
- **데이터 일부만 샘플링하여 Fisher-Vector Product 계산** (전체 데이터 사용 X)
- **최종적으로 CG를 사용하면 일반적인 그래디언트 업데이트와 비슷한 계산량으로 자연 그래디언트를 구할 수 있음!**

---

## **6. CG vs. 다른 방법 비교**

| 방법 | 특징 | 계산 비용 |
|------|------|------|
| **직접 역행렬 계산** | \\( F^{-1} g \\)직접 계산 | \\( O(n^3) \\) (비효율적) |
| **Gradient Descent** | 일반적인 경사 하강법 | \\( O(n) \\), 하지만 수렴이 느림 |
| **Conjugate Gradient (CG)** | 가장 빠른 방향으로 업데이트, 빠른 수렴 | \\( O(n) \\)~\\( O(n^2) \\), 가장 효율적 |

---

## **7. 결론**
### **📌 Conjugate Gradient의 핵심**
- **큰 규모의 선형 시스템 \\( Ax = b \\)를 푸는 효율적인 알고리즘.**
- **자연 정책 그래디언트 계산에서 \\( F^{-1} g \\)를 직접 구하지 않고 근사적으로 해결.**
- **역행렬을 직접 계산하는 것보다 훨씬 빠르고 효율적.**
- **TRPO에서는 CG를 활용하여 Fisher-Vector Product를 효과적으로 계산함.**
- **데이터 서브샘플링과 결합하면 계산 비용이 일반적인 그래디언트 업데이트와 비슷해짐.**

---

이해 안 되는 부분 있으면 질문해줘! 🚀
</div>
</details>
\
Iteration을 $k=10$ 정도로 설정하면 충분히 좋은 성능은 보인다고 한다. Conjugate gradient 없이는 90%의 computing 자원을 역행렬 연산에 소비해야 하는데 이를 해결할 수 있었다. 마지막으로 데이터 전체를 사용하는 것이 아닌 10% 가량을 사용하여 Fisher matrix를 계산하는 연산 비용을 줄인다.

이제 gradient의 방향$s\approx A^-1g$은 구하였고 얼만큼 업데이트 할지 정하면 된다. Maximal step length $\beta$에 대해 $\theta+\beta s$는 KL divergence constraint를 만족해야 한다. 따라서 \\(\delta = \overline{D}_{\text{KL}} \approx \frac{1}{2} (\beta s)^T A (\beta s) = \frac{1}{2} \beta^2 s^T A s\\)를 만족하게 되고 \\(\beta = \sqrt{2\delta / s^T A s}\\)임을 알 수 있다. 여기서 적절한 값을 line search를 통해 찾는다.

Analytic한 부분과 practical한 알고리즘 구현 과정을 요약하면 다음과 같다.
- KL divergence penalty를 두어 surrogate objective를 최적화한다.
- 그런데 penalty coefficient $C$를 너무 크게 잡으면 업데이트가 별로 이루어지지 않는다.
- $C$를 고른다는 것은 경험적으로 어렵기 때문에 KL divergence에 bound를 주는 방식으로 대신한다.
- 또한 KL divergence의 maximization을 구하는 것은 연산량 측면에서 좋지 않기 때문에, KL divergence의 평균을 대신 사용하여 실제로는 샘플링으로 KL divergence의 평균을 근사하게 된다.
- Advantage function의 estimation error에 대해서는 simplicity를 위해 무시한다.

## Connections with Prior Work

이 연구와 유사한 개념으로 natural policy gradient가 있다. Objective function에 대해 1차 근사식으로 사용하고, KL divergence에 대해 2차 근사식을 사용하는 것이다. 

$$
\max_{\theta} L_{\theta_{\text{old}}}(\theta)\\
\text{subject to} \quad \overline{D}_{\text{KL}}^{\rho_{\theta_{\text{old}}}} (\theta_{\text{old}}, \theta) \leq \delta.
$$

그러면 다음과 같이 optimization problem이 바뀐다.

$$
\max_{\theta} \left[ \nabla_{\theta} L_{\theta_{\text{old}}} (\theta) \big|_{\theta = \theta_{\text{old}}} \cdot (\theta - \theta_{\text{old}}) \right]\\
\text{subject to} \quad \frac{1}{2} (\theta_{\text{old}} - \theta)^T A(\theta_{\text{old}}) (\theta_{\text{old}} - \theta) \leq \delta,\\
\text{where} \quad A(\theta_{\text{old}})_{ij} = \frac{\partial}{\partial \theta_i \partial \theta_j} \mathbb{E}_{s \sim \rho_{\pi}} \left[ D_{\text{KL}} (\pi(\cdot | s, \theta_{\text{old}}) \parallel \pi(\cdot | s, \theta)) \right] \big|_{\theta = \theta_{\text{old}}}.\\
$$

그러면 다음과 같이 policy parameter가 업데이트된다.

$$
\theta_{\text{new}} = \theta_{\text{old}} + \frac{1}{\lambda} A(\theta_{\text{old}})^{-1} \nabla_{\theta} L(\theta) \big|_{\theta = \theta_{\text{old}}}.
$$

여기서는 고정된 step size로 업데이트가 이루어지는데, TRPO에서는 KL divergence의 제약조건에 따라 step size가 달라진다. 논문에서는 이 차이가 상당한 성능 향상을 이끌었다고 말하고 있다.

## Experiments

<center>
<img src='{{"assets/images/TRPO/trpo3.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Results</figcaption>
</center>

<center>
<img src='{{"assets/images/TRPO/trpo4.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Results</figcaption>
</center>

DQN 보다는 다소 성능이 떨어지지만 대부분 좋은 점수를 받았다.

## Discussion

