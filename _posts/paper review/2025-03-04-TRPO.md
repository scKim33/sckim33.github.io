---
title: "[TRPO] Trust Region Policy Optimization"
last_modified_at: 2025-03-04
categories:
  - paper_review
tags:
  - TRPO
  - Reinforcement Learning
  - On policy
  - Model free
excerpt: "TRPO paper review"
use_math: true
classes: wide
---

> ICML 2015. [[Paper](https://arxiv.org/abs/1502.05477)]  
> John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel  
> 19 Feb 2015

## Summary

Natural policy gradientì˜ ì•„ì´ë””ì–´ë¥¼ ì´ìš©í•˜ì—¬ local approximation of expected returnì˜ monotonic improvementë¥¼ ë³´ì¥í•˜ëŠ” on policy ì•Œê³ ë¦¬ì¦˜ì„ ì œì‹œí•œë‹¤.

## Introduction

Policy optimizationì˜ ë°©ë²•ì—ëŠ” í¬ê²Œ policy iteration, policy gradient, derivative-free optimizationì´ ìˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” surrogate objective functionì„ minimizingí•˜ëŠ” ê²ƒì´ policy improvementë¥¼ ë³´ì¥í•œë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•œë‹¤. ì´ë¥¼ approximationí•˜ì—¬ TRPO ì•Œê³ ë¦¬ì¦˜ì„ ë‘ ê°€ì§€ variantë¡œ ì œì‹œí•œë‹¤.

## Preliminaries

MDP settingì—ì„œ ëª©ì  í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.

$$
\eta(\pi) = \mathbb{E}_{s_0, a_0, \dots} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t) \right], \text{ where } \\
s_0 \sim \rho_0(s_0), \quad a_t \sim \pi(a_t | s_t), \quad s_{t+1} \sim P(s_{t+1} | s_t, a_t).
$$

ê·¸ë¦¬ê³  state distributionì„ ì •ì˜í•˜ë©´

$$
\rho_{\pi}(s) = P(s_0 = s) + \gamma P(s_1 = s) + \gamma^2 P(s_2 = s) + \dots,
$$

ì´ì œ ë‹¤ìŒ ì‹ì´ ì„±ë¦½í•¨ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤. ì´ ì‹ì€ policy update ì „í›„ë¡œ policy improvementê°€ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ í™•ì¸í•˜ëŠ” ìš©ë„ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

$$
\begin{align}
    \eta(\tilde{\pi}) &= \eta(\pi) + \mathbb{E}_{s_0, a_0, \dots \sim \tilde{\pi}} \left[ \sum_{t=0}^{\infty} \gamma^t A_{\pi}(s_t, a_t) \right], \\
    &= \eta(\pi) + \sum_{t=0}^{\infty} \sum_{s} P(s_t = s | \tilde{\pi}) \sum_{a} \tilde{\pi}(a | s) \gamma^t A_{\pi}(s, a) \\
    &= \eta(\pi) + \sum_{s} \sum_{t=0}^{\infty} \gamma^t P(s_t = s | \tilde{\pi}) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a) \\
    &= \eta(\pi) + \sum_{s} \rho_{\pi}(s) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a).
\end{align}
$$

ì‹ì„ ì‚´í´ë³´ë©´ policy improvementê°€ ì´ë£¨ì–´ì§€ëŠ” ì¡°ê±´ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.$\sum_{a} \tilde{\pi}(a \| s) A_{\pi}(s, a)\geq 0$ê°€ ëª¨ë“  $s$ì— ëŒ€í•´ ì„±ë¦½í•œë‹¤ë©´ policy improvementê°€ ì´ë£¨ì–´ì§„ ê²ƒì´ë‹¤. Greedy policyë¥¼ ì·¨í•œë‹¤ë©´ ê° $s$ë§ˆë‹¤ positive advantage functionì„ ê°–ëŠ” $(s,a)$ ìŒì´ í•˜ë‚˜ì”©ë§Œ ì¡´ì¬í•˜ë©´ ëœë‹¤(ì¡´ì¬í•  ìˆ˜ ë°–ì— ì—†ëŠ”ê²Œ, ì–´ë–¤ ìŒì— ëŒ€í•´ advantageê°€ negativeì´ë©´ ë°˜ë“œì‹œ ë‹¤ë¥¸ ìŒì— ëŒ€í•´ì„œëŠ” positiveê°€ ëœë‹¤). ê·¸ëŸ¬ë‚˜ approximation settingì—ì„œëŠ” ì´ëŸ¬í•œ ê°€ì •ì´ ì§€ì¼œì§€ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤. ê²Œë‹¤ê°€ $\tilde{\pi}$ëŠ” ë³€í•˜ê¸° ë•Œë¬¸ì— ì´ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ë³µì¡í•œ dependencyê°€ optimizeí•˜ê¸° ì–´ë µê²Œ ë§Œë“ ë‹¤. ë”°ë¼ì„œ $\pi$ì˜ state distributionì„ ì‚¬ìš©í•˜ëŠ” ë‹¤ìŒì˜ local approximationì„ ê³ ë ¤í•œë‹¤.

$$
L_{\pi}(\tilde{\pi}) = \eta(\pi) + \sum_{s} \rho_{\pi}(s) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a).
$$

ì´ì œ state distributionì€ ê³ ì •ë˜ì—ˆë‹¤. ì—¬ê¸°ì„œ approximationìœ¼ë¡œ \\(\pi_\theta\\)ë¥¼ ì‚¬ìš©í•˜ê³  íŠ¹ì • ê°’ \\(\theta_0\\)ë¥¼ ì‚¬ìš©í•˜ë©´ RHSì˜ ë‘ë²ˆì§¸ í•­ì€ ì‚¬ë¼ì§€ê³ (sum of $A$ = 0), local approximationê³¼ policy gradientê°€ 1ì°¨ ë¯¸ë¶„ì´ ë™ì¼í•œ ê°’ì„ ê°€ì§„ë‹¤.

$$
\begin{align*}
    L_{\pi_{\theta_0}}(\pi_{\theta_0}) &= \eta(\pi_{\theta_0}), \\
    \nabla_{\theta} L_{\pi_{\theta_0}}(\pi_{\theta}) \Big|_{\theta = \theta_0} &= \nabla_{\theta} \eta(\pi_{\theta}) \Big|_{\theta = \theta_0}.
\end{align*}
$$

ìœ„ ì‹ì„ í†µí•´ ì¶©ë¶„íˆ ì‘ì€ ì •ë„ë¡œ policyë¥¼ ì—…ë°ì´íŠ¸ í•˜ë©´ local approximationì˜ gradientë¡œë¶€í„° true objectiveì˜ gradientë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ°ë° ì¶©ë¶„íˆ ì‘ì€ ê°’ì— ëŒ€í•œ ëª…í™•í•œ ê¸°ì¤€ì´ ì—†ë‹¤. ì´ë¥¼ Improvementì˜ lower boundë¥¼ ë³´ì¥í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ conservative policy iterationì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. Conservative policy iterationì„ ì§„í–‰í•˜ê¸° ìœ„í•´ \\(\pi' = \arg\max_{\pi'} L_{\pi_{\text{old}}}(\pi').\\)ë¼ê³  í•˜ì. ë‹¤ìŒê³¼ ê°™ì´ policy ì—…ë°ì´íŠ¸ë¥¼ ì§„í–‰í•˜ëŠ” ê²½ìš° objectiveì— lower boundë¥¼ ë§Œì¡±í•˜ë©° ì—…ë°ì´íŠ¸ëœë‹¤.

$$
\pi_{\text{new}}(a | s) = (1 - \alpha) \pi_{\text{old}}(a | s) + \alpha \pi'(a | s).\\
\eta(\pi_{\text{new}}) \geq L_{\pi_{\text{old}}}(\pi_{\text{new}}) - \frac{2 \epsilon \gamma}{(1 - \gamma)^2} \alpha^2, \\
\text{where} \quad \epsilon = \max_s \left| \mathbb{E}_{a \sim \pi'(a | s)} \left[ A_{\pi}(s, a) \right] \right|.
$$

ì´ lower boundëŠ” ìœ„ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ policy updateê°€ ì´ë£¨ì–´ì ¸ì•¼ë§Œ ë§Œì¡±í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. $\arg\max$ë¥¼ ë§Œì¡±í•˜ëŠ” policyë¥¼ ì°¾ëŠ” ê²ƒì´ ë²ˆê±°ë¡­ê¸° ë•Œë¬¸ì— ì¢€ ë” practicalí•œ ë°©ì‹ì„ ì œì•ˆí•œë‹¤.

## Monotonic Improvement Guarantee for General Stochastic Policies

$\alpha$ ëŒ€ì‹  total variance \\(D_{\text{TV}}(p \parallel q) = \frac{1}{2} \sum_i \|p_i - q_i\|\\)ë¥¼ ì—…ë°ì´íŠ¸ ë˜ëŠ” ì •ë„ë¡œ ì •í•œë‹¤. 

$$
D_{\text{TV}}^{\max} (\pi, \tilde{\pi}) = \max_s D_{\text{TV}} (\pi(\cdot | s) \parallel \tilde{\pi}(\cdot | s)).
$$

> \\\(\textbf{Theorem 1.}\\\)Let \\( \alpha = D_{\text{TV}}^{\max}(\pi_{\text{old}}, \pi_{\text{new}}) \\). Then the following bound holds: \\
> \\[
    \eta(\pi_{\text{new}}) \geq L_{\pi_{\text{old}}}(\pi_{\text{new}}) - \frac{4 \epsilon \gamma}{(1 - \gamma)^2} \alpha^2,
\\]
\\[
    \text{where} \quad \epsilon = \max_{s, a} |A_{\pi}(s, a)|.
\\]

Total variation divergenceì™€ KL divergence ê°„ì˜ ê´€ê³„ \\(D_{\text{TV}}(p \parallel q)^2 = D_{\text{KL}}(p \parallel q)\\)ë¥¼ ì´ìš©í•˜ë©´ Theorem 1.ì„ ë‹¤ìŒê³¼ ê°™ì´ ë³€í˜•í•  ìˆ˜ ìˆë‹¤. \\(D_{\text{KL}}^{\max} (\pi, \tilde{\pi}) = \max_s D_{\text{KL}} (\pi(\cdot \| s) \parallel \tilde{\pi}(\cdot \| s))\\)ë¼ í•˜ë©´

$$
\eta(\tilde{\pi}) \geq L_{\pi}(\tilde{\pi}) - C D_{\text{KL}}^{\max} (\pi, \tilde{\pi}),\\
\text{where} \quad C = \frac{4 \epsilon \gamma}{(1 - \gamma)^2}.
$$

ì´ë¥¼ í†µí•´ policyê°€ monotonically improvingí•œë‹¤ëŠ” ì‚¬ì‹¤ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤. \\( M_i(\pi) = L_{\pi_i}(\pi) - C D_{\text{KL}}^{\max} (\pi_i, \pi) \\)ë¼ í•˜ë©´

$$
\eta(\pi_{i+1}) \geq M_i(\pi_{i+1}) \quad \text{by Equation above}\\
\eta(\pi_i) = M_i(\pi_i), \text{ therefore,}\\  
\eta(\pi_{i+1}) - \eta(\pi_i) \geq M_i(\pi_{i+1}) - M(\pi_i).\\
\text{Finally, }\eta(\pi_0) \leq \eta(\pi_1) \leq \eta(\pi_2) \leq \dots
$$

ì´ë¥¼ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

<center>
<img src='{{"assets/images/TRPO/trpo1.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Policy iteration in TRPO</figcaption>
</center>

## Optimization of Parameterized Policies

ì´ì œ parameterizedëœ policyì— ëŒ€í•´ì„œë„ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ë©´ ë‹¤ìŒì„ ê³„ì‚°í•´ì•¼ í•œë‹¤.

$$
\max_{\theta} \left[ L_{\theta_{\text{old}}}(\theta) - C D_{\text{KL}}^{\max} (\theta_{\text{old}}, \theta) \right].
$$

$C$ì˜ ê²½ìš° ì•ì„œ ë³´ì•˜ë˜ $C$ì˜ ì¡°ê±´ì— ë§ì¶”ì–´ ê°’ì„ ë„£ê²Œ ë˜ë©´ policy update ì •ë„ê°€ ë„ˆë¬´ ì‘ì•„ì§€ëŠ” ê²½ìš°ê°€ ìƒê¸´ë‹¤. ë”°ë¼ì„œ $C$ë¥¼ ì—†ì• ëŠ” ëŒ€ì‹  KL divergence ê°’ì— ì œí•œì„ ë‘ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.

$$
\max_{\theta} L_{\theta_{\text{old}}}(\theta)\\
\text{subject to} \quad D_{\text{KL}}^{\max} (\theta_{\text{old}}, \theta) \leq \delta.
$$

í•œí¸ constraintëŠ” ëª¨ë“  $s$ì— ëŒ€í•´ ê³ ë ¤í•´ì•¼ í•˜ë¯€ë¡œ ë„ˆë¬´ ë§ì€ constraintê°€ ì¡´ì¬í•œë‹¤. ì´ë¥¼ í‰ê·  KL divergenceë¡œ ë°”ê¾¸ë©´ ì¢€ ë” practicalí•œ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.

$$
\overline{D}_{\text{KL}}^{\rho}(\theta_1, \theta_2) := \mathbb{E}_{s \sim \rho} \left[ D_{\text{KL}} (\pi_{\theta_1}(\cdot | s) \parallel \pi_{\theta_2}(\cdot | s)) \right].\\
\max_{\theta} L_{\theta_{\text{old}}}(\theta)\\
\text{subject to} \quad \overline{D}_{\text{KL}}^{\rho_{\theta_{\text{old}}}} (\theta_{\text{old}}, \theta) \leq \delta.
$$

## Sample-Based Estimation of the Objective and Constraint

ìœ„ì—ì„œ ì‚´í´ë³´ì•˜ë˜ ìµœì í™” ë¬¸ì œë¥¼ ë°ì´í„° ìƒ˜í”Œì— ë§ì¶”ì–´ í•™ìŠµí•˜ê¸° ìœ„í•´ ì¶”ê°€ì ì¸ ì‹ì˜ ë³€í˜•ì´ í•„ìš”í•˜ë‹¤. ë°ì´í„° ìƒ˜í”Œì˜ ê²½ìš° ìš°ë¦¬ëŠ” old policyì˜ ë°ì´í„°ë§Œ ê°€ì§€ê³  ì—…ë°ì´íŠ¸ë¥¼ í•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ importance samplingì„ ì´ìš©í•˜ì—¬ objectiveë¥¼ ë³´ì •í•´ ì¤€ë‹¤.

$$
\sum_{a} \pi_{\theta}(a | s_n) A_{\theta_{\text{old}}} (s_n, a) = \mathbb{E}_{a \sim q} \left[ \frac{\pi_{\theta}(a | s_n)}{q(a | s_n)} A_{\theta_{\text{old}}} (s_n, a) \right].
$$

ì´ì œ ìµœì í™” ë¬¸ì œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë³€í˜•ëœë‹¤.

$$
\max_{\theta} \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}, a \sim q} \left[ \frac{\pi_{\theta}(a | s)}{q(a | s)} Q_{\theta_{\text{old}}} (s, a) \right]\\
\text{subject to} \quad \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}} \left[ D_{\text{KL}} (\pi_{\theta_{\text{old}}}(\cdot | s) \parallel \pi_{\theta}(\cdot | s)) \right] \leq \delta.
$$

<center>
<img src='{{"assets/images/TRPO/trpo2.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Single path vs Vine</figcaption>
</center>

ì´ì œ estimationì„ ìˆ˜í–‰í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì¸ single path, vine(ë‚˜ë­‡ê°€ì§€ë¼ëŠ” ëœ»)ì— ëŒ€í•´ ì†Œê°œí•œë‹¤. TRPOì˜ ê²½ìš° KL divergence ì œì•½ì„ vineì„ ì´ìš©í•´ì„œ ìƒ˜í”Œë§ ê³¼ì •ì— ì ìš©í•œë‹¤.
- Single path: ê¸°ì¡´ê³¼ ê°™ì´ \\(\pi_{\theta_{\text{old}}}(a\|s)\\)ë¥¼ ì´ìš©í•˜ì—¬ ì „ì²´ episodeì— ëŒ€í•œ trajectoryë¥¼ ìƒì„±í•˜ê³  ì´ë¥¼ ì´ìš©í•˜ì—¬ policy updateí•œë‹¤.
- Vine: ì „ì²´ episodeì— ëŒ€í•œ trajectory ëŒ€ì‹  $N$ stepê¹Œì§€ë§Œ \\(\pi_{\theta_{i}}(a\|s)\\)ë¥¼ ì´ìš©í•´ì„œ ë½‘ëŠ”ë‹¤. ì´ë¥¼ rollout setì´ë¼ í•œë‹¤. Rollout setì˜ ê° \\(s_n\\)ì—ì„œ \\(\pi_{\theta_{i}}(a\|s)\\)ì´ $K$ê°œì˜ actionì„ ìƒ˜í”Œë§í•œë‹¤. ìƒì„±ëœ $K$ê°œì˜ $(s,a)$ pairì—ì„œ ì‹œì‘í•˜ëŠ” rolloutì„ í†µí•´ \\(\hat{Q}\_{\theta_i}(s_n,a_{n,k})\\)ë¥¼ ì¶”ì •í•˜ì—¬ objective functionì„ estimateí•œë‹¤. ì´ë¥¼ ì´ìš©í•˜ì—¬ ì´ë¥¼ í†µí•´ varianceë¥¼ ê°ì†Œì‹œì¼œ better estimateí•˜ëŠ” íš¨ê³¼ê°€ ìˆë‹¤. ë‹¤ë§Œ ì—°ì‚°ëŸ‰ì˜ ì¦ê°€ ë° íŠ¹ì • stateë¡œì˜ ì´ˆê¸°í™”ê°€ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µë  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ í•„ìš”ë¡œ í•œë‹¤.

\\[
L_n(\theta) = \frac{\sum_{k=1}^{K} \frac{\pi_{\theta} (a_{n,k} | s_n)}{\pi_{\theta_{\text{old}}} (a_{n,k} | s_n)} Q(s_n, a_{n,k})}
{\sum_{k=1}^{K} \frac{\pi_{\theta} (a_{n,k} | s_n)}{\pi_{\theta_{\text{old}}} (a_{n,k} | s_n)}}.
\\]


## Practical algorithm

1. Single path ë˜ëŠ” vine ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œë‹¤.
2. State, actionì— ëŒ€í•´ averageí•˜ì—¬ objective functionì„ ê³„ì‚°í•œë‹¤.
3. Constrained optimization problemì„ approximateí•˜ì—¬ gradientë¥¼ êµ¬í•œë‹¤. Conjugate gradient algorithm ì´í›„ line searchë¥¼ ìˆ˜í–‰í•˜ì—¬ êµ¬í•  ìˆ˜ ìˆë‹¤.

3ì˜ ê³¼ì •ì—ì„œ KL divergenceì˜ quadratic approximationì„ Fisher matrixë¼ ì •ì˜í•œë‹¤.

$$
Ax=g\\
D_{\text{KL}} (\theta_{\text{old}}, \theta) \approx \frac{1}{2} (\theta - \theta_{\text{old}})^T A (\theta - \theta_{\text{old}}).\\
A_{ij} = \frac{\partial}{\partial \theta_i} \frac{\partial}{\partial \theta_j} \overline{D}_{\text{KL}}(\theta_{\text{old}}, \theta).
$$

Large scale problemì—ì„œëŠ” Fisher matrixì˜ inverseë¥¼ ì§ì ‘ ê³„ì‚°í•˜ê¸° í˜ë“¤ë‹¤. ë”°ë¼ì„œ Fisher-vector productë¥¼ ì´ìš©í•˜ì—¬ ì—°ì‚°ëŸ‰ì„ ì¤„ì¸ë‹¤. êµ¬ì²´ì ì¸ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. KL divergenceë¥¼ ë‘ë²ˆ ë¯¸ë¶„í•œë‹¤.

$$
D_{\text{KL}} (\pi_{\theta_{\text{old}}} (\cdot | x) \parallel \pi_{\theta} (\cdot | x)) = \text{kl}(\mu_{\theta}(x), \mu_{\text{old}})\\
\frac{\partial \mu_a (x)}{\partial \theta_i} \frac{\partial \mu_b (x)}{\partial \theta_j} \text{kl}''_{ab} (\mu_{\theta}(x), \mu_{\text{old}})
+ \frac{\partial^2 \mu_a (x)}{\partial \theta_i \partial \theta_j} \text{kl}'_a (\mu_{\theta}(x), \mu_{\text{old}})
$$

ìì½”ë¹„ì•ˆ \\(J := \frac{\partial \mu_a (x)}{\partial \theta_i}\\)ì„ ì´ìš©í•˜ì—¬ KL divergenceë¥¼ ë¶„í•´í•  ìˆ˜ ìˆë‹¤.

$$
F=J^T M J, \quad \text{where} \quad M = \text{kl}''_{ab} (\mu_{\theta}(x), \mu_{\text{old}})\\
\text{Now, Fisher-vector product is }y \to J^T M J y.
$$

ì´ì œ conjugate gradientë¥¼ ì´ìš©í•˜ì—¬ ì—­í–‰ë ¬ ê³„ì‚° ëŒ€ì‹  iteration ë°©ë²•ìœ¼ë¡œ numerical optimizationì„ ì§„í–‰í•œë‹¤.

<details>
<summary>Conjugate gradient methodì— ëŒ€í•´</summary>
<div markdown="1">
## **ğŸ“Œ Conjugate Gradient (CG) ìƒì„¸ ì„¤ëª…**
Conjugate Gradient(CG) ë°©ë²•ì€ **í° ê·œëª¨ì˜ ì„ í˜• ì‹œìŠ¤í…œ \\(Ax = b\\)ë¥¼ í’€ê¸° ìœ„í•œ íš¨ìœ¨ì ì¸ ìµœì í™” ì•Œê³ ë¦¬ì¦˜**ì´ì•¼.  
íŠ¹íˆ, **ê³ ì°¨ì› Fisher Information Matrixì˜ ì—­í–‰ë ¬ì„ ì§ì ‘ êµ¬í•˜ëŠ” ëŒ€ì‹  ê·¼ì‚¬ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ë° ìœ ìš©**í•´.

---

## **1. Conjugate Gradientê°€ í•„ìš”í•œ ì´ìœ **
TRPO ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì—ì„œ **ìì—° ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸(Natural Gradient) ì—…ë°ì´íŠ¸**ë¥¼ ìˆ˜í–‰í•˜ë ¤ë©´ ì•„ë˜ ê³µì‹ì„ í’€ì–´ì•¼ í•´.

\\(
F^{-1} g
\\)

ì—¬ê¸°ì„œ:
- \\( F \\)= **Fisher Information Matrix (í”¼ì…” ì •ë³´ í–‰ë ¬)**
- \\( g \\)= **ì¼ë°˜ì ì¸ ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸** \\( \nabla_{\theta} L(\theta) \\)

í•˜ì§€ë§Œ **Fì˜ í¬ê¸°ê°€ ë„ˆë¬´ í¬ë©´ ì§ì ‘ ì—­í–‰ë ¬ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì€ í˜„ì‹¤ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥**í•´.
- ì¼ë°˜ì ì¸ **í–‰ë ¬ ì—­í–‰ë ¬ ê³„ì‚° ë¹„ìš©ì€ \\(O(n^3)\\)** (ë„ˆë¬´ ë¹„ì‹¸!)
- \\( F \\)ì˜ í¬ê¸°ê°€ ìˆ˜ë§Œ ê°œ ì´ìƒì˜ íŒŒë¼ë¯¸í„°ì¼ ê²½ìš°, **ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œë„ ë°œìƒí•  ìˆ˜ ìˆìŒ.**

ğŸ’¡ **í•´ê²°ì±…?**  
**Conjugate Gradient(CG) ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ \\( F^{-1} g \\)ë¥¼ ì§ì ‘ êµ¬í•˜ì§€ ì•Šê³ ë„ ì„ í˜• ì‹œìŠ¤í…œì„ ê·¼ì‚¬ì ìœ¼ë¡œ í’€ ìˆ˜ ìˆìŒ!**

---

## **2. Conjugate Gradient ì•Œê³ ë¦¬ì¦˜ì´ë€?**
Conjugate GradientëŠ” **í¬ê³  í¬ì†Œí•œ(Sparse) ëŒ€ì¹­ ì–‘ì˜ ì •ë¶€í˜¸(SPD) í–‰ë ¬ \\( A \\)ì— ëŒ€í•œ ì„ í˜• ë°©ì •ì‹**:

\\(
Ax = b
\\)

ì„ í‘¸ëŠ” ë°©ë²•ì´ì•¼.

### **âœ… ì¼ë°˜ì ì¸ ë°©ë²•ê³¼ì˜ ë¹„êµ**
- **ê³ ì „ì ì¸ ë°©ë²• (ì§ì ‘ ê³„ì‚°)**
  - í–‰ë ¬ \\( A \\)ì˜ ì—­í–‰ë ¬ì„ ì§ì ‘ êµ¬í•´ì„œ \\( x = A^{-1} b \\)ë¥¼ ê³„ì‚°.
  - ê³„ì‚° ë¹„ìš©ì´ \\( O(n^3) \\)ì´ë¼ í˜„ì‹¤ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥.

- **CG ë°©ë²• (ê·¼ì‚¬ì ìœ¼ë¡œ í‘¸ëŠ” ë°©ë²•)**
  - \\( A \\)ì˜ ì—­í–‰ë ¬ì„ êµ¬í•˜ì§€ ì•Šê³  \\( x \\)ë¥¼ ì ì§„ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” ë°©ì‹.
  - **í–‰ë ¬-ë²¡í„° ê³± \\( Av \\)ë§Œ í•„ìš”**í•˜ë¯€ë¡œ ê³„ì‚°ëŸ‰ì´ ëŒ€í­ ê°ì†Œ.
  - ì¼ë°˜ì ìœ¼ë¡œ **\\( O(n) \\)~\\( O(n^2) \\)ë¡œ í•´ê²° ê°€ëŠ¥**.

### **âœ… í•µì‹¬ ì•„ì´ë””ì–´**
1. **ìµœì ì˜ ë°©í–¥ì„ ì°¾ì•„ ë°˜ë³µì ìœ¼ë¡œ í•´ë¥¼ ê°œì„ **í•˜ëŠ” ë°©ì‹.
2. ë‹¨ìˆœí•œ ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ê²Œ ìˆ˜ë ´.
3. **ì—­í–‰ë ¬ ì—†ì´ ì„ í˜• ë°©ì •ì‹ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•´ê²° ê°€ëŠ¥**.

---

## **3. Conjugate Gradient ì•Œê³ ë¦¬ì¦˜ ì‘ë™ ë°©ì‹**
ëª©í‘œëŠ” \\( Ax = b \\)ë¥¼ í‘¸ëŠ” ê²ƒì´ì•¼.

### **âœ… ê¸°ë³¸ì ì¸ ê²½ì‚¬ í•˜ê°•ë²• (Gradient Descent)**
ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²•ì€ **ê²½ì‚¬ í•˜ê°•ë²•**ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì¸ë°:

\\(
x_{k+1} = x_k + \alpha \cdot r_k
\\)

ì—¬ê¸°ì„œ:
- \\( r_k = b - Ax_k \\)ëŠ” **í˜„ì¬ ì˜¤ì°¨(residual)**
- \\( \alpha \\)ëŠ” **ìŠ¤í… í¬ê¸°**

í•˜ì§€ë§Œ, ê²½ì‚¬ í•˜ê°•ë²•ì€ **ëª¨ë“  ë°©í–¥ì—ì„œ ë™ì¼í•œ ë¹„ìœ¨ë¡œ ì´ë™**í•˜ê¸° ë•Œë¬¸ì— ìˆ˜ë ´ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆì–´.

### **âœ… CGì˜ ê°œì„ ì : ì§êµ(conjugate) ë°©í–¥ì„ í™œìš©**
Conjugate GradientëŠ” **ê¸°ì¡´ì˜ ê²½ì‚¬ ë°©í–¥ê³¼ ë…ë¦½ì ì¸(=ì§êµí•˜ëŠ”) ë°©í–¥ì„ ì‚¬ìš©í•˜ì—¬ ìµœì í™”**í•˜ëŠ” ë°©ì‹ì´ì•¼.

ğŸ’¡ **CGëŠ” ê¸°ì¡´ Gradient Descentë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ëŠ” ì´ìœ ?**
- **ìƒˆë¡œìš´ ë°©í–¥ì„ ì„ íƒí•  ë•Œ, ì´ì „ ë°©í–¥ê³¼ ì§êµ(Conjugate)í•˜ë„ë¡ ë³´ì¥í•¨**.
- ì´ë ‡ê²Œ í•˜ë©´ **í•œ ë²ˆ ì—…ë°ì´íŠ¸í•œ ë°©í–¥ìœ¼ë¡œ ë‹¤ì‹œ ëŒì•„ì˜¬ í•„ìš”ê°€ ì—†ê¸° ë•Œë¬¸ì—, ìµœì†Œí•œì˜ ë°˜ë³µìœ¼ë¡œ ìˆ˜ë ´ ê°€ëŠ¥**.

---

## **4. Conjugate Gradient ì•Œê³ ë¦¬ì¦˜ ë‹¨ê³„ë³„ ê³¼ì •**
### **ğŸ”¹ Step 1: ì´ˆê¸°í™”**
- ì´ˆê¸°ê°’ \\( x_0 \\)ì„ ì„ì˜ë¡œ ì„¤ì •.
- ì´ˆê¸° ì˜¤ì°¨(residual): \\( r_0 = b - Ax_0 \\).
- ì²« ë²ˆì§¸ íƒìƒ‰ ë°©í–¥ì„ **ì˜¤ì°¨ ë°©í–¥ìœ¼ë¡œ ì„¤ì •**: \\( p_0 = r_0 \\).

### **ğŸ”¹ Step 2: ë°˜ë³µ ì—…ë°ì´íŠ¸**
ë°˜ë³µë¬¸ì„ ìˆ˜í–‰í•˜ë©´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì—…ë°ì´íŠ¸:

1. **ìŠ¤í… í¬ê¸°(learning rate) ê³„ì‚°**:

   \\(
   \alpha_k = \frac{r_k^T r_k}{p_k^T A p_k}
   \\)

2. **ìƒˆë¡œìš´ í•´ ì—…ë°ì´íŠ¸**:

   \\(
   x_{k+1} = x_k + \alpha_k p_k
   \\)

3. **ìƒˆë¡œìš´ ì˜¤ì°¨ ê³„ì‚°**:

   \\(
   r_{k+1} = r_k - \alpha_k A p_k
   \\)

4. **ìƒˆë¡œìš´ ë°©í–¥ ë²¡í„° ì„¤ì •**:

   \\(
   \beta_k = \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}
   \\)

   \\(
   p_{k+1} = r_{k+1} + \beta_k p_k
   \\)

5. ìˆ˜ë ´ ì¡°ê±´ì´ ë§Œì¡±ë  ë•Œê¹Œì§€ ë°˜ë³µ.

---

## **5. TRPOì—ì„œ CGë¥¼ ì–´ë–»ê²Œ í™œìš©í•˜ëŠ”ê°€?**
TRPOì—ì„œëŠ” **ìì—° ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸**ë¥¼ ê³„ì‚°í•  ë•Œ ë‹¤ìŒ ë¬¸ì œë¥¼ í’€ì–´ì•¼ í•´:

\\(
F v = g
\\)

ì—¬ê¸°ì„œ:
- \\( F \\)= **Fisher Information Matrix**
- \\( g \\)= **ì¼ë°˜ì ì¸ ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸**
- \\( v = F^{-1} g \\)ë¥¼ ì°¾ì•„ì•¼ í•¨ â†’ **CGë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¼ì‚¬ì ìœ¼ë¡œ í•´ê²°!**

ğŸ’¡ **CGì˜ ì¥ì **
- **Fì˜ ì—­í–‰ë ¬ì„ ì§ì ‘ êµ¬í•˜ì§€ ì•Šê³ ë„ ìì—° ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚° ê°€ëŠ¥!**
- **\\( F v \\)ì—°ì‚°ë§Œ í•„ìš”í•˜ë¯€ë¡œ ê³„ì‚°ëŸ‰ì´ ëŒ€í­ ê°ì†Œ!**
- **ê³ ì°¨ì›ì—ì„œë„ íš¨ìœ¨ì ì¸ í•™ìŠµ ê°€ëŠ¥!**

### **TRPOì—ì„œ CG ìµœì í™” ê¸°ë²•**
- **CG ë°˜ë³µ íšŸìˆ˜ \\( k \\)ë¥¼ 10ìœ¼ë¡œ ì„¤ì •** (ë„ˆë¬´ ë§ì´ ë°˜ë³µí•˜ë©´ ë¹„íš¨ìœ¨ì )
- **ë°ì´í„° ì¼ë¶€ë§Œ ìƒ˜í”Œë§í•˜ì—¬ Fisher-Vector Product ê³„ì‚°** (ì „ì²´ ë°ì´í„° ì‚¬ìš© X)
- **ìµœì¢…ì ìœ¼ë¡œ CGë¥¼ ì‚¬ìš©í•˜ë©´ ì¼ë°˜ì ì¸ ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ì™€ ë¹„ìŠ·í•œ ê³„ì‚°ëŸ‰ìœ¼ë¡œ ìì—° ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ!**

---

## **6. CG vs. ë‹¤ë¥¸ ë°©ë²• ë¹„êµ**

| ë°©ë²• | íŠ¹ì§• | ê³„ì‚° ë¹„ìš© |
|------|------|------|
| **ì§ì ‘ ì—­í–‰ë ¬ ê³„ì‚°** | \\( F^{-1} g \\)ì§ì ‘ ê³„ì‚° | \\( O(n^3) \\) (ë¹„íš¨ìœ¨ì ) |
| **Gradient Descent** | ì¼ë°˜ì ì¸ ê²½ì‚¬ í•˜ê°•ë²• | \\( O(n) \\), í•˜ì§€ë§Œ ìˆ˜ë ´ì´ ëŠë¦¼ |
| **Conjugate Gradient (CG)** | ê°€ì¥ ë¹ ë¥¸ ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸, ë¹ ë¥¸ ìˆ˜ë ´ | \\( O(n) \\)~\\( O(n^2) \\), ê°€ì¥ íš¨ìœ¨ì  |

---

## **7. ê²°ë¡ **
### **ğŸ“Œ Conjugate Gradientì˜ í•µì‹¬**
- **í° ê·œëª¨ì˜ ì„ í˜• ì‹œìŠ¤í…œ \\( Ax = b \\)ë¥¼ í‘¸ëŠ” íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜.**
- **ìì—° ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì—ì„œ \\( F^{-1} g \\)ë¥¼ ì§ì ‘ êµ¬í•˜ì§€ ì•Šê³  ê·¼ì‚¬ì ìœ¼ë¡œ í•´ê²°.**
- **ì—­í–‰ë ¬ì„ ì§ì ‘ ê³„ì‚°í•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ê³  íš¨ìœ¨ì .**
- **TRPOì—ì„œëŠ” CGë¥¼ í™œìš©í•˜ì—¬ Fisher-Vector Productë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê³„ì‚°í•¨.**
- **ë°ì´í„° ì„œë¸Œìƒ˜í”Œë§ê³¼ ê²°í•©í•˜ë©´ ê³„ì‚° ë¹„ìš©ì´ ì¼ë°˜ì ì¸ ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ì™€ ë¹„ìŠ·í•´ì§.**

---

ì´í•´ ì•ˆ ë˜ëŠ” ë¶€ë¶„ ìˆìœ¼ë©´ ì§ˆë¬¸í•´ì¤˜! ğŸš€
</div>
</details>
\
Iterationì„ $k=10$ ì •ë„ë¡œ ì„¤ì •í•˜ë©´ ì¶©ë¶„íˆ ì¢‹ì€ ì„±ëŠ¥ì€ ë³´ì¸ë‹¤ê³  í•œë‹¤. Conjugate gradient ì—†ì´ëŠ” 90%ì˜ computing ìì›ì„ ì—­í–‰ë ¬ ì—°ì‚°ì— ì†Œë¹„í•´ì•¼ í•˜ëŠ”ë° ì´ë¥¼ í•´ê²°í•  ìˆ˜ ìˆì—ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ë°ì´í„° ì „ì²´ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ 10% ê°€ëŸ‰ì„ ì‚¬ìš©í•˜ì—¬ Fisher matrixë¥¼ ê³„ì‚°í•˜ëŠ” ì—°ì‚° ë¹„ìš©ì„ ì¤„ì¸ë‹¤.

ì´ì œ gradientì˜ ë°©í–¥$s\approx A^-1g$ì€ êµ¬í•˜ì˜€ê³  ì–¼ë§Œí¼ ì—…ë°ì´íŠ¸ í• ì§€ ì •í•˜ë©´ ëœë‹¤. Maximal step length $\beta$ì— ëŒ€í•´ $\theta+\beta s$ëŠ” KL divergence constraintë¥¼ ë§Œì¡±í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ \\(\delta = \overline{D}_{\text{KL}} \approx \frac{1}{2} (\beta s)^T A (\beta s) = \frac{1}{2} \beta^2 s^T A s\\)ë¥¼ ë§Œì¡±í•˜ê²Œ ë˜ê³  \\(\beta = \sqrt{2\delta / s^T A s}\\)ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ ì ì ˆí•œ ê°’ì„ line searchë¥¼ í†µí•´ ì°¾ëŠ”ë‹¤.

Analyticí•œ ë¶€ë¶„ê³¼ practicalí•œ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ê³¼ì •ì„ ìš”ì•½í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
- KL divergence penaltyë¥¼ ë‘ì–´ surrogate objectiveë¥¼ ìµœì í™”í•œë‹¤.
- ê·¸ëŸ°ë° penalty coefficient $C$ë¥¼ ë„ˆë¬´ í¬ê²Œ ì¡ìœ¼ë©´ ì—…ë°ì´íŠ¸ê°€ ë³„ë¡œ ì´ë£¨ì–´ì§€ì§€ ì•ŠëŠ”ë‹¤.
- $C$ë¥¼ ê³ ë¥¸ë‹¤ëŠ” ê²ƒì€ ê²½í—˜ì ìœ¼ë¡œ ì–´ë µê¸° ë•Œë¬¸ì— KL divergenceì— boundë¥¼ ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ëŒ€ì‹ í•œë‹¤.
- ë˜í•œ KL divergenceì˜ maximizationì„ êµ¬í•˜ëŠ” ê²ƒì€ ì—°ì‚°ëŸ‰ ì¸¡ë©´ì—ì„œ ì¢‹ì§€ ì•Šê¸° ë•Œë¬¸ì—, KL divergenceì˜ í‰ê· ì„ ëŒ€ì‹  ì‚¬ìš©í•˜ì—¬ ì‹¤ì œë¡œëŠ” ìƒ˜í”Œë§ìœ¼ë¡œ KL divergenceì˜ í‰ê· ì„ ê·¼ì‚¬í•˜ê²Œ ëœë‹¤.
- Advantage functionì˜ estimation errorì— ëŒ€í•´ì„œëŠ” simplicityë¥¼ ìœ„í•´ ë¬´ì‹œí•œë‹¤.

## Connections with Prior Work

ì´ ì—°êµ¬ì™€ ìœ ì‚¬í•œ ê°œë…ìœ¼ë¡œ natural policy gradientê°€ ìˆë‹¤. Objective functionì— ëŒ€í•´ 1ì°¨ ê·¼ì‚¬ì‹ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , KL divergenceì— ëŒ€í•´ 2ì°¨ ê·¼ì‚¬ì‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. 

$$
\max_{\theta} L_{\theta_{\text{old}}}(\theta)\\
\text{subject to} \quad \overline{D}_{\text{KL}}^{\rho_{\theta_{\text{old}}}} (\theta_{\text{old}}, \theta) \leq \delta.
$$

ê·¸ëŸ¬ë©´ ë‹¤ìŒê³¼ ê°™ì´ optimization problemì´ ë°”ë€ë‹¤.

$$
\max_{\theta} \left[ \nabla_{\theta} L_{\theta_{\text{old}}} (\theta) \big|_{\theta = \theta_{\text{old}}} \cdot (\theta - \theta_{\text{old}}) \right]\\
\text{subject to} \quad \frac{1}{2} (\theta_{\text{old}} - \theta)^T A(\theta_{\text{old}}) (\theta_{\text{old}} - \theta) \leq \delta,\\
\text{where} \quad A(\theta_{\text{old}})_{ij} = \frac{\partial}{\partial \theta_i \partial \theta_j} \mathbb{E}_{s \sim \rho_{\pi}} \left[ D_{\text{KL}} (\pi(\cdot | s, \theta_{\text{old}}) \parallel \pi(\cdot | s, \theta)) \right] \big|_{\theta = \theta_{\text{old}}}.\\
$$

ê·¸ëŸ¬ë©´ ë‹¤ìŒê³¼ ê°™ì´ policy parameterê°€ ì—…ë°ì´íŠ¸ëœë‹¤.

$$
\theta_{\text{new}} = \theta_{\text{old}} + \frac{1}{\lambda} A(\theta_{\text{old}})^{-1} \nabla_{\theta} L(\theta) \big|_{\theta = \theta_{\text{old}}}.
$$

ì—¬ê¸°ì„œëŠ” ê³ ì •ëœ step sizeë¡œ ì—…ë°ì´íŠ¸ê°€ ì´ë£¨ì–´ì§€ëŠ”ë°, TRPOì—ì„œëŠ” KL divergenceì˜ ì œì•½ì¡°ê±´ì— ë”°ë¼ step sizeê°€ ë‹¬ë¼ì§„ë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ ì°¨ì´ê°€ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ì´ëŒì—ˆë‹¤ê³  ë§í•˜ê³  ìˆë‹¤.

## Experiments

<center>
<img src='{{"assets/images/TRPO/trpo3.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Results</figcaption>
</center>

<center>
<img src='{{"assets/images/TRPO/trpo4.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Results</figcaption>
</center>

DQN ë³´ë‹¤ëŠ” ë‹¤ì†Œ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ì§€ë§Œ ëŒ€ë¶€ë¶„ ì¢‹ì€ ì ìˆ˜ë¥¼ ë°›ì•˜ë‹¤.

## Discussion

