---
title: "[A3C] Asynchronous Methods for Deep Reinforcement Learning"
last_modified_at: 2025-02-26
categories:
  - paper_review
tags:
  - A3C
  - Policy gradient
  - Reinforcement Learning
excerpt: "A3C paper review"
use_math: true
classes: wide
---

> International conference on machine learning, 2016. [[Paper](https://arxiv.org/abs/1602.01783)]
> Volodymyr Mnih, Adrià P
> uigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu  
> 4 Feb 2016

## Summary

4종류의 RL 알고리즘을 asynchronous 방식으로 바꾸어 학습한 결과 안정적으로 학습한다. 그 중 actor-critic variant(A3C)가 가장 좋은 성능을 보이는데, single multi-core CPU로 학습함에도 Atari domain에서 가장 좋은 성능을 보인다.

## Introduction

Deep neural network와 online RL을 결합하면 학습이 불안정해진다고 여겨진다. 이를 위해 replay buffer 등의 방식이 사용된다. 하지만 replay buffer의 경우 transition tuple을 추가적으로 담기 위한 메모리를 요구하고 off-policy 방식으로밖에 구현하지 못해 오래된 policy로 학습하게 된다.

비동기적 학습은 병렬적으로 agent를 구성하여 데이터가 correlation 되는 것을 막고, on/off policy 알고리즘 모두에 적용할 수 있다. 멀티코어 CPU에서도 학습이 가능하다. 비동기적 학습 알고리즘 중 actor-critic 구조의 A3C는 가장 좋은 성능을 보인다.


## Related Work

병렬학습을 하는 다른 연구로 Gorila가 있다. 100대의 actor-learner processes와 30대의 parameter server로 학습한다. 학습 속도를 높였지만 이는 현실적인 제한이 많다. 유전 알고리즘을 활용하여 병렬 학습을 한 연구도 있다.


## Reinforcement Learning Background

## Asynchronous RL Framework

One-step Sarsa, one-step Q-learning, n-step Q-learning, advantage actor-critics에 대해 asynchronous variants를 확인한다. 개별 machine을 두기보다는 여러 threads를 이용하여 각각의 agents를 구성한다. 그렇게 함으로써 각각의 agent들이 environments의 서로 다른 부분을 동시에 탐색하는 효과를 준다. Q-learning을 예시로 들었을때, 알고리즘은 다음과 같다.

<center>
<img src='{{"assets/images/A3C/a3c5.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">one-step Q-learning asynchronous algorithm</figcaption>
</center>


병렬 구조를 이용하는 것은 training time을 감소시키는 효과와 experience replay를 사용하지 않음으로써 on-policy RL을 안정적으로 학습할 수 있는 효과를 준다.

Asynchronous update를 위해 global count $T$와 thread count $t$를 별개로 둔다. 누적된 Gradient는 target policy/thread policy의 업데이트 주기에 따라 업데이트하는데 이용된다. One-step Sarsa의 경우에는 $r+\gamma Q(s',a';\theta^-)$를 이용한다. n-step Q learning은 $\sum\_{k=0}^{n-1} \gamma^k r\_{t+k} + \gamma^n \max\_{a'} Q(s\_{t+n}, a')$를 이용한다. A3C의 경우 critic update $A(s\_t, a\_t; \theta, \theta\_v) = \sum\_{i=0}^{k-1} \gamma^i r\_{t+i} + \gamma^k V(s\_{t+k}; \theta\_v) - V(s\_t; \theta\_v)$와 actor update $\nabla\_{\theta} \log \pi (a\_t \| s\_t; \theta') A(s\_t, a\_t; \theta, \theta\_v)$가 이루어진다.

<center>
<img src='{{"assets/images/A3C/a3c6.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">n-step Q-learning asynchronous algorithm</figcaption>
</center>

<center>
<img src='{{"assets/images/A3C/a3c7.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">advantage actor-critic asynchronous algorithm</figcaption>
</center>

Policy를 위한 네트워크는 CNN + softmax output, value function을 위한 네트워크는 CNN + one linear output이다.

Optimization의 경우 RMSProp을 사용한다.

## Experiments

<center>
<img src='{{"assets/images/A3C/a3c1.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Experimental results</figcaption>
</center>

<center>
<img src='{{"assets/images/A3C/a3c2.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Training methods</figcaption>
</center>

<center>
<img src='{{"assets/images/A3C/a3c3.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Scalability</figcaption>
</center>

Scalability를 확인하기 위해 특정 score에 도달하기까지 걸린 시간을 토대로 training speed를 계산하였다. Thread 수 이상의 speedup 배율을 보인 알고리즘들도 있다.

<center>
<img src='{{"assets/images/A3C/a3c4.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Efficiency</figcaption>
</center>

병렬 Thread 학습의 경우 많은 threads를 사용할 수록 같은 시간 내에 학습하는 프레임의 수는 많을 것이다. 따라서 학습한 프레임 수에 따른 score를 확인한 결과 Q learning에서는 확실히 성능 개선을 볼 수 있고, A3C도 약간의 개선이 이루어지는 것으로 보인다.

Training hours에 대한 score 비교는 다음과 같다.

## Conclusions and Discussion