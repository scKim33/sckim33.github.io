---
title: "[TD3] Addressing Function Approximation Error in Actor-Critic Methods"
last_modified_at: 2025-03-02
categories:
  - paper_review
tags:
  - TD3
  - Reinforcement Learning
  - Off policy
  - Model free
  - Q Overestimation
excerpt: "TD3 paper review"
use_math: true
classes: wide
---

> PMLR 2018. [[Paper](https://arxiv.org/abs/1802.09477)]  
> Scott Fujimoto, Herke van Hoof, David Meger  
> 26 Feb 2018

## Summary

Q value estimation을 통해 Q learning을 하는 경우 overestimation 문제가 생긴다. Actor critic setting에서도 마찬가지로 발생한다. TD3는 이를 해결하기 위해 double q learning, delayed policy updates를 수행한다.

## Introduction

Q learning은 approximation으로 인해 특정 행동이 과대평가된다. 또한 Q learning의 경우 TD error를 통해 학습하는 bootstrapping 방식으로, 작은 오차라 하더라도 점점 누적되어 커지게 된다. 그 결과 suboptimal한 policy를 학습하게 될 수 있다.

이를 해결하기 위해 Double DQN 등을 사용하게 된다. 하지만 actor critic의 경우 policy가 서서히 변하기 때문에 여전히 과대평가 문제가 발생한다. TD3에서는 이를 Double Q learning과 다른 기법들을 통해 해결한다.

## Related Work

함수 근사에 대한 오류와 bias, variance에 대한 연구는 Double Q learning 외에도 variance를 감소(Early high-variance를 방지, 보정항을 적용), error accumulation을 방지(Importance sampling, multi-step returns)하는 방식으로 연구되었다.

## Background

DDPG에서 사용되는 deterministic policy gradient는 다음과 같다.

$$
\nabla_{\phi} J(\phi) = \mathbb{E}_{s \sim p_{\pi}} \left[ \nabla_{a} Q^{\pi} (s, a) \big|_{a = \pi(s)} \nabla_{\phi} \pi_{\phi} (s) \right]
$$

## Overestimation Bias
### Overestimation Bias in Actor-Critic
Target value를 추정할 때 발생하는 오차 $\epsilon$는 zero-mean이라 하더라도 overestimation을 유발한다.

$$
\mathbb{E} \left[ \max_{a} \left( Q(s', a) + \epsilon \right) \right] \geq \max_{a} Q(s', a)
$$

Discrete action setting에서 overestimation이 발생한다는 사실은 analytical하게 명확하다. 아울러 이 현상은 continuous space인 DDPG에서도 practical하게 확인할 수 있다. 논문에서는 DDPG setting에서도 overestimation이 발생함을 analytical하게 증명한다.

먼저 Policy가 DPG를 이용하여 업데이트 된다고 가정한다. DDPG의 policy gradient term에서는 Q funtion을 신경망으로 근사하여 사용한다. 이를 $Q\_\theta$라고 하고 true Q function w.r.t. $\pi\_\phi$를 $Q^\pi$라 하자. 원래는 $\pi\_\phi$를 $Q\_\theta$를 이용해서 업데이트 하게 된다. 하지만 비교를 위해 true Q function을 이용하여 업데이트 하는 경우도 생각해 보자.

$$
\begin{align*}
  \phi_{\text{approx}} &= \phi + \frac{\alpha}{Z_1} \mathbb{E}_{s \sim p_{\pi}} \left[ \nabla_{\phi} \pi_{\phi} (s) \nabla_{a} Q_{\theta} (s, a) \big|_{a = \pi_{\phi} (s)} \right]\\

  \phi_{\text{true}} &= \phi + \frac{\alpha}{Z_2} \mathbb{E}_{s \sim p_{\pi}} \left[ \nabla_{\phi} \pi_{\phi} (s) \nabla_{a} Q^{\pi} (s, a) \big|_{a = \pi_{\phi} (s)} \right],  
\end{align*}
$$

$Z$는 normalization term이다. 각각의 방식을 통해 업데이트 된 policy를 $\phi\_{\text{approx}},\phi_{\text{true}}$라고 하자. 그리고 gradient의 방향은 locally maximize하는 방향이므로 충분히 작은 $\alpha$에 대해 다음을 만족한다.

$$
\exists \, \alpha \text{ such that, } \alpha\leq\epsilon_1,\quad
\mathbb{E} \left[ Q_{\theta} (s, \pi_{\text{approx}} (s)) \right] \geq \mathbb{E} \left[ Q_{\theta} (s, \pi_{\text{true}} (s)) \right]\\
\exists \, \alpha \text{ such that, } \alpha\leq\epsilon_2,\quad
\mathbb{E} \left[ Q^{\pi} (s, \pi_{\text{true}} (s)) \right] \geq \mathbb{E} \left[ Q^{\pi} (s, \pi_{\text{approx}} (s)) \right]
$$

여기서 한 가지 가정이 추가된다. 만약 True Q value를 통해 업데이트한 policy가 다음을 만족한다고 가정하자.

$$
\mathbb{E} \left[ Q_{\theta} (s, \pi_{\text{true}} (s)) \right] \geq
\mathbb{E} \left[ Q^{\pi} (s, \pi_{\text{true}} (s)) \right]
$$

이제 더 작은 구간에 해당하는 $\alpha$를 고르면 approximation Q value를 이용하여 policy를 업데이트 한 경우, true Q value를 통해 policy 업데이트 한 경우와 비교하여 overestimation이 발생할 수 있음을 알 수 있다.

$$
\text{take }\alpha \text{ such that, } \alpha<\min(\epsilon_1,\epsilon_2)\text{ then,}\\
\mathbb{E} \left[ Q_{\theta} (s, \pi_{\text{approx}} (s)) \right] \geq \mathbb{E} \left[ Q^{\pi} (s, \pi_{\text{approx}} (s)) \right]
$$

이러한 overestimation이 미치는 영향은 두 가지로 분류할 수 있다. 누적된 overestimation이 상당한 수준의 bias를 발생시킬 수 있다. 또는 부정확한 estimate가 이어서 진행되는 policy update에 부정적인 영향을 미칠 수 있다.

<center>
<img src='{{"assets/images/TD3/td3_1.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Q overestimation of DDPG</figcaption>
</center>

중간에 expectation에 대한 가정이 들어갔기 때문에 생기는 의문점에 대해 practical하게 DDPG에서 overestimation이 발생하고 있음을 제시하고 있다.

### Clipped Double Q-Learning for Actor-Critic
Double DQN에서는 current value network를 maximize하는 action을 선택하고, 선택한 action을 바탕으로 target value를 계산할 때 target value network를 사용하게 된다. 이를 그대로 actor-critic setting에 적용하면 current policy network를 통해 action을 선택하고 선택한 action을 바탕으로 target value를 계산할 때 target value network를 사용하게 된다.

$$
y = r + \gamma Q_{\theta^{\prime}} (s', \pi_{\phi} (s')).
$$

문제는 actor-critic setting에서는 policy가 천천히 변하다 보니 current network도 충분히 업데이트 되지 않을 수 있다. Current/target network가 너무 비슷하면 independent estimation을 못하고 학습의 속도를 느리게 할 수 있다. 그렇기 때문에 Double DQN 형식 대신 Double Q learning의 형식을 사용하기로 한다. 두 actors, critics가 cross되어 target value를 구하게 된다.

$$
y_1 = r + \gamma Q_{\theta_{2}^{\prime}} (s', \pi_{\phi_1} (s'))\\
y_2 = r + \gamma Q_{\theta_{1}^{\prime}} (s', \pi_{\phi_2} (s'))
$$

### Does this theoretical overestimation occur in practice for state-of-the-art methods?

<center>
<img src='{{"assets/images/TD3/td3_2.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Double DQN vs Double Q learning</figcaption>
</center>

Double DQN과 Double Q learning에 대한 비교 결과를 제시하고 있다. Double Q learning이 overestimation을 덜 하긴 하지만 여전히 overestimation이 발생함을 알 수 있다. 왜 그럴까?

Policy gradient에서는 $\pi\_{\phi\_1}$가 $Q\_{\theta\_1}$을 최대화하도록 업데이트되기 때문에, $Q\_{\theta\_1}$의 target update에서 independent estimate를 사용하는 것이 Policy update로 인해 발생한 bias를 피할 수 있게 한다. 하지만 두 critic은 완전히 independent하지는 않다. Target value를 구할 때 서로의 critic을 이용하여 구하기 때문이다. 또한 같은 replay buffer를 사용하는 것도 또다른 이유가 된다. 그 결과 $Q\_{\theta_2} (s, \pi\_{\phi\_1} (s)) > Q\_{\theta\_1} (s, \pi\_{\phi\_1} (s))$를 만족하는 $s$가 존재할 수 있다. 기본적으로 $Q\_{\theta\_1} (s, \pi\_{\phi\_1} (s))$가 overestimation을 유발하기 때문에 target value에 그 보다 더 큰 값인 $Q\_{\theta_2} (s, \pi\_{\phi\_1} (s))$가 들어가게 되어 overestimation을 증폭시킬 수 있다. 이를 해결하기 위해 다음과 같이 Clipped Double Q-learning algorithm을 제시한다.

$$
y_1 = r + \gamma \min\limits_{i=1,2} Q_{\theta_{i}^{\prime}} (s', \pi_{\phi_1} (s')).
$$

CDQ에서는 underestimation이 발생할 수 있지만, overestimation이 미치는 영향에 비해서는 훨씬 바람직하다. Overestimation의 경우 policy update를 통해 지속적으로 전파될 수 있기 때문이다. CDQ는 Implementation에도 장점이 있다. 하나의 actor만 사용하기 때문에 연산량이 감소하는 효과가 있다. 또한 minimization 연산이 variance가 적은 estimation에 대해 더 높은 value를 부여한다는 점에서 안전하게 policy를 업데이트하도록 한다. CDQ는 finite MDP setting에서 수렴함을 보일 수 있다(Supplement).

## Addressing Variance
### Accumulating Error
Q estimation variance 또한 noisy한 update를 막기 위해서 해결해야 하는 부분이다. Bellman equation의 approximation version에서 식을 변형하면 다음과 같다. Approximation이기 때문에 redisual TD error가 남게 된다.

$$
\begin{aligned}
Q_{\theta} (s_t, a_t) &= r_t + \gamma \mathbb{E} [Q_{\theta} (s_{t+1}, a_{t+1})] - \delta_t \\
&= r_t + \gamma \mathbb{E} \left[ r_{t+1} + \gamma \mathbb{E} \left[ Q_{\theta} (s_{t+2}, a_{t+2}) - \delta_{t+1} \right] \right] - \delta_t \\
&= \mathbb{E}_{s_t \sim p_{\pi}, a_t \sim \pi} \left[ \sum_{i=t}^{T} \gamma^{i-t} (r_i - \delta_i) \right].
\end{aligned}
$$

Q estimate의 variance는 미래 보상과 estimation error에 비례함을 알 수 있다. 그리고 summation을 통해 누적되는 형태이다.

### Target Networks and Delayed Policy Updates

타겟 네트워크 없이 critic을 업데이트하면 residual error가 남게 된다. 이 오차 자체가 누적되기도 하고, policy gradient로 인해 더 증폭되는 방향으로 학습될 수 있기 때문에 주의가 필요하다.

<center>
<img src='{{"assets/images/TD3/td3_3.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Convergence analysis of fixed/learned policy</figcaption>
</center>

그림을 통해 살펴보면, fixed policy의 경우(critic만 업데이트 되는 경우) 타겟 네트워크가 없을 때($\tau=1$)를 포함하여 모든 경우에 수렴할 수 있다. 그러나 learned policy의 경우 파라미터에 따라 수렴하지 못하는 것을 확인할 수 있다. 

### When do actor-critic methods fail to learn?
앞서 target network의 유무와 관계없이 policy update 또한 수렴하지 못하는 원인이 된다는 것을 확인하였다. TD3에서는 이를 방지하기 위해 delayed policy updates를 수행한다.

전체적인 플로우는 이렇게 설명된다. Target value network를 통해 current value network가 수렴할 시간을 주고, 충분히 수렴한 다음에 policy update를 진행하게 된다. 그리고 policy update를 할 때에도 $\theta' \gets \tau \theta + (1 - \tau) \theta'$와 같이 천천히 업데이트한다.

### Target Policy Smoothing Regularization

Deterministic policy의 특성이 가지는 약점들이 있다. Q estimation이 특정 행동에 대해서만 급격히 증가하는 경우, policy가 특정 행동에 치우칠 수 있다. 그리고 approximation에 취약한 구조를 갖는데, target value가 deterministic policy에 의존하므로 특정 행동이 반복적으로 선택된다면 오차가 증폭되는 결과를 초래할 수 있다. 이를 해결하기 위해 smoothing regularization을 사용한다. Target value에서도 policy에 clipped된 noise를 적용하게 된다.

$$
y = r + \gamma Q_{\theta'} (s', \pi_{\phi'} (s') + \epsilon), \\
\epsilon \sim \text{clip} (\mathcal{N} (0, \sigma), -c, c)
$$

## Experiments

TD3의 알고리즘은 다음과 같다.

<center>
<img src='{{"assets/images/TD3/td3_4.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">TD3 Algorithm</figcaption>
</center>

<center>
<img src='{{"assets/images/TD3/td3_5.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Results</figcaption>
</center>

<center>
<img src='{{"assets/images/TD3/td3_6.png" | relative_url}}' style="max-width: 100%; width: auto;">
<figcaption style="text-align: center;">Ablation studies</figcaption>
</center>

Ablation study로 TD3의 구조에서 delayed policy, target policy smoothing, clipped double q learning 요소를 각각 빼거나, AHE의 구조에서 요소 하나만을 사용하여 test한 결과이다. Double Q learning과 Double DQN의 actor-critic variant와도 비교한다. 공정한 비교를 위해 DP와 TPS는 적용되었고 CDQ만 적용하지 않았다.

## Conclusions
